<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic6505">
<title>HP Helion OpenStack(R) 1.1 and 1.1.1: Ceph Configuration</title>
<titlealts>
<searchtitle>HP Helion OpenStack(R) 1.1: Ceph Configuration</searchtitle>
</titlealts>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HP Helion OpenStack(R)"/>
<othermeta name="product-version" content="HP Helion OpenStack(R) 1.1"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Paul F"/>
<othermeta name="product-version1" content="HP Helion OpenStack(R)"/>
<othermeta name="product-version2" content="HP Helion OpenStack(R) 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/ceph/1.1commercial-ceph-hp-helion-openstack-ceph-configuration.md-->
 <!--permalink: /helion/openstack/1.1/ceph-hp-helion-openstack-ceph-configuration/--></p>
<p>This section describes the integration of HP Helion OpenStack(R) 1.1 or 1.1.1, and Ceph Firefly 0.80.7 on a hLinux3.14.29.4 kernel.</p>
<p>Before configuring Helion OpenStack(R) Ceph make sure that HP Helion OpenStack(R) 1.1 or 1.1.1 is installed successfully.</p>
<p>
<b>Note</b>: The Ceph client script is only run on the Helion controller and compute nodes during a manual deployment of the Ceph cluster. If you are using Ansible to configure your Helion nodes and Ceph cluster, you can ignore the manual section.</p>
<section id="ceph-client-install"> <title>Ceph Client Install</title>
<p>The Helion Overcloud Controller and compute nodes are consumers of the storage services provided by the Ceph cluster. To enable them to act as Ceph clients, perform the following steps (either the automated or manual process).</p>
<p>
<b>Note</b>: See note above. If you are using Ansible to configure your Helion nodes and Ceph cluster, you can ignore the manual section.</p>
</section>
<section id="automated-install"> <title>Automated Install</title>
<p>The <codeph>ceph_client_setup.tar</codeph> contains Debian packages, Helion OpenStack(R) Ceph configuration scripts, and the patch file to make the required alterations to the Nova, Glance, and Cinder configuration files. These Debian packages must be installed while installing the Ceph firefly 0.80.7 client packages on the HP Helion overcloud controller and compute nodes. The script present in the tar file helps to automate the installation process.</p>
<p>Once the Ceph cluster is fully operational, run the tar file on the Helion controller nodes (occ0, occ1, and oc mgmt nodes) and the compute node. At the final stage of the installation process, the Ceph client confirms connectivity to the Ceph cluster.</p>
</section>
<section id="running-the-automated-scripts"> <title>Running the automated scripts</title>
<p>Perform the following steps to run the automated installation scripts on the Helion overcloud controller nodes and the compute nodes.</p>
<p>For more details about the Ceph storage cluster with Glance integration, refer to <xref href="http://ceph.com/docs/master/rbd/rbd-openstack/?highlight=openstack%20glance" scope="external" format="html" >http://ceph.com/docs/master/rbd/rbd-openstack/?highlight=openstack%20glance </xref>
</p>
<ol>
<li>
<p>Untar <codeph>ceph_client_setup.tar</codeph> in root home on each of the Helion overcloud controller and compute nodes where the Ceph client needs to be installed.</p>

<codeblock>
<codeph>/home
</codeph>
</codeblock>

<p>A directory named <codeph>ceph_client_setup</codeph> is created. There are several sub directories created under the <codeph>ceph_client_setup</codeph> directory.</p>
</li>
<li>
<p>Copy <codeph>ceph.conf</codeph> file and the keyrings from the admin node to the install directories.</p>
</li>
<li>
<p>Copy the Ceph cluster config files  from the ceph admin node into the <codeph>ceph_cluster_config_files</codeph> directory.</p>
</li>
<li>
<p>The script <codeph>fixuuid.sh</codeph> in the <codeph>setup_scripts</codeph> directory modifies the Helion configuration file patches with the new UUID from the cephx authentication process. To execute<!--A BR tag was used here in the original source.--><codeph>fixuuid.sh</codeph>, enter:</p>

<codeblock>
<codeph>fixuuid.sh *&lt;the new-UUID*&gt;
</codeph>
</codeblock>

<p>The new Cephx UUID is passed as an argument to the <codeph>fixuuid.sh</codeph> script. This will load the new UUID into the patch files that will be used to modify the Helion config files.</p>
</li>
<li>
<p>Run <codeph>ceph_client_install.sh</codeph> on one of the controller nodes or compute nodes.</p>

<p>
<b>Note</b>: The installer script is located at <codeph>/home/ceph_client_setup/setup_scripts/ceph_client_install.sh</codeph>
</p>
</li>
</ol>
<p>The installation script does the following:</p>
<ol>
<li>Checks if the file is untarred in the correct location (<codeph>/home/ceph_client_setup</codeph>) and exits if the script is not available. </li>
<li>Verifies that the ceph cluster <codeph>ceph.conf</codeph> and the keyring files are copied in the correct location (<codeph>/home/ceph_client_setup/ceph_cluster_config_files</codeph>/) and exits if they are not present. If they are present in the correct location, the <codeph>/etc/ceph</codeph> directory is created and the <codeph>ceph.conf</codeph> and the keyring files are copied into the <codeph>/etc/ceph</codeph> directory.</li>
<li>
<p>The script will search for the following files in <codeph>/home/ceph_client_setup/ceph_cluster_config_files/</codeph>
</p>

<p>a. ceph.conf - main Ceph configuration file</p>

<p>b. ceph.client.nova.keyring - Nova user authorization key file</p>

<p>c. ceph.client.cinder.keyring - Cinder user authorization key file</p>

<p>d. ceph.client.cinder-backup.keyring - Cinder backup user authorization key file</p>

<p>e. ceph.client.glance.keyring - Glance user authorization key file</p>

<p>f. ceph.client.admin.keyring - admin user authorization key file</p>

<p>g. ceph.client.radosgw.keyring - RADOSGW authorization key file</p>

<p>
<b>NOTE</b>: if a RADOS gateway is not going to be used for this installation, create an empty file with this name in the <codeph>/home/ceph_client_setup/ceph_cluster_config_files/</codeph> directory so the script will still find a file with the expected name.</p>
</li>
<li>
<p>Checks the availability of the Ceph client Debian packages in <codeph>/ceph_client_setup/ceph_client_debs/</codeph> and exits the script if it is unavailable.</p>
</li>
<li>
<p>Checks whether the system is a hLinux system and verifies the existence of the <codeph>/etc/apt/sources.list</codeph> file. If it meets these requirements, then a backup copy of the original <codeph>sources.list</codeph> file is created in <codeph>/home/ceph_client_setup/orig_backup/</codeph> directory and then the local Ceph packages location is added to the file as a valid local repository.</p>
</li>
<li>
<p>Does an <codeph>apt-get update</codeph> and <codeph>apt-get install</codeph> of the Ceph client packages once the repository is added to the <codeph>apt sources.list</codeph> file. If <codeph>apt-get update</codeph> and <codeph>apt-get install</codeph> of the Ceph client packages executes without errors,  <codeph>dpkg</codeph> is executed to verify the installation of the correct packages.</p>
</li>
<li>
<p>Checks the <codeph>ceph</codeph> command to verify if it is available and executable. If the <codeph>ceph</codeph> command is available and executable then it is executed to verify whether the health of the Ceph cluster can be determined. If the Ceph cluster health shows as <codeph>HEALTH_OK</codeph>, the script displays the status and continues.</p>
</li>
</ol>
<p>The following sub-scripts run upon successful completion of the above steps:</p>
<p>
<b>
<codeph>copy_icinga_monscripts.sh</codeph>
</b>- copies the Ceph cluster monitoring scripts in the iCinga script directory on the Helion overcloud controller nodes only.</p>
<p>
<b>
<codeph>patch_config_files.sh</codeph>
</b> - patches the Helion configuration files with the changes required for  Ceph communication.</p>
<p>
<b>
<codeph>ceph_pythonlinks_create.sh</codeph>
</b> - creates softlinks to the necessary Ceph Python library files in the Helion Python directories.</p>
<p>
<b>
<codeph>create_helion_pools.sh</codeph>
</b> - creates the required Helion pools in the Ceph storage (only runs on  the overcloud controller node and management node)</p>
<p>
<b>
<codeph>helion_service_restarts.sh</codeph>
</b> - restarts the Helion services.</p>
<p>
<!--


####Availability of Ceph Client Script

<table>
<table style="text-align: left; vertical-align: top; width:650px;">
<tr style="background-color: #C8C8C8;">
    <th > Description</th>
    <th>File Location </th>
</tr>
    <tr>
<td>Ceph client package file to be installed on the running Controller and Compute nodes in the
HP Helion OpenStack(R) commercial build</td>
<td>`https://helion.hpwsportal.com` Click on the workloads category on the left side and then you click on the storage subcategory to find all the Ceph related files.<br /></td>
</tr>
  <table>

As a sanity-check, cross check with the attached "dpkg -l" output after Ceph package installation:

<table>
<table style="text-align: left; vertical-align: top; width:650px;">
<tr style="background-color: #C8C8C8;">
    <th > Description</th>
    <th > Description</th>  
    <th>File Location </th>
</tr>
    <tr>
<td>Controller node</td>
<td>DPKG -l output after Ceph package install</td>
<td>`https://helion.hpwsportal.com` Click on the workloads category on the left side and then you click on the storage subcategory to find all the Ceph related files.<tdt>
</tr>
<tr>
<td>Compute node</td>
<td>DPKG -l output after Ceph package install</td>
<td>https://helion.hpwsportal.com Click on the workloads category on the left side and then you click on the storage subcategory to find all the Ceph related files. <tdt>
</tr>
  </table>
-->
<!--

####Verify the Ceph Version

* Execute the following command to verify the Ceph version on the overcloud nova compute node:

        # dpkg -l|grep ceph

The following example displays the ceph version:

    root@overcloud-novacompute0-c6y5lbj2hvlu:/home# dpkg -l|grep ceph
    ii ceph 0.80.7-1+hLinux amd64
    [hLinux]distributed storage and file system
    ii ceph-common 0.80.7-1+hLinux amd64
    [hLinux]common utilities to mount and interact with a ceph storage cluster
    ii libcephfs1 0.80.7-1 amd64
    Ceph distributed file system client library
    ii python-ceph 0.80.7-1+hLinux amd64
    [hLinux]Python libraries for the Ceph distributed filesystem



--></p>
</section>
<section id="next-steps"> <title>Next Steps</title>
<p>
  <xref href="../../../commercial/GA1/ceph/1.1commenrcial.ceph-helion-openstack-glance-ceph-interoperability.dita" >Ceph Glance-Ceph Interoperability</xref>
</p>
<!-- ===================== horizontal rule ===================== -->
<p>
  <xref type="section" href="#topic6505/top"> Return to Top </xref>
</p>
</section>
</body>
</topic>
