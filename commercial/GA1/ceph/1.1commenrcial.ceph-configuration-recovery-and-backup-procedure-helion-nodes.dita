<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic3172">
<title>Useful tips</title>
<titlealts>
<searchtitle>HP <tm tmtype="reg">Helion Openstack</tm> HP Helion Ceph Backup and Recovery</searchtitle>
</titlealts>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HP <tm tmtype="reg">Helion Openstack</tm>"/>
<othermeta name="product-version" content="HP <tm tmtype="reg">Helion Openstack</tm> 1.1"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Paul F"/>
<othermeta name="product-version1" content="HP <tm tmtype="reg">Helion Openstack</tm>"/>
<othermeta name="product-version2" content="HP <tm tmtype="reg">Helion Openstack</tm> 1.1"/>
</metadata>
</prolog>
<body>

<!--PUBLISHED-->
 <!--./commercial/GA1/ceph/1.1commenrcial.ceph-configuration-recovery-and-backup-procedure-helion-nodes.md-->
 <!--permalink: /helion/openstack/1.1/ceph-configuration-recovery-and-backup-procedure-HP-Helion-nodes/-->

<!-- # HP <tm tmtype="reg">Helion Openstack</tm> 1.1: Ceph Configuring Backup and Recovery for HP Helion Nodes-->
<p>
The following steps ensure the recovery of the lost configuration.

1. Run the  ceph_client_setup script available at  <xref href ="https://helion.hpwsportal.com" format="html"> HDN</xref> on each of the controller/compute nodes. (For information on what files need to be copied from  the Ceph cluster node to their respective directories, refer to the script's README file.)
2. To confirm ceph health, run:

     ceph health

3. If you reboot your controller and compute node after running the ceph_client_setup script, the patch update for glance-api.conf, nova.conf, cinder.conf might default to the original state. In this case, from each controller and compute node, go to:


        /home/ceph_client_setup/setup_scripts

    1. Execute sh patch_config_files.sh

    2. Execute sh helion_service_restarts.sh

</p>
<p>Sample files</p>

<table>
  <tgroup cols="3">

    <thead> 
      <row>
        <entry>Helion Node</entry>
        <entry> Service/File name</entry>    
        <entry>File Attachment</entry>
       </row>
    </thead>
<tbody>
<row>
  <entry> Controller node</entry>
  <entry> /etc/glance/glance-api.conf /etc/cinder/cinder.conf /etc/nova/nova.conf</entry>

  <entry> 1. Log in to the <xref href ="https://helion.hpwsportal.com" format="html"> HDN</xref>.  
  2. Click the workloads category on the left side.  
  3. Click the storage subcategory to find all the Ceph related files.  <b>Filename</b>  
  </entry>
</row>
  <row>
    <entry>Compute Node</entry>
    <entry>/etc/nova/nova.conf</entry>
    <entry>1. Log in to the <xref href="https://helion.hpwsportal.com" format="html"> HDN</xref> 2.
              Click the workloads category on the left side.  3. Click the storage subcategory to
              find all the Ceph related files. <!-- <br /> Filename need ?-->
            </entry>
  </row>
</tbody>
  </tgroup>
</table>




<p>The following tips will help you troubleshoot errors:</p>
<ul>
<li>
<p>Verify that the http proxy settings are correct in both <codeph>/etc/environment</codeph> and <codeph>/etc/apt/apt.conf</codeph> for connecting to external resources.</p>
</li>
<li>
<p>The Firefly version of Ceph packages has a default replication set to 3x (three times). Therefore you need a minimum of three OSDs.</p>
</li>
<li>
<p>If a <codeph>Decrypt error</codeph> occurs from any of the Helion nodes with glance, cinder, or nova, make sure that all the Ceph cluster nodes are in same time zone. HP recommends that you configure a NTP server in the <codeph>/etc/ntp.conf</codeph> file on all Ceph cluster nodes.  After making changes, restart NTP [<codeph>/etc/init.d/ntp restart</codeph>]. Install the <codeph>ntp</codeph> package, if it is not there.</p>
</li>
<li>
<p>Make sure the Ceph cluster uses the same IP range as the <tm tmtype="reg">Helion Openstack</tm> overcloud nodes. Also, make sure the IP range for the Ceph cluster does not conflict with those used by Helion set up.</p>
</li>
<li>
<p>Be sure to check the log files in <codeph>/var/log/ceph/</codeph> for each node. Any errors or exceptions are useful for troubleshooting.</p>
</li>
<li>
<p>Any changes you make to the Ceph configuration file requires a restart for the changes to take affect.</p>
</li>
<li>
<p>You cannot change the default Cephx Authentication to <codeph>None</codeph> once the cluster is up and running. You will have to purge and reinstall the Ceph packages and build the cluster.</p>
</li>
<li>
<p>Enable logging if you encounter a problem at any of these steps:</p>

<p>To do this, edit <codeph>glance-api.conf</codeph>, <codeph>cinder.conf</codeph> and <codeph>nova.conf</codeph> files with the following in the default section.</p>

<codeblock>
<codeph>debug = True
verbose = True
</codeph>
</codeblock>

<p>Restart Glance, Cinder, and Nova services respectively.</p>

<p>On HP Helion Commercial, the logs for Glance and Cinder are stored by default in the following directories:</p>

<codeblock>
<codeph>Glance - /var/log/glance
Cinder - /var/log/upstart
</codeph>
</codeblock>
</li>
</ul>
<p>Gather these logs and contact HP support team.</p>
<ul>
<li>
<p>If any Placement Group (PG)-related issues occur in the Ceph cluster, refer to the following link:</p>

<p>
<xref href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/" scope="external" format="html" >http://docs.ceph.com/docs/master/rados/operations/placement-groups/</xref>
</p>
</li>
</ul>
<section id="choosing-the-number-of-the-placement-group"> <title>Choosing the number of the placement group</title>
<p>If there are more than 50 OSDs, we recommend approximately 50-100 PGs per OSD to balance resource usage, data durability and distribution. To get a baseline for a single pool of objects, use the following formula:</p>
<p>
  <image href="../../../media/commercial-ceph-formula-placement-group.png"/>
</p>
<p>Where pool size is either the number of replicas for replicated pools or the K+M sum for erasure coded pools (as returned by <codeph>ceph osd erasure-code-profile get</codeph>).
Round up the results to the nearest power of two. Rounding up is optional, but recommended for CRUSH to evenly balance the number of objects among PGs.</p>
<p>For example, for a cluster with 200 OSDs and a pool size of 3 replicas, estimate the number of PGs as follows:</p>
<codeblock>
  <codeph>(200 * 100)
----------- = 6667. Nearest power of 2: 8192
       3
</codeph>
</codeblock>

</section>
 
</body>
</topic>
