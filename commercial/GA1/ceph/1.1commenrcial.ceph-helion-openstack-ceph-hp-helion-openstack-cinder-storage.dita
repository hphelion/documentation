<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic4920">
<title>HP Helion OpenStack(R) 1.1 and 1.1.1: Cinder Ceph Storage</title>
<titlealts>
<searchtitle>HP Helion OpenStack(R) 1.1: Cinder Ceph Storage</searchtitle>
</titlealts>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HP Helion OpenStack(R)"/>
<othermeta name="product-version" content="HP Helion OpenStack(R) 1.1"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Paul F, Binamra S"/>
<othermeta name="product-version1" content="HP Helion OpenStack(R)"/>
<othermeta name="product-version2" content="HP Helion OpenStack(R) 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/ceph/1.1commenrcial.ceph-helion-openstack-ceph-hp-helion-openstack-cinder-storage.md-->
 <!--permalink: /helion/openstack/1.1/ceph-hp-helion-openstack-cinder-storage/--></p>
<p>The following steps are automatically performed when you download and execute <codeph>Ceph_client</codeph> script from the controller and compute nodes as per the readme file in the tar file.</p>
<ol>
<li>
<p>Verify the Ceph health by entering:</p>

<codeblock>
<codeph>ceph health
</codeph>
</codeblock>

<p>Output:</p>

<codeblock>
<codeph>HEALTH_OK
</codeph>
</codeblock>
</li>
<li>
<p>Create a new pool for the Cinder volume by entering:</p>

<codeblock>
<codeph>ceph osd pool create &lt;cinder pool name&gt; &lt;pg-num&gt;
</codeph>
</codeblock>

<p>For example:</p>

<codeblock>
<codeph>ceph osd pool create helion-ceph-cinder 100
</codeph>
</codeblock>
</li>
<li>
<p>To verify a new pool creation, enter:</p>

<codeblock>
<codeph>rados lspools
</codeph>
</codeblock>

<p>Output:</p>

<codeblock>
<codeph>rbd ls helion-ceph-cinder
</codeph>
</codeblock>
</li>
<li>
<p>Create symbolic or softlinks to the relevant files on overcloud controller nodes by entering:</p>

<codeblock>
<codeph>ln -s /usr/lib/python2.7/dist-packages/rados* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.
ln -s /usr/lib/python2.7/dist-packages/rbd* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.
</codeph>
</codeblock>
</li>
</ol>
<section id="configuring-cinder"> <title>Configuring Cinder</title>
<p>HP Helion OpenStack(R) requires the <codeph>rbd</codeph> driver to interact with Ceph block devices. The pool name must be specified for the block device. To specify the pool name on the overcloud controller nodes edit <codeph>cinder.conf</codeph>.</p>
<p>Perform the following steps to configure Cinder.</p>
<ol>
<li>
<p>Edit <codeph>/etc/cinder/cinder.conf</codeph> and add the following:</p>

<codeblock>
<codeph>[DEFAULT]

# For ceph integration
volume_driver=cinder.volume.drivers.rbd.RBDDriver
rbd_pool=helion-ceph-cinder
rbd_ceph_conf=/etc/ceph/ceph.conf
rbd_flatten_volume_from_snapshot=false
rbd_max_clone_depth=5
</codeph>
</codeblock>

<p>You can also insert a comment on the <codeph>lvm</codeph> backend in the <codeph>cinder.conf</codeph> file for your reference.</p>

<codeblock>
<codeph># LVM thin provision. This way we do not dd the disk

#enabled_backends = lvm-1
</codeph>
</codeblock>
</li>
<li>
<p>Restart the Cinder service by entering:</p>

<codeblock>
<codeph>service cinder-volume restart
service cinder-scheduler restart
service cinder-api restart
</codeph>
</codeblock>
</li>
</ol>
<p>Once the cinder service is restarted, you can create a volume.</p>
</section>
<section id="helion-openstack-cinder-backup-ceph-storage"> <title>Helion OpenStack(R) Cinder Backup Ceph Storage</title>
<p>The Ceph backup driver performs the data backup operation on the volume to a Ceph RADOS Block Device (RBD). Backups can be performed between different Ceph pools and Ceph clusters. This section explains the backup and restore procedure for Cinder volumes.</p>
<p>On the Helion controller and compute node, if the Ceph client softwares are successfully installed and the Ceph configuration and keyring files are copied in the Helion controller and compute nodes, then the Helion OpenStack(R) Ceph Configuration service performs the following steps:</p>
<ol>
<li>
<p>Verify Ceph health status by entering:</p>

<codeblock>
<codeph>ceph health
</codeph>
</codeblock>

<p>Output:</p>

<codeblock>
<codeph>HEALTH_OK
</codeph>
</codeblock>
</li>
<li>
<p>Create a new pool for Cinder backup by entering:</p>

<codeblock>
<codeph>ceph osd pool create &lt;cinder backup pool name&gt; &lt;pg-num&gt;
</codeph>
</codeblock>

<p>For example:</p>

<codeblock>
<codeph>ceph osd pool create helion-ceph-backups 100
</codeph>
</codeblock>
</li>
<li>
<p>To verify that a new backup pool is created, enter:</p>

<codeblock>
<codeph>rados lspools
</codeph>
</codeblock>

<p>Output:</p>

<codeblock>
<codeph>rbd ls helion-ceph-backups
</codeph>
</codeblock>
</li>
<li>
<p>Create symbolic or softlinks to the relevant files on overcloud controller nodes (if the following links are already created for other services, then ignore them) by entering:</p>

<codeblock>
<codeph>ln -s /usr/lib/python2.7/dist-packages/rados* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.

ln -s /usr/lib/python2.7/dist-packages/rbd* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.
</codeph>
</codeblock>
</li>
</ol>
</section>
<section id="configuring-cinder-backup"> <title>Configuring Cinder backup</title>
<p>Perform the following steps to enable the Ceph backup driver:</p>
<ol>
<li>
<p>Edit <codeph>/etc/cinder/cinder.conf</codeph> and add the following options in all three controller nodes [OCC mgmt, OCC0, OCC1].</p>

<codeblock>
<codeph>[DEFAULT]

# For cinder backup
    backup_driver=cinder.backup.drivers.ceph
backup_ceph_conf=/etc/ceph/ceph.conf
backup_ceph_user=cinder-backup
backup_ceph_chunk_size=134217728
backup_ceph_pool=helion-ceph-backups
backup_ceph_stripe_unit=0
backup_ceph_stripe_count=0
restore_discard_excess_bytes=true
</codeph>
</codeblock>

<p>The following table provides the description of the parameters.</p>

<table>
<tgroup cols="2">
<colspec colname="col1"/>
<colspec colname="col2"/>
<thead>
<row>
<entry> Parameters</entry>
<entry>Descriptions </entry>
    </row>
</thead>
<tbody>
<row>
<entry>backup_ceph_chunk_size = 134217728</entry>
<entry>The chunk size, in bytes, that a backup is broken into before transfer to the Ceph object store.</entry>
</row>
<row>
<entry>backup_ceph_conf = 
/etc/ceph/ceph.conf</entry>
<entry>Ceph configuration file to use.</entry>
</row>
<row>
<entry>backup_ceph_pool = helion-ceph-backups</entry>
<entry>The Ceph pool where volume backups are stored.</entry>
</row>
<row>
<entry>backup_ceph_stripe_count = 0</entry>
<entry>RBD stripe count to use when creating a backup image.</entry>
</row>
<row>
<entry>backup_ceph_stripe_unit = 0</entry>
<entry>RBD stripe unit to use when creating a backup image.</entry>
</row>
<row>
<entry>backup_ceph_user = cinder</entry>
<entry>The Ceph user to connect with. Default here is to use the same user as for Cinder volumes. If not using cephx this should be set to None.</entry>
</row>
<row>
<entry>restore_discard_excess_bytes = True</entry>
<entry>If True, always discard excess bytes when restoring volumes (that is, pad with zeroes).</entry>
</row>
</tbody>
</tgroup>
</table>
</li>
<li>
<p>Restart the Cinder service by entering:</p>

<codeblock>
<codeph>service cinder-backup restart
</codeph>
</codeblock>
</li>
</ol>
</section>
<section id="next-steps"> <title>Next Steps</title>
<p>
  <xref href="../../../commercial/GA1/ceph/1.1commercial-ceph-helion-openstack-nova-ceph-Storage.dita" >Ceph Nova Storage</xref>
</p>
<p>
  <xref type="section" href="#topic4920/top"> Return to Top </xref>
</p>
</section>
</body>
</topic>
