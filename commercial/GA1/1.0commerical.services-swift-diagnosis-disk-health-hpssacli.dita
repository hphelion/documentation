jgl
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic5788">
<title>HP <tm tmtype="reg">Helion Openstack</tm> 1.0: Diagnosis of disk health using hpssacli utility for HP servers</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HP Helion Openstack"/>
<othermeta name="product-version" content="HP Helion Openstack 1.0"/>
<othermeta name="product-version" content="HP Helion Openstack 1.0.1"/>
<othermeta name="product-version" content="HP Helion Openstack 1.1"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Karthik P, Binamra S"/>
<othermeta name="product-version1" content="HP Helion Openstack"/>
<othermeta name="product-version2" content="HP Helion Openstack 1.0"/>
<othermeta name="product-version3" content="HP Helion Openstack 1.0.1"/>
<othermeta name="product-version4" content="HP Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/1.0commerical.services-swift-diagnosis-disk-health-hpssacli.md-->
 <!--permalink: /helion/openstack/services/swift/diagnosis-disk-health/hpssacli/--></p>
<p>

</p>
<p>The health of the disk  of the HP servers can be diagnosed using the <b>
<i>hpsacli</i>
</b> utility.</p>
<section id="hp-smart-storage-administrator-cli-20220-hpssacli"> <title>HP Smart Storage Administrator CLI 2.0.22.0 (HPSSACLI)</title>
<p>The HP Smart Storage Administrator CLI (HPSSACLI) is a command line disk configuration program for Smart Array Controllers.</p>
</section>
<section id="deployment"> <title>Deployment</title>
<p>The <b>
<i>hpssacli</i>
</b> utility is deployed onto the servers wherever the disks are to be monitored, usually all Swift, VSA, and Compute nodes. This page explains the collection of diagnostic reports from disks in the servers where the utility has been loaded.</p>
</section>
<section id="download-the-hpssacli-debian-package-into-the-kvm-host"> <title>Download the hpssacli debian package into the KVM host</title>
<p>
  <xref href="http://downloads.linux.hp.com/SDR/repo/mcp/pool/non-free/hpssacli-2.0-16.0_amd64.deb" scope="external" format="html" >http://downloads.linux.hp.com/SDR/repo/mcp/pool/non-free/hpssacli-2.0-16.0_amd64.deb</xref>
</p>
</section>
<section id="copy-the-debian-to-seed-and-to-the-servers-where-the-disks-has-to-be-monitored"> <title>Copy the debian to seed and to the servers where the disks has to be monitored</title>
<p>Use <codeph>scp</codeph> to copy the utility package on to the servers and install it.</p>
<ol>
<li>
<p>Copy the package from KVM host to SEED.</p>

<codeblock>
<codeph># scp scp "hpssacli-2.0-16.0_amd64.deb" root@&lt;IP address of Seed&gt;
</codeph>
</codeblock>
</li>
<li>
<p>Copy the package from SEED to machine where the disks to be monitored.</p>

<codeblock>
<codeph># scp scp "hpssacli-2.0-16.0_amd64.deb" heat-admin@&lt;IP address of machine&gt;
</codeph>
</codeblock>
</li>
<li>
<p>Log in to the server where the debian is copied and install the package.</p>

<codeblock>
<codeph># ssh heat-admin@&lt;IP address of machine&gt;
</codeph>
</codeblock>
</li>
<li>
<p>Change the directory.</p>

<codeblock>
<codeph># cd /home/heat-admin/
</codeph>
</codeblock>
</li>
<li>
<p>List the files.</p>

<codeblock>
<codeph># ls
</codeph>
</codeblock>

<p>All the files present in that directory will be displayed.</p>
</li>
<li>
<p>Install the following package.</p>

<codeblock>
<codeph># dpkg -i hpssacli-2.0-16.0_amd64.deb
</codeph>
</codeblock>

<p>
<b>Note</b>: The diagnostics can be done in the server or from SEED through ssh.</p>
</li>
<li>
<p>You can collect the diagnostic report of the controller either in server or through Seed using ssh.</p>
</li>
</ol>
</section>
<section id="collect-the-diagnostic-report"> <title>Collect the diagnostic report</title>
<p>
<b>Using the server controller:</b>
<!--
1. Log in to the server

        # ssh heat-admin@<IP address of machine>
2. Change the directory

        /home/heat-admin/hp/hpssacli/bld
--></p>
<ol>
<li>
<p>Enter the following command to determine the controller slot.</p>

<codeblock>
<codeph># hpssacli ctrl all show status
</codeph>
</codeblock>

<p>The following sample displays slot details:</p>

<codeblock>
<codeph>Smart Array P420i in Slot 0 (Embedded)

   Controller Status: OK

   Cache Status: OK

   Battery/Capacitor Status: OK
</codeph>
</codeblock>
</li>
<li>
<p>Generate the diagnostic report for a particular slot.</p>

<codeblock>
<codeph># hpssacli ctrl slot=(slot number) diag file=&lt;filename.zip&gt;
</codeph>
</codeblock>

<p>Or <!--A BR tag was used here in the original source.-->
    Generate the report for all the slots</p>

<codeblock>
<codeph># hpssacli ctrl all diag file=&lt;filename.zip&gt;
</codeph>
</codeblock>

<p>The file will be stored in the selected location.</p>
</li>
<li>
<p>Copy the generated file to the desired location.</p>
</li>
<li>
<p>Extract the file.</p>
</li>
</ol>
<p>
  <b>Using ssh from Seed:</b>
</p>
<!--
1. Log in to the server

        ssh heat-admin@<Machine IP address>

2. Change the directory

        /home/heat-admin/hp/hpssacli/bld
-->
<ol>
<li>
<p>Generate the diagnostic report of the particular slot.</p>

<codeblock>
<codeph> ssh heat-admin@&lt;Machine IP address&gt; "sudo hpssacli ctrl slot=&lt;slot number&gt; diag file=details_slot_&lt;slot number&gt;.zip"
</codeph>
</codeblock>
</li>
<li>
<p>Copy the report from the server to Seed VM.</p>

<codeblock>
<codeph># scp heat-admin@&lt;Machine IP address&gt;:/home/heat-admin/&lt;filename.zip&gt; 
</codeph>
</codeblock>
</li>
<li>
<p>Copy the report &lt;filename.zip&gt; to the KVM_host</p>

<codeblock>
<codeph># scp details_slot_&lt;slot number&gt;.zip ubuntu@&lt;KVM_Host IP address&gt;:
</codeph>
</codeblock>

<!--Enter login credentails ???--></li>
<li>
<p>Extract the file.</p>
</li>
<li>
<p>Open the <codeph>ADUReport.htm</codeph> file in the browser. The html page displays complete details of the controller and the health status of the physical disks available in the machine.</p>

<!-- <image = utility_ADUR-report> --></li>
<li>
<p>Generate SmartSSD Wear Gauge Report from either the local server or from the seed:</p>

<ul>
<li>
<p>From the local server</p>

<p>
<codeph># hpssacli ctrl slot=&lt;slot number&gt; diag file=&lt;filename.zip&gt; ssdrpt=on</codeph>
</p>
</li>
<li>
<p>From the seed</p>

<p>
<codeph># ssh heat-admin@&lt;machine IP address&gt; "sudo hpssacli ctrl slot=&lt;slot number&gt; diag file=&lt;filename.zip&gt;</codeph>
</p>
</li>
</ul>
</li>
</ol>
<!-- **Now retrieve the ssd_report.zip to kvm host using scp from server to analyse.??? is this applicable for seed only??** -->
<p>The following SmartSSDWearGaugeReport.txt sample file displays the details of the  SSD drives.</p>
<codeblock>
  <codeph>ADU Version                             2.0.22.0

Diagnostic Module Version               8.0.22.0

Time Generated                          Tuesday September 16, 2014 12:26:57PM

Device Summary:

   Smart Array P420i in Embedded Slot

Report for Smart Array P420i in Embedded Slot

---------------------------------------------

Smart Array P420i in Embedded Slot : Internal Drive Cage at Port 2I : Box 2 : Physical Drive (400 GB SAS SSD) 2I:2:6 : SmartSSD Wear Gauge

   Status                               OK

   Supported                            TRUE

   Log Full                             FALSE

   Utilization                          0.000000

   Power On Hours                       1798

   Has Smart Trip SSD Wearout           FALSE

Smart Array P420i in Embedded Slot : Internal Drive Cage at Port 2I : Box 2 : Physical Drive (400 GB SAS SSD) 2I:2:7 : SmartSSD Wear Gauge

   Status                               OK

   Supported                            TRUE

   Log Full                             FALSE

   Utilization                          0.000000

   Power On Hours                       1797

   Has Smart Trip SSD Wearout           FALSE

Smart Array P420i in Embedded Slot : Internal Drive Cage at Port 2I : Box 2 : Physical Drive (400 GB SAS SSD) 2I:2:8 : SmartSSD Wear Gauge

   Status                               OK

   Supported                            TRUE

   Log Full                             FALSE

   Utilization                          0.000000

   Power On Hours                       1798

   Has Smart Trip SSD Wearout           FALSE
</codeph>
</codeblock>
</section>
<section id="other-useful-commands"> <title>Other useful commands</title>
<ul>
<li>
<p>To get the details of the physical disks and also the logical group status on the server</p>

<codeblock>
<codeph>ssh heat-admin@&lt;machine IP address&gt; "sudo hpssacli ctrl slot=&lt;slot number&gt; show config detail"
</codeph>
</codeblock>
</li>
<li>
<p>To execute other operations on physical drives</p>

<codeblock>
<codeph>ssh heat-admin@&lt;machine IP address&gt; "sudo hpssacli help physicaldrive"
</codeph>
</codeblock>
</li>
<li>
<p>Open the help to explore other options with the utility</p>

<codeblock>
<codeph>ssh heat-admin@&lt;machine IP address&gt; "sudo hpssacli help"
</codeph>
</codeblock>
</li>
</ul>
<p>
  <xref href="#topic5788"> Return to Top </xref>
</p>
<!-- ===================== horizontal rule ===================== -->
</section>
</body>
</topic>
