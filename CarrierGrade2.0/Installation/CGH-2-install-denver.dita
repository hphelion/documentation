<?xml version="1.0" encoding="UTF-8"?>
  <!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic10581c2id">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.0: Deploying the Denver
    Template</title>
  <prolog>
    <metadata>
      <othermeta name="layout" content="default"/>
      <othermeta name="product-version" content="HP Helion OpenStack Carrier Grade 2.0"/>
      <othermeta name="role" content="Storage Administrator"/>
      <othermeta name="role" content="Storage Architect"/>
      <othermeta name="role" content="Michael B"/>
      <othermeta name="product-version1" content="HP Helion OpenStack Carrier Grade 2.0"/>
    </metadata>
  </prolog>
  <body>
    <p>After the lifecycle manager is installed, the next task in installing the <xref
        href="carrier-grade-install-overview.dita#topic1925cgio/install-option">KVM topology</xref> is
      to deploy the HP Helion OpenStack cloud.</p>
    <p>If you are installing the <codeph>denver</codeph> template, which includes the KVM region and
      uses AVS networking, follow these instructions. If you are installing a different deployment,
      disregard these instructions. </p>
    <section conref="CGH-2-install-sacramento.dita#topic10581c2is/conref-hos-install"/>
    <section id="conref-provision">
      <title>Provision the new cloud</title>
      <ol id="ol_kwm_c1k_zt">
        <li>Execute the following command to provision and configure the HPE Helion OpenStack cloud.
            <codeblock>hlm define -t denver &lt;cloudname&gt;</codeblock><p>Where::
              <codeph>&lt;cloudname></codeph> is the name of the cloud to create.</p><p
            conref="CGH-2-install-sacramento.dita#topic10581c2is/cloudname"/><p>The command creates the
              <codeph>/var/hlm/clouds/&lt;cloudname></codeph> directory, which contains several JSON
            template files. </p></li>
      </ol>
    </section>
    <section>
      <title>Configuring JSON files</title>
    </section>
    <section id="jsons">The HP Helion OpenStack deployment requires several JSON
          files.<p><b>Important:</b> Do not store backup of the JSON files inside your cloud
        directory or any where inside the <codeph>/var/hlm/clouds</codeph> directory. You can create
        a backup folder on <codeph>/root/</codeph> directory or on a remote system.
        <!--HCG-681--></p></section>
    <section id="node-prov">
      <title>Configure the node-provision.json file</title>
      <p>Modify the <codeph>node-provision.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname></codeph> directory of the lifecycle manager. This
        file supplies input values to the hprovision script, later in the installation.</p>
      <p>Edit the <codeph>node-provision.json</codeph> file to change only the following
          fields:<table id="table_ycw_qrn_xs">
          <tgroup cols="2">
            <colspec colname="col1" colsep="1" rowsep="1"/>
            <colspec colname="col2" colsep="1" rowsep="1"/>
            <thead>
              <row>
                <entry colsep="1" rowsep="1">Field</entry>
                <entry colsep="1" rowsep="1">Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>Pxe-mac-address</entry>
                <entry>MAC address of the interface you want to PXE boot onto. This is not same as
                  iLO MAC address.</entry>
              </row>
              <row>
                <entry>pm_ip</entry>
                <entry>Power management IP (iLO ip)</entry>
              </row>
              <row>
                <entry>pm_user</entry>
                <entry>Power management user (iLO username)</entry>
              </row>
              <row>
                <entry>pm_pass</entry>
                <entry>Power management password (iLO password)</entry>
              </row>
              <row>
                <entry>failure_zone, vendor, model, os_partition_size, data_partition_size</entry>
                <entry>Enter the same value as for these fields an in the
                    <codeph>nodes.json</codeph> file used during cloud deployment</entry>
              </row>
            </tbody>
          </tgroup>
        </table></p>
      <p>Click here to see a <xref
        href="carrier-grade-install-kvm-only-json.dita#topic10581cgikoj">sample node-provision.json file</xref>. </p>
    </section>
    <section conref="CGH-2-install-sacramento.dita#topic10581c2is/conref-pxe-boot"/>
    <section>
      <title>Modify the ansible.json file</title>
      <p>Modify the <codeph>ansible.json</codeph> file in the /var/hlm/clouds/&lt;cloudname>
        directory of the lifecycle manager with values appropriate for your environment. In the
        following example, the IP addresses have been masked for security: </p>
      <p>
        <codeblock>{
    "product": {
        "version": 1
    },

    "property-groups": [
        {
            "name": "ansible-vars",
            "properties": {
                "dns_domain_name": "helion.cg",
                "dns_address": "10.x.x.x",
                "ext_dns_ip": "10.x.x.x",
                "upstream_ntp_servers": [
                    "10.x.x.x",
                    "16.x.x.x",
                    "2.debian.pool.ntp.org"
                ],
                "ssl_cert_file": "ca.crt",
                "ssl_key_file": "cakey.pem",
                "ssl_passphrase": "cghelion"
            }
        }
    ]
}
</codeblock>
      </p>
    </section>
    <section>
      <title>Modify the ldap.json file</title>
      <p>Modify the <codeph>ldap.json</codeph> file in the /var/hlm/clouds/&lt;cloudname> directory
        of the lifecycle manager with values appropriate for your environment. In the following
        example, the IP addresses have been masked for security:  </p>
      <p>
        <codeblock>
    "product": {
        "version": 1
    },

    "property-groups": [
        {
            "name": "ldap-vars",
            "properties": {
                "ldap_domain_name": "americas",
                "ldap_url": "10.x.x.x",
                "ldap_username": "admin",
                "ldap_password": "admin",
                "ldap_domain": "dc=helioncg,dc=local",
                "ldap_ou": "CGTestUsers",
                "ldap_nova_password": "nova",
                "ldap_nova_user": "nova",
                "ldap_neutron_password": "neutron",
                "ldap_neutron_user": "neutron",
                "ldap_cinder_password": "cinder",
                "ldap_cinder_user": "cinder",
                "ldap_glance_password": "glance",
                "ldap_glance_user": "glance",
                "ldap_enabled": 1
            }
        }
    ]
}</codeblock>
      </p>
    </section>
    <section>
      <title>Modify the wr.json file</title>
      <p>Modify the <codeph>wr.json</codeph> file in the /var/hlm/clouds/&lt;cloudname> directory of
        the lifecycle manager with values appropriate for your environment. In the following
        example, the IP addresses have been masked for security:  </p>
      <p>
        <codeblock>{
    "product": {
        "version": 1
    },

    "property-groups": [
        {
            "name": "wr-vars",
            "properties": {
                "database_storage": 50,
                "backup_storage": 300,
                "shared_instance_storage": 250,
                "region_name": "regionone",
                "logical_interface": [
                    {
                        "lag_interface": "N",
                        "lag_mode": 4,
                        "interface_mtu": 1500,
                        "interface_ports": [
                            "eth0"
                        ],
                        "network": [
                            {
                                "ip_start_address": "10.x.x.x",
                                "ip_end_address": "10.x.x.x",
                                "name": "CLM"
                            },
                            {
                                "ip_start_address": "172.x.x.x",
                                "ip_end_address": "172.x.x.x",
                                "name": "BLS"
                            },
                            {
                                "ip_start_address": "10.x.x.x",
                                "ip_end_address": "10.x.x.x",
                                "name": "CAN"
                            }
                        ]
                    }
                ],
                "pxeboot_cidr": "10.x.x.x/24",
             "license_file_name": "license.lic",
                "physnet_VLAN_range_mappings": ["physnet1:1000:1999","physnet2:2000:2020"],
                "pci_vendor_id": ["8086:10ca","8086:10ed"]
            }
        }
    ]
}</codeblock>
      </p>
    </section>
    <section id="environment">
      <title id="env-json">Modify the <codeph>environment.json</codeph> file</title>
      <p>Modify the <codeph>environment.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname></codeph> directory of the lifecycle manager.
        Configure the VLANs and network addresses as appropriate for your environment. Set the
        following for the CLM, CAN, and BLS
        network:<codeblock>"cidr": 
          "start-address": </codeblock> The three controller
        nodes should have CLM, CAN, EXT, BLS on eth0 and TUL on eth1.
        <!--Hiding for RC0 
              <p>The two compute nodes should have CLM, EXT, BLS on eth0 and TUL on eth1.</p>
           -->
      </p>
      <p>
        <b>Example:</b>
      </p>
      <p>In the following example, the IP addresses have been masked for security: </p>
      <p><codeblock>{
    "product": {
        "version": 1
    },

    "node-type": [
        {
            "name": "CCN",
            "interface-map": [
                {
                    "name": "INTF0",
                    "ethernet-port-map": {
                        "interface-ports": [ "eth0" ]
                    },
                    "logical-network-map": [
                        {
                            "name": "CLM",
                            "type": "vlan",
                            "segment-id": "102",
                            "network-address": {
                                "cidr": "10.x.x.x/24",
                                "start-address": "10.x.x.x",
                                "gateway": "10.x.x.x"
                            }
                        },
                        {
                            "name": "CAN",
                            "type": "vlan",
                            "segment-id": "103",
                            "network-address": {
                                "cidr": "10.x.x.x/24",
                                "start-address": "10.x.x.x"
                            }
                        },
                        {
                            "name": "BLS",
                            "type": "vlan",
                            "segment-id": "71",
                            "network-address": {
                                "cidr": "172.x.x.x/24",
                                "start-address": "172.x.x.x"
                            }
                        }
                    ]
                }
            ]
        }
    ]
}
</codeblock>
        <b>NOTE:</b> The configuration processor assigns the first address of the CLM address range
        to itself for serving python and debian repositories. Make sure that you set the first IP
        address of the CLM range for the eth2 (CLM) address of the HLM host.</p>
    </section>
    <section id="def-json"><title>Modify the <codeph>definition.json</codeph></title>
        <p>Modify the <codeph>definition.json</codeph> file in the /var/hlm/clouds/&lt;cloudname>
        directory of the lifecycle manager: </p>
      <ol>
        <li>Set the number of compute systems to 2.
          <codeblock>"count": 2, //number of computes in the resource pool</codeblock></li>
        <li>Update the <codeph>ansible-vars</codeph> section with all the information based on
          your setup.</li>
        <li>Make sure you have two NTP entries in the <codeph>upstream_ntp_servers</codeph> fields
          in the <codeph>definition.json</codeph> file as seen in the following example. If you have
          only one NTP server in your environment, specify the same NTP server twice. Example:
          <codeblock>{
  "cloud": {
    "environment": "environment.json",
    "name": "tb1b36newdenver",
    "network-config": ".hos/lnet-control-data.json",
    "nickname": "base",
    "properties": "vars",
    "server-config": "nodes.json"
  },
  "control-planes": [
    {
      "file": ".hos/ccp-1x3-ss.json",
      "resource-nodes": []
    }
  ],
  "failure-zones": [
    {
      "name": "fz1"
    },
    {
      "name": "fz2"
    },
    {
      "name": "fz3"
    }
  ],
  "product": {
    "version": 1
  }
}          </codeblock></li>
      </ol>
    </section>    
    
    <section id="verify-json">
      <title>Verify the JSON files</title>
      <p>After editing the JSON files, validate each JSON file to make sure there are no syntax
        errors using the tool of your preference. For example, using the Python json.tool: </p>
      <codeblock>python -m json.tool &lt;filename>.json</codeblock>
    </section>
    <section conref="CGH-2-install-sacramento.dita#topic10581c2is/cobbler"/>
    <section conref="CGH-2-install-sacramento.dita#topic10581c2is/conref-create-cloud"/>
    <section conref="CGH-2-install-sacramento.dita#topic10581c2is/conref-back-end" id="storage"/>
    <section conref="CGH-2-install-sacramento.dita#topic10581c2is/providernet-patch"/>
    <section conref="CGH-2-install-sacramento.dita#topic10581c2is/conref-config-proc"/>
    <section conref="CGH-2-install-sacramento.dita#topic10581c2is/conref-hdeploy"/>
    <section conref="CGH-2-install-sacramento.dita#topic10581c2is/conref-ldap-keystone" id="ldap3"/>
    
    <section id="next-step">
      <title>Next Step</title>
      <p><xref href="carrier-grade-install-launch-horizon-denver.dita#topic10581cgilhd">Launching
          the Horizon Interface</xref></p>
      
    </section>
  </body>
</topic>
