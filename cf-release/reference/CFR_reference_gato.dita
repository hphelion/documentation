<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="reference_cd3_bs3_rt">
  <title><keyword keyref="adminUtil"/> Command Reference</title>
  <shortdesc><filepath><keyword keyref="adminUtil"/></filepath> is the <keyword keyref="prodLong"/>
    administration utility.</shortdesc>
  <prolog>
    <metadata>
      <othermeta name="author" content="Michael Khmelnitsky"/>
    </metadata>
  </prolog>
  <refbody>
    <section>
      <title>Usage</title>
      <codeblock><keyword keyref="adminUtilCLI"/> <varname>[global options]</varname> <varname>command</varname> <varname>[command options]</varname> <varname>[arguments...]</varname></codeblock>
    </section>
    <section>
      <title>Global Options</title>
      <dl>
        <dlentry>
          <dt><filepath>--help</filepath>, <filepath>-h</filepath></dt>
          <dd>Show help.</dd>
        </dlentry>
        <dlentry>
          <dt><filepath>--version</filepath>, <filepath>-v</filepath></dt>
          <dd>Print the version.</dd>
        </dlentry>
      </dl>
    </section>
    <section>
      <title>Commands</title>
      <dl>
        <dlentry>
          <dt><filepath>api</filepath></dt>
<<<<<<< HEAD
          <dd>Set  a <keyword keyref="prodShort"/> config store.</dd>
=======
          <dd>Set <keyword keyref="adminUtilCLI"/> to use the specified <keyword keyref="prodShort"
            /> config store for subsequent operations. For
            example:<codeblock>$ <keyword keyref="adminUtilCLI"/> api http://hcf-consul-server.hcf:8501</codeblock></dd>
>>>>>>> de2d2bc... Recreating branch without old cruft in it
        </dlentry>
        <dlentry>
          <dt><filepath>config</filepath></dt>
          <dd>Manipulate configuration values of <keyword keyref="prodShort"/> components.<ul
              id="ul_x4n_dtj_rt">
              <li><xref href="#reference_cd3_bs3_rt/config_get" format="dita"><filepath>config
                    get</filepath></xref></li>
              <li><xref href="#reference_cd3_bs3_rt/config_set" format="dita"><filepath>config
                    set</filepath></xref></li>
            </ul></dd>
        </dlentry>
        <dlentry>
          <dt><filepath>help</filepath></dt>
          <dd>Show a list of commands or help for one command</dd>
        </dlentry>
        <dlentry>
          <dt><filepath>role</filepath></dt>
          <dd>Manage node roles. By default, uses the <codeph>$DOCKER_HOST</codeph> environment
            variable to determine the docker server endpoint. Use the <filepath>--node</filepath>
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
            flag to stop <varname>&lt;role></varname> container on a specific node.<ul
=======
            flag to stop <xref href="#available_role_containers" format="dita"
              >a <varname>&lt;role></varname> container</xref> on a specific node.<ul
>>>>>>> de2d2bc... Recreating branch without old cruft in it
=======
            flag to stop a <varname>&lt;role></varname> container on a specific node.<ul
>>>>>>> d3f9032... Removed broken xrefs
=======
            flag to stop a <xref href="#reference_cd3_bs3_rt/available_role_containers"
              format="dita"><varname>&lt;role></varname> container</xref> on a specific node.<ul
>>>>>>> 4619deb... Finally fixed the xrefs
              id="ul_blk_nhj_rt">
              <li><xref href="#reference_cd3_bs3_rt/role_start" format="dita"><filepath>role
                    start</filepath></xref></li>
              <li><xref href="#reference_cd3_bs3_rt/role_stop_restart" format="dita"><filepath>role
                    stop</filepath></xref></li>
              <li><xref href="#reference_cd3_bs3_rt/role_stop_restart" format="dita"><filepath>role
                    restart</filepath></xref></li>
            </ul></dd>
        </dlentry>
      </dl>
      <dl>
        <dlentry>
          <dt><filepath>status</filepath></dt>
          <dd>Retrieve the status of a <keyword keyref="prodShort"/> cluster.<dl>
              <dlentry>
                <dt><filepath>--all</filepath>, <filepath>-a</filepath></dt>
<<<<<<< HEAD
                <dd>Show each individual process.</dd>
=======
                <dd>Show each individual process, for
                  example:<codeblock>$ <keyword keyref="adminUtilCLI"/> status --all</codeblock></dd>
>>>>>>> de2d2bc... Recreating branch without old cruft in it
              </dlentry>
            </dl></dd>
        </dlentry>
      </dl>
    </section>
    <section>
      <title>Command Options</title>
      <dl>
        <dlentry id="config_get">
          <dt><filepath><keyword keyref="adminUtilCLI"/> config get <varname>[command
                options]</varname>
              <varname>[arguments...]</varname></filepath></dt>
<<<<<<< HEAD
          <dd>Look up the current and default values of a configuration.<dl>
=======
          <dd>Look up the current and default values of a configuration, for example: <codeblock>$ <keyword keyref="adminUtilCLI"/> config get nats/user</codeblock><codeblock>$ <keyword keyref="adminUtilCLI"/> config get nats/password</codeblock><codeblock>$ <keyword keyref="adminUtilCLI"/> config get nats/machines</codeblock><codeblock>$ <keyword keyref="adminUtilCLI"/> config get system_domain</codeblock><dl>
>>>>>>> de2d2bc... Recreating branch without old cruft in it
              <dlentry>
                <dt><filepath>--quiet</filepath>, <filepath>-q</filepath></dt>
                <dd>Output only the config values.</dd>
              </dlentry>
              <dlentry>
                <dt><filepath>--role</filepath>, <filepath>-r</filepath></dt>
<<<<<<< HEAD
<<<<<<< HEAD
                <dd>Get a specific role's config variable.</dd>
=======
                <dd>Get <xref href="#available_role_containers" format="dita">a specific
=======
                <dd>Get <xref href="#reference_cd3_bs3_rt/available_role_containers" format="dita">a specific
>>>>>>> 4619deb... Finally fixed the xrefs
                    role's</xref> config variable, for example:
                  <codeblock>$ <keyword keyref="adminUtilCLI"/> config get --role uaa uaa/jwt/signing_key</codeblock><codeblock>$ <keyword keyref="adminUtilCLI"/> config get --role hm9000 hm9000/url</codeblock></dd>
>>>>>>> de2d2bc... Recreating branch without old cruft in it
              </dlentry>
            </dl></dd>
        </dlentry>
        <dlentry id="config_set">
          <dt><filepath><keyword keyref="adminUtilCLI"/> config set <varname>[command
                options]</varname>
              <varname>[arguments...]</varname></filepath></dt>
          <dd>Set the current value of a configuration.<dl>
              <dlentry>
                <dt><filepath>--role</filepath>, <filepath>-r</filepath></dt>
<<<<<<< HEAD
<<<<<<< HEAD
                <dd>Set a specific role's configuration.</dd>
=======
                <dd>Set <xref href="#available_role_containers" format="dita">a
=======
                <dd>Set <xref href="#reference_cd3_bs3_rt/available_role_containers" format="dita">a
>>>>>>> 4619deb... Finally fixed the xrefs
                    specific role's</xref> configuration.</dd>
>>>>>>> de2d2bc... Recreating branch without old cruft in it
              </dlentry>
              <dlentry>
                <dt><filepath>--file</filepath>, <filepath>-f</filepath></dt>
                <dd>Read values from file or from stdin if followed by dash (-).</dd>
              </dlentry>
            </dl></dd>
        </dlentry>
      </dl>
      <dl>
        <dlentry id="role_start">
          <dt><filepath><keyword keyref="adminUtilCLI"/> role start <varname>[command
                options]</varname>
              <varname>[arguments...]</varname></filepath></dt>
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
          <dd>Start the <varname>&lt;role></varname> container.<dl>
=======
          <dd>Start the <xref href="#reference_cd3_bs3_rt/available_role_containers" format="dita"
                ><varname>&lt;role></varname> container</xref>.<dl>
>>>>>>> 4619deb... Finally fixed the xrefs
              <dlentry>
                <dt><filepath>--node</filepath>, <filepath>-n</filepath></dt>
                <dd>Set  the Docker server endpoint. Schemes and port number are required. Supported
=======
          <dd>Start the <xref href="#available_role_containers" format="dita"
                ><varname>&lt;role></varname> container</xref>.<dl>
=======
          <dd>Start the <varname>&lt;role></varname> container.<dl>
>>>>>>> d3f9032... Removed broken xrefs
              <dlentry>
                <dt><filepath>--node</filepath>, <filepath>-n</filepath></dt>
                <dd>Set the Docker server endpoint. Schemes and port number are required. Supported
>>>>>>> de2d2bc... Recreating branch without old cruft in it
                  schemes: <codeph>tcp://</codeph>
                  <codeph>unix://</codeph>
                  <codeph>http://</codeph>
                  <codeph>https://</codeph></dd>
              </dlentry>
            </dl></dd>
        </dlentry>
        <dlentry id="role_stop_restart">
          <dt><filepath><keyword keyref="adminUtilCLI"/> role stop <varname>[command
                options]</varname>
              <varname>[arguments...]</varname></filepath></dt>
          <dt><filepath><keyword keyref="adminUtilCLI"/> role restart <varname>[command
                options]</varname>
              <varname>[arguments...]</varname></filepath></dt>
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
          <dd>Stop or restart the <varname>&lt;role></varname> container.<!-- The available
              <varname>&lt;role></varname> containers are: <ul id="ul_y3t_nrz_5t">
              <li>uaa</li>
              <li>stats</li>
              <li>runner</li>
              <li>router</li>
              <li>postgres</li>
              <li>nats</li>
              <li>loggregator_trafficcontroller</li>
              <li>hm9000</li>
              <li>ha_proxy</li>
              <li>etcd</li>
              <li>doppler</li>
              <li>consul</li>
              <li>clock_global</li>
              <li>api_worker</li>
              <li>api</li>
            </ul>--><dl>
=======
          <dd>Stop or restart the <xref href="#available_role_containers"
              format="dita"><varname>&lt;role></varname> container</xref>.<dl>
>>>>>>> de2d2bc... Recreating branch without old cruft in it
=======
          <dd>Stop or restart the <varname>&lt;role></varname> container.<dl>
>>>>>>> d3f9032... Removed broken xrefs
=======
          <dd>Stop or restart the <xref href="#reference_cd3_bs3_rt/available_role_containers"
              format="dita"><varname>&lt;role></varname> container</xref>.<dl>
>>>>>>> 4619deb... Finally fixed the xrefs
              <dlentry>
                <dt><filepath>--node</filepath>, <filepath>-n</filepath></dt>
                <dd>Set the Docker server endpoint. Schemes and port number are required. Supported
                  schemes: <codeph>tcp://</codeph>
                  <codeph>unix://</codeph>
                  <codeph>http://</codeph>
                  <codeph>https://</codeph></dd>
              </dlentry>
              <dlentry>
                <dt><filepath>--timeout</filepath>, <filepath>-t "0"</filepath></dt>
                <dd>Set the number of seconds to wait before killing the container.</dd>
              </dlentry>
            </dl></dd>
        </dlentry>
      </dl>
<<<<<<< HEAD
    </section>
=======
      <draft-comment author="michaelk">
        <ul id="ul_rg4_tmm_xt">
          <li>To find descriptions for everything except <filepath>doppler</filepath> and
              <filepath>loggregator_trafficcontroller</filepath>, <xref
              href="https://github.com/cloudfoundry/cf-release/tree/v217/jobs" format="html"
              scope="external">go here</xref> and drill down to
              <uicontrol>&lt;component>/spec</uicontrol>.</li>
          <li>To find a description of <filepath>loggregator_trafficcontroller</filepath>, <xref
              href="https://github.com/cloudfoundry/loggregator/blob/lgr-cf-v217/README.md"
              format="html" scope="external">go here</xref>.</li>
          <li>To find a description of <filepath>doppler</filepath>, <xref
              href="https://github.com/cloudfoundry/loggregator/tree/lgr-cf-v217#architecture"
              format="html" scope="external">go here</xref>.</li>
          <li>To find a description for Consul, <xref href="https://consul.io/intro/index.html"
              format="html" scope="external">go here</xref>.</li>
        </ul>
      </draft-comment>
    </section>
    <section id="available_role_containers">
      <title>Available <varname>&lt;role></varname>
        Containers</title>
    </section>
    <table frame="all" rowsep="1" colsep="1" id="table_cls_wfz_wt">
      <tgroup cols="2">
        <colspec colname="c1" colnum="1" colwidth="1*"/>
        <colspec colname="c2" colnum="2" colwidth="2.5*"/>
        <thead>
          <row>
            <entry><varname>&lt;role></varname> Container</entry>
            <entry>Description</entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry><filepath>api</filepath><draft-comment author="michaelk"
              >cloud_controller_ng</draft-comment></entry>
            <entry>The Cloud Controller provides the primary Cloud Foundry API to the Cloud Froundry
              command-line interface (CLI). The Cloud Controller uses a database to keep tables for
              organizations, spaces, applications, services, service instances, user roles, so on.
              Typically. multiple instances of Cloud Controller are load balanced.</entry>
          </row>
          <row>
            <entry><filepath>api_worker</filepath><draft-comment author="michaelk"
              >cloud_controller_worker</draft-comment></entry>
            <entry>The Cloud Controller worker processes background tasks.</entry>
          </row>
          <row>
            <entry><filepath>clock_global</filepath><draft-comment author="michaelk"
              >cloud_controller_clock</draft-comment></entry>
            <entry>The Cloud Controller clock periodically schedules Cloud Controller clean-up tasks
              for application usage events, audit events, failed jobs, and so on. <note>Only one
                clock instance is necessary.</note></entry>
          </row>
          <row>
            <entry><filepath>consul</filepath></entry>
            <entry>Consul is a tool for discovering and configuring services in an infrastructure.
              It provides service discovery, health checking, and key/value storage and supports
              multiple datacenters.</entry>
          </row>
          <row>
            <entry><filepath>doppler</filepath></entry>
            <entry>Doppler is a component of Loggregator that is responsible for gathering logs from
              Metron agents. Doppler stores the logs in temporary buffers in addition to forwarding
              them to third-party syslog drains.</entry>
          </row>
          <row>
            <entry><filepath>etcd</filepath><draft-comment author="michaelk"
              >etcd_metrics_server</draft-comment></entry>
            <entry>The ETCD Metrics Server, when collocated with the ETCD server, collects and
              exposes runtime statistics for the stats collector.</entry>
          </row>
          <row>
            <entry><filepath>ha_proxy</filepath></entry>
            <entry>The HAProxy server can terminate SSL in front of routers. <note>Each HAProxy
              instance should point to multiple routers.</note></entry>
          </row>
          <row>
            <entry><filepath>hm9000</filepath></entry>
            <entry>HM9000 periodically compares the list of expected running applications as
              specified by the Cloud Controller against a list of actually running applications as
              reported by the DEAs. The HM9000 attempts to reconcile any differences between the two
              lists.</entry>
          </row>
          <row>
            <entry><filepath>loggregator_trafficcontroller</filepath></entry>
            <entry>Loggregator is the user application logging subsystem of Cloud Foundry. It allows
              users to tail their application logs, dump a set of recent application logs,
              continually drain their application logs to third-party log archive and analysis
              services, and access the firehose which includes the combined stream of logs from all
              apps, in addition to metrics data from Cloud Foundry components.</entry>
          </row>
          <row>
            <entry><filepath>nats</filepath></entry>
            <entry>The NATS server provides publish-subscribe messaging system for the Cloud
              Controller, the DEA, HM9000, and other Cloud Foundry components.</entry>
          </row>
          <row>
            <entry><filepath>postgres</filepath></entry>
            <entry>The Postgres server provides a single instance Postgres database that can be used
              with the Cloud Controller or the UAA. </entry>
          </row>
          <row>
            <entry><filepath>router</filepath><draft-comment author="michaelk"
              >gorouter</draft-comment></entry>
            <entry>The router maintains a list of live routes for the applications running on each
              DEA. The router load-balances requests (based on their host header) between each
              application instance registered for a specific route. It must be behind a load
              balancer that can terminate SSL connections.</entry>
          </row>
          <row>
            <entry><filepath>runner</filepath><draft-comment author="michaelk"
              >dea_next</draft-comment></entry>
            <entry>The Droplet Execution Agent (DEA) is responsible for running customer
              applications and maintaining associated routes as live. It also periodically
              advertises its capacity to Cloud Controllers and accepts requests to start and stop
              applications.</entry>
          </row>
          <row>
            <entry><filepath>stats</filepath><draft-comment author="michaelk"
              >collector</draft-comment></entry>
            <entry>The stats collector discovers the various components on the message bus and
              queries their <codeph>/healthz</codeph> and <codeph>/varz</codeph> interfaces. The
              collected metric data is published to collector plugins such as OpenTSDB, AWS
              CloudWatch, and DataDog.</entry>
          </row>
          <row>
            <entry><filepath>uaa</filepath></entry>
            <entry>User Account and Authentication (UAA) is the identity management service for
              Cloud Foundry. Its primary role is acting as an OAuth2 provider, issuing tokens for
              client applications when they act on behalf of Cloud Foundry users. UAA can also
              authenticate users with their Cloud Foundry credentials, and it can act as an SSO
              service using Cloud Foundry (or other) credentials. UAA has endpoints for managing
              user accounts and for registering OAuth2 clients and other management
              functions.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>
>>>>>>> de2d2bc... Recreating branch without old cruft in it
  </refbody>
</reference>
