<?xml version="1.0" encoding="UTF-8"?>


<!DOCTYPE topic   PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >


<topic id="psa1410807838150" xml:lang="en-us">


<!-- Modification History


        


-->


    <title >Restore Procedure</title>


    <shortdesc >You can restore a HP Helion OpenStack Carrier Grade cluster from


        available system data backup files to bring it back to the operational state it was when the


        backup procedure took place.</shortdesc>


    <prolog>


        <author >Pedro Sanchez</author>


    </prolog>


    <body>


        <section id="section_N1001F_N1001C_N10001" >


            <p >Restoring a HP Helion OpenStack Carrier Grade cluster from a set of


                backup files is done by restoring one host at a time, starting with the controllers,


                then  the compute nodes.</p>


        </section>


        <section id="section_N1002D_N1001F_N10001" >


            <p >Before you start the restore procedure you must ensure the following conditions are


                in place:</p>


            <ul id="ul_rfq_qfg_mp">


                <li >


                    <p >All cluster hosts must be powered down, just as if they were


                        going to be initialized anew.</p>


                </li>


                <li >


                    <p >All backup files are accessible from a USB flash drive


                        locally attached to the controller where the restore operation takes place


                            (<b >controller-0</b>).</p>


                </li>


                <li >


                    <p >You have the HP Helion OpenStack Carrier Grade installation image available on


                        a USB flash drive, just as when the software was installed the first time.


                        It is mandatory that you use the exact same version of the software used


                        during the original installation, otherwise the restore procedure will


                        fail.</p>


                </li>


                <li >


                    <p >The restore procedure requires all hosts but <b >controller-0</b> to boot over the internal


                        management network using the PXE protocol, just as it was done during the


                        initial software installation. Ideally, you cleaned up all hard drives in


                        the cluster, and the old boot images are no longer present; the hosts then


                        default to boot from the network when powered on. If this is not the case,


                        you must configure each host manually for network booting right after this


                        exercise asks you to power them on.</p>


                </li>


            </ul>


        </section>


        <ol>


            <li id="li_N1002B_N10028_N1001C_N10001" >


                <p >Install the HP Helion OpenStack Carrier Grade software on


                        <b >controller-0</b> from the USB flash drive.</p>


                


                    <p >Refer to the <cite >HP Helion OpenStack Carrier Grade Software Installation Guide</cite> for


                        details.</p>


                


                


                    <p >When the software installation is complete, you should be


                        able to log in using the host's console and the web administration


                        interface.</p>


                


            </li>


            <li id="li_N10085_N1005F_N1001F_N10001" >


                <p >Log in to the console as user <b >wrsroot</b> with password


                        <b >wrsroot</b>.</p>


            </li>


            <li id="li_N10096_N1005F_N1001F_N10001" >


                <p >Ensure the backup files are available to the controller.</p>


                


                    <p >Plug the USB flash drive containing the backup files into the


                        system. The USB flash drive is mounted automatically. Use the command


                            <i>df</i> to list the mount points.</p>


                    <p >The following ol assume that the backup files are


                        available from a USB flash drive mounted in <filepath >/media/wrsroot</filepath> in the directory <filepath >backups</filepath>.</p>


                


            </li>


            <li id="li_N100CB_N1005F_N1001F_N10001" >


                <p >Update the controller's software to the previous patching


                    level.</p>


                


                    <p >When restoring the system configuration, the current


                        software version on the controller, at this time, the original software


                        version from the ISO image, is compared against the version available in the


                        backup files. They differ if the latter includes patches applied to the


                        controller's software at the time the backup files were created. If


                        different, the restore process automatically applies the patches and forces


                        an additional reboot of the controller to make them effective.</p>


                    <p >The following is the command output if patching is


                        necessary:</p>


                


                


                    <codeblock ><systemoutput >$ </systemoutput><userinput >sudo config_controller --restore-system \


/media/wrsroot/backups/backup_20140918_system.tgz</userinput>


<systemoutput >Restoring system (this will take several minutes):


li  4 of 19 [#########                                    ] [21%]


This controller has been patched. A reboot is required.


After the reboot is complete, re-execute the restore command.


Enter 'reboot' to reboot controller:</systemoutput></codeblock>


                    <p >You must enter <b >reboot</b> at the prompt as requested. Once the controller is


                        back, log in as user <b >wrsroot</b> as


                        before to continue.</p>


                


                


                    <p >After the reboot, you can verify that the patches were applied, as


                        illustrated in the following example:</p>


                    <codeblock ><systemoutput >$ </systemoutput><userinput >sudo wrs-patch query</userinput>


<systemoutput >        Patch ID          Repo State  Patch State


========================  ==========  ===========


COMPUTECONFIG             Available       n/a


LIBCUNIT_CONTROLLER_ONLY   Applied        n/a</systemoutput></codeblock>


                


            </li>


            <li id="li_N10125_N1006E_N1001F_N10001" >


                <p >Restore the system configuration.</p>


                


                    <p >The controller's software is up to date now, at the same


                        stage it was when the backup operation was executed. The restore procedure


                        can be invoked again, and should run without interruptions.</p>


                


                


                    <codeblock ><systemoutput >$ </systemoutput><userinput >sudo config_controller --restore-system \


/media/wrsroot/backups/backup_20140918_system.tgz</userinput>


<systemoutput >Restoring system (this will take several minutes):


li 19 of 19 [##########################] [100%]


Restoring node states (this will take several minutes):





Locking nodes:li 10 of 10 [#############################################] [100%]


Powering-off nodes:


li 10 of 10 [#############################################] [100%]





System restore complete </systemoutput></codeblock>


                


            </li>


            <li id="li_N100ED_N1005F_N1001F_N10001" >


                <p >Authenticate to the system as Keystone user <b >admin</b>.</p>


                


                    <p >Source the <b >admin</b>


                        user environment as follows:</p>


                


                


                    <codeblock ><systemoutput >$ </systemoutput><userinput >cd; source /etc/nova/openrc</userinput></codeblock>


                


            </li>


            <li id="li_N10113_N1005F_N1001F_N10001" >


                <p >Restore the system images.</p>


                


                    <codeblock ><systemoutput >~(keystone_admin)$ </systemoutput><userinput >sudo config_controller --restore-images \


/media/wrsroot/backups/backup_20140918_images.tgz</userinput>


<systemoutput >li 2 of 2 [#############################################] [100%]





Images restore complete</systemoutput></codeblock>


                


                


                    <p >This li assumes that the backup file resides on the


                        attached USB drive. If instead you copied the image backup file to the


                            <filepath >/opt/backups</filepath> directory, for example using the


                            <i>scp</i> command, you can remove it now.</p>


                


            </li>


            <li id="li_N10135_N1005F_N1001F_N10001" >


                <p >Restore <b >controller-1</b>.</p>


                <ol id="ol_wt4_5dh_mp">


                    <li >


                        <p >List the current state of the hosts.</p>


                        


                            <codeblock ><systemoutput >~(keystone_admin)$ </systemoutput><userinput >system host-list</userinput>


<systemoutput >+----+--------------+-------------+----------------+-------------+--------------+


| id | hostname     | personality | administrative | operational | availability |


+----+--------------+-------------+----------------+-------------+--------------+


| 1  | controller-0 | controller  | unlocked       | enabled     | available    |


| 2  | controller-1 | controller  | locked         | disabled    | offline      |


| 3  | compute-0    | compute     | locked         | disabled    | offline      |


+----+--------------+-------------+----------------+-------------+--------------+</systemoutput></codeblock>


                        


                    </li>


                    <li >


                        <p >Power on the host.</p>


                        


                            <p >Remember to ensure that the host boots from the


                                network and not from an old disk image that might still be present


                                on its hard drive.</p>


                        


                    </li>


                    <li >


                        <p >Unlock the host.</p>


                        


                            <codeblock ><systemoutput >~(keystone_admin)$ </systemoutput><userinput >system host-unlock 2</userinput>


<systemoutput >+-----------------+--------------------------------------+


| Property        | Value                                |


+-----------------+--------------------------------------+


| action          | none                                 |


| administrative  | locked                               |


| availability    | online                               |


| ...             | ...                                  |


| uuid            | 5fc4904a-d7f0-42f0-991d-0c00b4b74ed0 |


+-----------------+--------------------------------------+</systemoutput></codeblock>


                        


                    </li>


                    <li >


                        <p >Verify the new state of the hosts.</p>


                        


                            <codeblock ><systemoutput >~(keystone_admin)$ </systemoutput><userinput >system host-list</userinput>


<systemoutput >+----+--------------+-------------+----------------+-------------+--------------+


| id | hostname     | personality | administrative | operational | availability |


+----+--------------+-------------+----------------+-------------+--------------+


| 1  | controller-0 | controller  | unlocked       | enabled     | available    |


| 2  | controller-1 | controller  | unlocked       | enabled     | available    |


| 3  | compute-0    | compute     | locked         | disabled    | offline      |


+----+--------------+-------------+----------------+-------------+--------------+</systemoutput></codeblock>


                        


                    </li>


                </ol>


                


                    <p >The unlocking operation forces a reboot of the host, which


                        is then initialized with the corresponding image available from the system


                        backup.</p>


                    <p >You must wait for the host to become enabled and available


                        before proceeding to the next li.</p>


                


            </li>


            <li id="li_N101ED_N1005F_N1001F_N10001" >


                <p >Restore the compute nodes, one at a time.</p>


                


                    <p >You restore these hosts following the same procedure used to


                        restore <b >controller-1</b>.</p>


                


                


                    <p >The state of the hosts when the restore operation is


                        complete is as follows:</p>


                    <codeblock ><systemoutput >~(keystone_admin)$ </systemoutput><userinput >system host-list</userinput>


<systemoutput >+----+--------------+-------------+----------------+-------------+--------------+


| id | hostname     | personality | administrative | operational | availability |


+----+--------------+-------------+----------------+-------------+--------------+


| 1  | controller-0 | controller  | unlocked       | enabled     | available    |


| 2  | controller-1 | controller  | unlocked       | enabled     | available    |


| 3  | compute-0    | compute     | unlocked       | enabledd    | available    |


+----+--------------+-------------+----------------+-------------+--------------+</systemoutput></codeblock>


                    <p >As each compute node is restored, the original instances at the time the


                        backups were done are started automatically.</p>


                


            </li>


        </ol>





            <p >The system is fully restored. The state of the system, including


                virtual machines, is identical to the state the cluster was in when the backup


                procedure took place.</p>


            <p >Remember however, that passwords for local user accounts must be


                restored manually since they are not included as part of the backup and restore


                procedures. See <xref href="jow1404333563834.xml" /> for additional


                information.</p>


        


    </body>


</topic>


