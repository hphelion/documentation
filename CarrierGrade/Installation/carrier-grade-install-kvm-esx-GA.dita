<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic10581">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 1.1: Installing the KVM + ESX<tm
      tmtype="reg"/> Deployment</title>
  <prolog>
    <metadata>
      <othermeta name="layout" content="default"/>
      <othermeta name="product-version" content="HP Helion Openstack Carreir Grade 1.1"/>
      <othermeta name="role" content="Storage Administrator"/>
      <othermeta name="role" content="Storage Architect"/>
      <othermeta name="role" content="Michael B"/>
      <othermeta name="product-version1" content="HP Helion Openstack Carreir Grade 1.1"/>
    </metadata>
  </prolog>
  <body>
    <p>After the <xref href="carrier-grade-install-hlm-vm.dita#topic10581">Helion Lifecycle
        Management (HLM) VM is installed</xref>, the next task in installing the <xref
        href="carrier-grade-install-pb-overview.dita#topic1925/install-option">KVM + ESX
        deployment</xref> is to deploy the HP Helion OpenStack cloud and install the HP Distributed
      Cloud Networking (DCN) components. You must install the VMware ESX<tm tmtype="reg"/> compute
      proxy.</p>
    <p><b>Note:</b> If you are installing the KVM-only deployment, see <xref
        href="carrier-grade-install-kvm.dita#topic10581">Installing the KVM Deployment</xref>.</p>
    <section id="prereqs">
      <p>Before starting the HP Helion OpenStack Carrier Grade installation, the VMware vSphere
        application and HP Distributed Cloud Networking components must be fully installed and
        functioning:</p>
      <p><b>DCN Requirements</b></p>
      <p>HP Helion Distributed Cloud Networking (DCN) must be deployed before starting the cloud
        deployment. Please refer to DCN documentation for details. </p>
      <p>For a quick overview of the DCN components, see <xref
          href="carrier-grade-technical-overview-esx.dita#topic3485/dcn">KVM + ESX Deployment
          Architecture Reference</xref></p>
      <p>In production cloud deployments:</p>
      <ul id="ul_hnw_pry_zs">
        <li>DCN should be deployed in HA mode (with 2 VSCs, 2 VRS-Gs).</li>
        <li>VSD should be deployed in HA mode (3+1 VSDs). </li>
        <li>Configure appropriate entries for VSD into DNS so that the SRV record for
            <codeph>_xmpp-client</codeph> resolves. This is required for DCN Cloud. Refer to  the
          DCN documentation examples of <codeph>BIND</codeph> commands and instructions for testing
          the DNS configuration. </li>
        <li>VSD should be assigned a DCM IP accessible for cloud deployment. Refer to the <xref
            href="carrier-grade-technical-overview-esx.dita#topic3485">architecture
          diagram</xref>.</li>
        <li>Verify that the IP address and domain name of VSD can be accessed using PING and DIG
          commands. </li>
        <li>Make sure all the VSD services are in PASS state.</li>
        <li><xref href="#topic10581/vsd" format="dita">Apply the VSD license</xref> based on single
          or clustered VSD setup. </li>
        <li><xref href="#topic10581" format="dita">Create the required VSD users</xref>.</li>
      </ul>
      <p>Optionally, you can integrate LDAP into the DCN environment.</p>
    </section>
    <section>
      <p><b>ESXi requirements</b></p>
      <p>ESX should be deployed before starting the cloud deployment. For more information, refer to the VMware product
        documentation.</p>
      <p>In production cloud deployments:</p>
      <ul id="ul_ejv_35y_zs">
        <li>ESX Cluster with two (2) or more hosts installed with ESXi 5.5 U2 and above</li>
        <li>ESX Cluster should have HA, DRS &amp; Vmotion Enabled</li>
        <li>ESX Cluster should have Shared Storag</li>
        <li>The CLM and TUL networks must be available to all ESX hosts VMNICs</li>
      </ul>
      <p><b>ESX Network Configuration Requriements</b></p>
      <p>In production cloud deployments, you will need to create the following three (3) port
        groups.</p>
      <ul id="ul_mdl_r5y_zs">
        <li>Cloud Lab Management (CLM VLAN)</li>
        <li>DVRS Datapath (TUL VLAN)</li>
        <li>Trunk Network (All VLANS)</li>
      </ul>
      <p>The names are case sensitive.</p>
      <p>In the VMware vCenter client, the port group list should appear as follows:</p>
      <p><image href="../../media/CGH-install-ESX-ports.png" id="image_os2_2vy_zs"/></p>
    </section>
    <section>
      <title>Deploy the VMware vSphere Distributed Switch</title>
      <p>The VMware vSphere Distributed Switch you obtained from VMware must be installed. For more
        information, see <xref href="carrier-grade-install-vsphere-switch.dita#topic10581">Deploying
          the VMware vSphere Distributed Switch</xref>. </p>
    </section>
    <section>
      <title>Deploy the VRSvApp</title>
      <p>The VRSvApp you obtained from DCN must be installed. For more information, see <xref
          href="carrier-grade-install-vsvapp.dita#topic10581">Deploying the VRS vApp</xref>. </p>
    </section>
    <section id="vsd">
      <title>Apply the VSD license</title>
      <p>Before you start, make sure the VSD node is installed by logging into the VSD VM using SSH
        and running the following command:</p>
      <codeblock>service vsd status</codeblock>
      <p>You should see the status as below from VSD VM.</p>
      <p>
        <image href="../../media/CGH-install-virsh-vsd.png" width="400"/></p>
    </section>
    <!--<section id="vsd-performance-workaround"> <title>VSD Performance workaround</title>
<p>By default VSD has only 8G memory. For better performance behavior, you can update the VSD memory to 16G.</p>
<ol>
<li>From HLM host, execute the following command to power down the VSD node:
          VM:<codeblock>virsh destroy vsd</codeblock></li>
<li>Execute the following command to edit the memory setting in the VSD XML
          file:<codeph>virsh edit vsd 

Current value 
    &lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;8388608&lt;/currentMemory&gt;

Change to 
    &lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;16777216&lt;/currentMemory&gt;
</codeph></li>
<li>Save the value<codeph>virsh save vsd
</codeph></li>
<li>Use the following command to start the
            VM:<codeph>virsh start vsd
</codeph><p>It can take 15 minutes or
            more to sync with NTP and to get all the other VSD services up.</p></li>
</ol>
</section>-->
    <p>You should have recevied a license file when you purchased DCN. You need to apply that
      license on the HLM host. </p>
    <p>To apply the VSD license, on the HLM host:</p>
    <ol>
      <li>If you do not have the license file, through the DCN post-sales process, you received a
        confirmation email. Use a browser and go to the link in the email to obtain the
        license.</li>
      <li>After you have obtained the license, use a browser to navigate to the VSD dashboard using
        the VSD URI.</li>
      <li>In the login page, enter the default
        credentials:<codeblock>User Name: Csproot 
Password: csproot 
Org: csp 
VSD Server : auto </codeblock></li>
      <li>From VSD Dashboard, click the <b>Open VSP Configuration</b> tab on the top right corner of
        the dashboard.</li>
      <li>Click the <b>Licenses</b> tab and click <b>+</b>.</li>
      <li>Copy and paste your DCN license key into the dialog box.</li>
    </ol>
    <!--Follow the instructions in the pdf to get the license - <b>Get confirmation from Nayana</b><p><xref href="https://wiki.hpcloud.net/display/HCG/HP+Helion+OpenStack+Carrier+Grade+%28NFVi%29+Home#HPHelionOpenStackCarrierGrade%28NFVi%29Home-HPHelionOpenStackCarrierGradeDCN/Nuage" format="html" scope="external">https://wiki.hpcloud.net/display/HCG/HP+Helion+OpenStack+Carrier+Grade+%28NFVi%29+Home#HPHelionOpenStackCarrierGrade%28NFVi%29Home-HPHelionOpenStackCarrierGradeDCN/Nuage</xref></p>-->
    <section id="create-user-for-plugin-login">
      <title>Create an OSadmin user for plugin login</title>
      <p>You must create an administrative user called OSadmin and add it to CMS Group.</p>
      <ol>
        <li>From VSD Dashboard, click the <b>Open VSP Configuration</b> tab on the top right corner
          of the dashboard.</li>
        <li>Click the <b>CSP Users</b> tab and click <b>+</b>.</li>
        <li>Create a user named <codeph>OSadmin</codeph> with the password <codeph>OSadmin</codeph>.
          The name and password are case-sensitive. Do not copy and paste from here into the field.
              <p><image href="../../media/CGH-Install-VSD-create-user.png" id="image_pjz_wy3_bt"
              width="600"/></p></li>
        <li>Add the user to the <codeph>CMS Group</codeph>.<p><image
              href="../../media/CGH-install-VSD-user-to-group.png" width="600" id="image_q4m_sz3_bt"
            /></p></li>
      </ol>
    </section>
    <!--<section><title>Install the DVswitch</title></section><section><p>The VMware® vNetwork Distributed Switch (vDS) enables you to manage virtual machine networking for a number of hosts in a datacenter using the VMware vSphere application </p><p>Please Refer to the DCN VSP 3.0 R7 installation Guide for DVswitch installation.</p></section>-->
    <section id="configure-a-json-file-for-installation">
      <title>Deploy HP Helion OpenStack</title>
      <p>Use the following steps on the HLM host to deploy HP Helion OpenStack:</p>
      <ol>
        <li>Login to HLM VM.
            <codeblock>ssh &lt;HLM_VM_IP>
where: HLM_VM_IP is the CLM IP of the HLM VM. </codeblock><p>Use
            the default credentials: </p><p>
            <codeblock>User Name: cghelion
Password: cghelion</codeblock>
          </p><p><b>Important:</b> After logging in with the default password, make sure you change
            the password for the <codeph>cghelion</codeph> user. </p></li>
        <li>Execute the following command to switch to the root
          user:<codeblock>sudo su -</codeblock></li>
        <li>Change to the home directory.<codeblock>cd ~</codeblock></li>
        <li>Provision and configure HP Helion
            OpenStack.<codeblock>hnewcloud  &lt;cloudname&gt; memphis</codeblock><p>Where:</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create.<p>The cloud
                name can be any combination of up to 63 digits, letters, and the <codeph>-</codeph>
                  chracter:<ul id="ul_lz5_rbf_bt">
                  <li>
                    <p>Cannot start or end with the <codeph>-</codeph> chracter;</p>
                  </li>
                  <li>
                    <p>Must be greater than 0 characters;</p>
                  </li>
                  <li>
                    <p>No restriction on the number of letters or digits (0 or more).</p>
                  </li>
                </ul></p></li>
            <li><codeph>memphis</codeph> is the name of the template to use. The installation kit
              includes a template called <codeph>memphis</codeph> designed to install HP Helion
              OpenStack, specific to the KVM + ESX deployment.</li>
          </ul><p>The command creates the <codeph>&lt;cloudname&gt;</codeph> directory, which
            contains several JSON template files. </p></li>
      </ol>
    </section>
    <section id="configure-node-provision-json">
      <title>Configure the node-provision JSON file</title>
      <p>Modify the <codeph>node-provision.json</codeph> file in the <codeph>&lt;cloudname></codeph>
        directory of the HLM VM. This file supplies input values to the <codeph>hprovision</codeph>
        script, later in the installation. </p>
      <p><b>Note:</b> To see a sample <codeph>node-provision.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-node-json.dita">Sample node-provision.json File for
          Installing the KVM + ESX Topology</xref>.</p>
      <ol id="ol_ub2_xfl_zs">
        <li>Edit <codeph>node-provision.json</codeph> file to change only the following fields: 
          <table>
            <tgroup cols="2">
              <colspec colname="col1" colsep="1" rowsep="1"/>
              <colspec colname="col2" colsep="1" rowsep="1"/>
              <thead>
                <row>
                  <entry colsep="1" rowsep="1">Field</entry>
                  <entry colsep="1" rowsep="1">Description</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>Pxe-mac-address</entry>
                  <entry>The MAC address of the interface you want to PXE boot onto. This is not
                    same as iLO* MAC address.</entry>
                </row>
                <row>
                  <entry>pm_ip</entry>
                  <entry>The power management IP address (iLO IP address).</entry>
                </row>
                <row>
                  <entry>pm_user</entry>
                  <entry>The power management user name (iLO user name).</entry>
                </row>
                <row>
                  <entry>pm_pass</entry>
                  <entry>The power management password (iLO password).</entry>
                </row>
                <row>
                  <entry>failure_zone, vendor, model, os_partition_size, data_partition_size</entry>
                  <entry>The values in these fields are added to the <codeph>nodes.json</codeph>
                    file used during cloud deployment. </entry>
                </row>
              </tbody>
            </tgroup>
          </table><p>*iLO is HP Integrated Lights-Out, a server management tool embedded with the
            ProLiant servers. Consult your iLO documentation for information on how to locate the
            iLO IP address, user name, and password.</p></li>
        <li>Save and close the file.</li>
      </ol>
    </section>
    <p>
      <b>Configure PXE boot</b>
    </p>
    <p>After you edit the <codeph>node-provision.json</codeph> file, you must enable one-time PXE
      boot on the servers to set the correct boot order. Execute the following on the HLM VM:</p>
    <ol>
      <li>Copy the <codeph>ilopxebootonce.py</codeph> from the
          <systemoutput>/root/cg-hlm/dev-tools/ilopxebootonce.py</systemoutput> to the
          <codeph>&lt;cloudname></codeph> directory where you have the
          <codeph>node-provision.json</codeph> file.</li>
      <li>Execute the following script:
        <codeblock>python ilopxebootonce.py node-provision.json</codeblock></li>
    </ol>
    <p>After the script is run, the <codeph>Current One-Time Boot Option</codeph> is set to
        <codeph>Network Device 1</codeph> on all the servers listed in
        <codeph>node-provision.json</codeph> file.</p>
    <section id="configure-def-json">
      <title>Configure the definition.json file</title>
      <p>Modify the <codeph>definition.json</codeph> file in the <codeph>&lt;cloudname></codeph>
        directory of the HLM VM to define the number of ESX compute proxies required in your
        environment. You need one proxy per vCenter; make sure this value is set to
        2<!--set this value to the number of vCenters you have.-->, as shown in the following
        example.</p>
      <p><b>Note:</b> To see a sample <codeph>definition.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-def-json.dita#topic4797">Sample definition.json File
          for Installing the KVM + ESX Topology</xref>.<ol id="ol_zzr_khl_zs">
          <li>Set the number of compute systems to 2 for  an HA VRS-G
            installation.<codeblock>"file": ".hos/ccp-vrsg.json",
"count": 2</codeblock></li>
        </ol></p>
    </section>
    <section id="configure-env-json">
      <title>Configure the environment.json file</title>
      <p>Modify the <codeph>environment.json</codeph> file in the <codeph>&lt;cloudname></codeph>
        directory of the HLM VM to configure the VLANs and network addresses as appropriate for your
        environment. Configure the CLM (management), CAN (api), and BLS (blockstore) networks.</p>
      <p><b>Note:</b> To see a sample <codeph>environment.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-env-json.dita">Sample environment.json File for
          Installing the KVM + ESX Topology</xref>.</p>
      <p>For each network provide:</p>
      <codeblock>{
"name": "management",
"type": "vlan",
"segment-id": "1551",
"network-address": {
  "cidr": "10.200.51.0/24",
  "start-address": "10.200.51.100",
  "gateway": "10.200.51.1"
  }
},        </codeblock>
    </section>
    <section id="configure-ansible-json">
      <title>Configure the ansible.json file</title>
      <p>Modify the <codeph>ansible.json</codeph> file in the <codeph>&lt;cloudname>/vars</codeph>
        directory of the HLM VM.</p>
      <p>To see a sample <codeph>ansible.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-ansible-json.dita">Sample ansible.json File for
          Installing the KVM + ESX Topology</xref>.</p>
    </section>
    <section id="configure-esx-json">
      <title>Configure the esx.json file</title>
      <p>Modify the <codeph>esx.json</codeph> file in the <codeph>&lt;cloudname></codeph> directory
        on the HLM VM. This file is called by the script that installs the HP Helion OpenStack
        cloud, later in the installation. </p>
      <p>To see a sample <codeph>esx.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-esx-json.dita">Sample esx.json File for Installing the
          KVM + ESX Topology</xref>.</p>
    </section>
    <section id="configure-ldap-json">
      <title>Configure the ldap.json file</title>
      <p>Modify the <codeph>ldap.json</codeph> file in the <codeph>&lt;cloudname>/vars</codeph>
        directory of the HLM VM.</p>
      <p>To see a sample <codeph>ldap.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-ldap-json.dita">Sample ldap.json File for Installing
          the KVM + ESX Topology</xref>. </p>
    </section>
    <section id="configure-wr-json">
      <title>Configure the wr.json file</title>
      <p>Modify the <codeph>wr.json</codeph> file in the <codeph>&lt;cloudname>/vars</codeph>
        directory on the HLM VM.</p>
      <p>To see a sample <codeph>wr.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-wr-json.dita">Sample wr.json File for Installing the
          KVM + ESX Topology</xref>. </p>
    </section>
    <section>
      <title>Configure the dcn.json file</title>
      <p>Modify the <codeph>dcn.json</codeph> file in the
          <codeph>/opt/share/hlm/1/site/services/dcn.json</codeph> directory of the HLM VM to remove
          <b>both</b> instances of the following code:
        <codeblock>      {
          "description": "Ingress 47 GRE",
          "firewall": {
            "direction": "in",
            "protocol": "gre"
            },
           "port": 47,
            "scope": "private"
      },    </codeblock></p>
    </section>
    <section>
      <title>Verify the JSON files</title>
      <p>After editing the JSON files, validate each JSON file to make sure there are no syntax
        errors using the tool of your preference. For example, using the Python json.tool: </p>
      <codeblock>python -m json.tool &lt;filename>.json</codeblock>
    </section>
    <section>
      <title>Configure the ESX Compute Proxy</title>
      <p>The HP Helion OpenStack Carrier Grade vCenter ESX compute proxy (compute proxy) is a driver
        that enables the Compute service to communicate with a VMware vCenter server. The HP Helion
        OpenStack Compute Service (Nova) requires this driver to interface with VMware ESX
        hypervisor APIs. You can download the proxy installation files from the <xref
          href="https://helion.hpwsportal.com/catalog.html#/Home/Show" format="html"
          scope="external">Helion Download Network</xref>.
        <!-- Review with Michael Duncan all JSON files and proxy re: bonded set up --></p>
      <p>For instructions on installing the ESX compute proxy, see <xref
          href="carrier-grade-install-esx-proxy.dita#topic10581">Deploy the ESX Compute
        Proxy</xref>.</p>
    </section>
    <section id="create-a-new-cloud-template-and-bring-the-cloud-nodes-up">
      <title>Provision the cloud nodes and bring the cloud nodes up</title>
      <ol>
        <li>On the HLM host, use the following script to start the provisioning of the HP Helion
          OpenStack cloud: <codeblock>hprovision &lt;cloudname&gt;          </codeblock><p>Where:
              <codeph>&lt;cloudname&gt;</codeph> is the name of the cloud you created. The script
            takes approximately 15 to 30 minutes.</p><p>This script prepares the bare metal servers
            for the HP Helion OpenStack Carrier Grade cloud installation including the hLinux
            operating system. The script also PXE boots the nodes specified in
              <codeph>node-provision.json</codeph> file and tracks the PXE boot completion process.
            The script also creates the <codeph>nodes.json</codeph> file in the directory.
            </p><p>You can log in to the iLO server management tool for each of the nodes to monitor
            the boot process. Consult yout iLO documentation for information on how to log into iLO.
          </p></li>
        <li>Make sure the nodes are booted up using iLO. </li>
        <li>Once the baremetal nodes are provisioned, make sure the <codeph>nodes.json</codeph> file
          is generated. The <codeph>nodes.json</codeph> file will have entries of 3 controllers, 2
          DCN Hosts, 2 VRS-G and the compute proxy node.</li>
        <li>Verify that each node in the <codeph>nodes.json</codeph> file is installed and active. <ol>
            <li>Ping proxy node from HLM VM with the IP using IP specified in
                <codeph>nodes.json</codeph> file.</li>
            <li>SSH to the compute proxy node from the HLM VM using IP specified in
                <codeph>nodes.json</codeph> file.</li>
          </ol></li>
        <li>Configure the back-end drivers to enable management of the OpenStack Block Storage
          volumes on vCenter-managed data stores and 3PAR and/or VSA storage arrays. The HP Block
          Storage (Cinder) service allows you to configure multiple storage back-ends. Use the
          following steps to configure back-end support:<ol id="ul_btr_l52_xs">
            <li>Change to the <codeph>/cinder/blocks</codeph> directory:
              <codeblock>cd ~/&lt;cloudname>/services/cinder/blocks</codeblock> Where
                <codeph>&lt;cloudname></codeph> is the name you assigned to the cloud. The directory
              contains several sample Cinder configuration files that you can edit, depending upon
              which storage method(s) you are using.</li>
            <li>Depending upon the type of storage you are using, edit the following file: <ul>
                <li><codeph>cinder_conf.multiBackendSample</codeph> - this sample file provides all
                  the variables needed to use ESX in the non-KVM region and HP StoreVirtual VSA
                  and/or HP StoreServ (3PAR) attched to the KVM region;</li>
              </ul><p>Use the <codeph>enabled_backends</codeph> variable to list each of the
                back-ends you are using. You must specify at least one back-end for the non-KVM
                region and one or more for the KVM region.</p><table>
                <tgroup cols="3">
                  <colspec colname="col1" colsep="1" rowsep="1"/>
                  <colspec colname="col2" colsep="1" rowsep="1"/>
                  <colspec colname="col3" colsep="1" rowsep="1"/>
                  <thead>
                    <row>
                      <entry colsep="1" rowsep="1">Region/Hypervisor</entry>
                      <entry colsep="1" rowsep="1">Volume Backend</entry>
                      <entry colsep="1" rowsep="1">Backend Name</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>KVM</entry>
                      <entry>3PAR </entry>
                      <entry>hp3par</entry>
                    </row>
                    <row>
                      <entry>KVM</entry>
                      <entry>VSA</entry>
                      <entry>hplefthand</entry>
                    </row>
                    <row>
                      <entry>Non-KVM </entry>
                      <entry>ESX Datastores</entry>
                      <entry>vmdk </entry>
                    </row>
                  </tbody>
                </tgroup>
              </table><p><b>Note:</b> You can use either 3PAR and VSA in the KVM region or select
                both if you have respective storage arrays </p><p>A typical
                  <codeph>cinder_conf</codeph> that enables all three back ends appears similar to
                the following example:</p>
              <codeblock>[DEFAULT]
enabled_backends=vmdk, hp3par, hplefthand
...                                
[vmdk]
volume_backend_name = &lt;mybackendname3>
vmware_host_ip = &lt;hostip>
vmware_host_username = &lt;username>
vmware_host_password = &lt;password>
vmware_volume_folder = &lt;volumes_folder>
vmware_image_transfer_timeout_secs = 7200
vmware_task_poll_interval = 0.5
vmware_max_objects_retrieval = 100
volume_driver = cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver
          
[hp3par]
volume_backend_name=&lt;mybackendname1>
hp3par_api_url=https://&lt;management_ip>:8080/api/v1
hp3par_username=&lt;username>
hp3par_password=&lt;password>
hp3par_cpg=&lt;cpg>
san_ip=&lt;san_ip>
san_login=&lt;username>
san_password=&lt;password>
hp3par_iscsi_ips=&lt;iscsi_target_ips seperated by ,>
volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
hp3par_debug=False
hp3par_iscsi_chap_enabled=false
hp3par_snapshot_retention=48
hp3par_snapshot_expiration=72
                            
[hplefthand]
volume_backend_name=&lt;mybackendname2>
hplefthand_username = &lt;username>
hplefthand_password = &lt;password>
hplefthand_clustername = &lt;Cluster Name>
hplefthand_api_url = https://&lt;Iscsi Virtual IP address>/lhos </codeblock></li>
            <li>Save the file to <codeph>cinder_conf</codeph>.</li>
          </ol></li>
        <li>Run the HP Helion OpenStack Configuration Processor:
            <codeblock>hcfgproc -d definition.json</codeblock><p>The HP Helion OpenStack
            Configuration Processor is a script, called <codeph>hcfgproc</codeph>, that is
            incorporated into the installation environment. This command generates the necessary
            configuration for the cloud. </p><p>When the command completes, you will see the
            following:
            message:</p><codeblock>#################################################################################
The configuration processor completed successfully.
################################################################################</codeblock><p>To
            view the complete output of the command, see <xref
              href="carrier-grade-install-hcfgproc-output.dita#topic10581"> Output from the hcfgproc
              command</xref>.</p></li>
        <li>Review the CloudDiagram, <codeph>hosts.hf</codeph>, and
            <codeph>net/interfaces.d/eth.cfg</codeph> files to make sure the network settings are
          correct.</li>
        <li>Initialize network interfaces on all the cloud nodes using the following command:
            <codeblock><codeph>hnetinit &lt;cloudname&gt;</codeph></codeblock><p>Where:
              <codeph>&lt;cloudname&gt;</codeph> is the name of the cloud you created.</p><p>After
            this command completes, all cloud nodes and CLM network interfaces are configured. The
            output for the command should appear similar to the following:</p><p><image
              href="../../media/CGH-install-hnetinit-output.png" id="image_tft_r11_1t"/></p></li>
        <li>Apply the ESX compute proxy patch.<p>This patch fixes an intermittent issue where the
            ESX compute proxy is not configured after the <codeph>hprovision</codeph> completes
            successfully. The patch increase the timeout to allow enough time for the proxy VM to
              boot.<ol id="ol_n4h_mxm_bt">
              <li>On the HLM host, execute the following command to create directory for the patch
                <codeblock>mkdir ~/patch_cghelion1.1\ </codeblock></li>
              <li>Copy the <codeph>compute_proxy_patch_1.1.tar.gz</codeph> to the
                  <codeph>~/patch_cghelion1.1</codeph> directory. </li>
              <li>Use the following command to extract the
                  <codeph>compute_proxy_patch_1.1.tar.gz</codeph>:
                <codeblock>tar -zxvf compute_proxy_patch_1.1.tar.gz</codeblock></li>
              <li>Use the following command to install the Ubuntu patch
                utility:<codeblock>apt-get install patch </codeblock></li>
              <li>Change to the  <codeph>~/cg-hlm </codeph>
                directory:<codeblock>cd ~/cg-hlm</codeblock></li>
              <li>You can confirm that the patch will execute properly before performing the patch
                operation  by executing the following
                command:<codeblock>patch -p1 --dry-run &lt; ~/patch_cghelion_1.1/ compute-proxy-timeout.patch</codeblock></li>
              <li>The command shoild result in the following
                message:<codeblock>checking file node-provision/hlm_provisionproxy.py</codeblock></li>
              <li>Execute the following command to perform tha patching operation:
                <codeblock>patch -p1 &lt; ~/patch_cghelion_1.1/ compute-proxy-timeout.patch</codeblock>The
                patch application returns the following message, if
                successful:<codeblock> patching file node-provision/hlm_provisionproxy.py</codeblock></li>
              <li>Change to the <codeph>~/cg-hlm/hlm-build</codeph>
                folder:<codeblock>cd ~/cg-hlm/hlm-build</codeblock></li>
              <li>Execute the following command to synch the patch
                file:<codeblock>./hlm_prepare-env.sh</codeblock></li>
            </ol></p></li>
        <li>Apply the GRE patch. <p>This patch contains three files. The first file fixes an issue
            with the iptables role failing during installation while enabling GRE in firewall in VRG
            node. The other two files enable GRE on VRG node in iptables.</p><p>You should have
            downloaded the patch files in the <xref
              href="carrier-grade-install-pb-prereqs-GA.dita#topic7148/getinstall"
              >Prerequisites</xref>. If not, execute the following
            command:<codeblock>apt-get install patch</codeblock></p><p>
            <ol id="ol_kjd_hvm_bt">
              <li>Extract the gre_patch_1.1.tar.gz file on the HLM host to any directory.</li>
              <li> Change to the following directory:
                <codeblock>cd /root/kenobi-configuration-processor</codeblock></li>
              <li> Execute the following command:
                <codeblock>patch -p1 &lt; &lt;path_to_patch_file>/GRE-related-installer-failure-fix.patch </codeblock></li>
              <li>Change to the following directory:
                <codeblock>cd /root/kenobi-node-configuration </codeblock></li>
              <li>Execute the following command:
                <codeblock>patch -p1 &lt; &lt;path_to_patch_file>/enabling-GRE-in-iptables-for-VRG.patch </codeblock></li>
              <li>Execute the following command:
                <codeblock>~/cg-hlm/hlm-build/./hlm_prepare-env.sh</codeblock></li>
              <li>Verfiy that the GRE is enabled in iptables on VRG node by running the following
                command on the VRG node:<codeblock>iptables -S | grep gre</codeblock> The following
                output should appear: <codeph>A ufw-before-input -p gre -j ACCEPT</codeph>. </li>
            </ol>
          </p></li>
        <li>Use the following command to deploy the HP Helion OpenStack cloud:
            <codeblock><codeph>hdeploy &lt;cloudname&gt;</codeph></codeblock><p>This command takes a
            significant period of time. When this command is complete, the non-KVM cloud
            installation is complete. Use the following sections to configure the Horizon interface,
            configure networking, and configure the ESX environment. </p></li>
        <li>Apply the LDAP patch. <!--Details forthcoming--></li>
        <li>Enter <codeph>exit</codeph> to leave the root shell.</li>
      </ol>
    </section>
    <section>
      <title> Apply the Horizon interface patch:</title>
      <ol>
        <li>Locate the <codeph>hcg_1.1.0_horizon_patch1.tar</codeph> file you downloaded to the HLM
          host and extract the files.</li>
        <li>From the HLM VM, SSH into any controller node using the default credentials: <p>
            <codeblock>User Name: cghelion
Password: cghelion</codeblock>
          </p></li>
        <li>Execute the following command to switch to the root
          user:<codeblock>sudo su -</codeblock></li>
        <li>Copy the <codeph>user.py</codeph> file to first controller node in the
            <codeph>/opt/stack/venvs/horizon/lib/python2.7/site-packages/openstack_auth/</codeph>
          directory.</li>
        <li>Use the following command to restart the Horizon service on that node:
          <codeblock>sudo service apache2 restart</codeblock></li>
        <li>Enter <codeph>exit</codeph> to leave the root shell.</li>
        <li>Log into each of the other two controller nodes in the non-KVM region and repeat above
          steps to copy the <codeph>user.py</codeph> to each controller.</li>
      </ol>
    </section>
    <section>
      <title>Import the Horizon browser certificate</title>
      <p>After applying the Horizon patch, you need to import the Horizon security certificate
          (<codeph>ca.crt</codeph>) into your browser. The <codeph>ca.crt</codeph> is available in
        the HLM VM in the <codeph>~/&lt;cloudname></codeph> folder.</p>
      <p>Use the instructions provided by the browser manufacturer to import the
          <codeph>ca.crt</codeph> file. For a list of browsers supported by OpenStack, see <xref
          href="https://wiki.openstack.org/wiki/Horizon/BrowserSupport" format="html"
          scope="external">https://wiki.openstack.org/wiki/Horizon/BrowserSupport</xref>.</p>
      <p>Also, clear the cache from the browser.</p>
    </section>
    <section>
      <title>Login to Horizon</title>
      <p>At this point, you can log into the Horizon interface, a web-interface you can use to
        manage and maintain HP Helion OpenStack Carrier Grade. Use the <xref href="#topic10581/CAN"
          format="dita">CAN IP address</xref> to launch Horizon. </p>
      <p>
        <codeblock>https://&lt;CAN_IP></codeblock>
      </p>
      <p>The default credentials are:</p>
      <codeblock>login: admin
password: admin        </codeblock>
      <p id="CAN">You can locate the CAN IP address in the <codeph>/root/stackrc</codeph> file on any of the
        controller nodes in the non-KVM region.</p>
      <p>The CAN IP address contains port <codeph>5000</codeph>. </p>
      <p>For example:</p>
      <codeblock>root@B53NEW-CCP-T2-M1-NETCLM:~# cat /root/stackrc
export OS_PASSWORD=admin
export OS_USERNAME=admin
export OS_TENANT_NAME=admin
#export OS_AUTH_URL=http://10.x.x.x:35357/v2.0
export OS_AUTH_URL=https://10.x.x.x:5000/v2.0
export OS_CACERT=/etc/stunnel/ssl/hlm/certs/ca.crt </codeblock>
      <p>In this case, the CAN IP address is located in the
          <codeph>OS_AUTH_URL=https://10.x.x.x:5000/v2.0</codeph> line. You would log in using
          <codeph>https://10.x.x.x</codeph>.</p>
      <p>The Horizon interface appears similar to the following:</p>
      <p><image href="../../media/CGH-install-horizon.png" id="image_rcc_td1_1t"/></p>
    </section>
    <section>
      <b>Verify the VSC VMs</b>
      <p>The installation process creates virtual machines for VSC. Use the following steps to
        verify that the VSC VMs are installed and are operational:</p><ol id="ol_fkv_xjj_1t">
        <li>SSH to your VSC VM from HLM Host using the DCM IP. The default username and password:
            <codeph>admin/admin</codeph></li>
        <li>Execute the following command: <codeblock>admin display-config</codeblock></li>
        <li>Execute the following commands to verify the VMs are active:
          <codeblock>show vswitch-controller vsd
show vswitch-controller xmpp-server
ping router "management" &lt;vsd IP or domain name>  </codeblock></li>
      </ol></section>
    <section><p><b>Verify the VRS-G Node</b>
      </p><p>The installation creates a VRS-G node as part of the cloud deployment. Use the
        following command to verify that the VRS-G is active:
        </p><codeblock>show vswitch-controller vswitches  </codeblock><p>The output for the command
        should appear similar to the following image:</p>
      <image href="../../media/CGH-install-verify-vrsg.png" id="image_x3t_jvp_vs"/>
      <p>Verify that the domain name and DNS server are listed in the
          <codeph>/etc/resolv.conf</codeph> file on all the cloud
        nodes,.</p><codeblock>cat etc/resolv.conf          </codeblock><image
        href="../../media/CGH-install-resolv-conf.jpg" id="image_idc_5wp_vs">
        <alt>Verify VRS-G </alt>
      </image>
    </section>
    
    <section><title>Prerequisite to be performed on switch for the uplink subnet </title>You need to
      create the static IP route for external network/FIP to point to point IP subnet from Gateway
      Example from main Lab Router.<p>The output for the command is similar to the following;</p><codeblock>#
interface Vlan-interface 1553
description  EXT UPLINK SUBNET
ip address 10.200.70.1 255.255.255.192
#
IP Packet Frame Type: PKTFMT_ETHNT_2,  Hardware Address: abcd-1234-lmno
Destination/Mask    Proto  Pre  Cost         NextHop         Interface
10.200.70.0/26      Direct 0    0            10.200.70.1     Vlan1553
10.200.70.1/32      Direct 0    0            127.0.0.1       InLoop0
10.200.70.64/26     Static 60   0            10.200.70.2     Vlan1553
10.200.70.128/26    Static 60   0            10.200.70.2     Vlan1553
10.200.70.192/26    Static 60   0            10.200.70.2     Vlan1553</codeblock><ol>
        <li>From the HLM VM, SSH into any controller node using the default credentials: <p>
            <codeblock>User Name: cghelion
Password: cghelion</codeblock>
          </p></li>
        <li>Execute the following command to switch to the root
          user:<codeblock>sudo su -</codeblock></li>
        <li>If you want to be able to execute CLI command on the current controller, add the
          following line to the /<codeph>root/stackrc</codeph>
          file:<codeblock>export OS_REGION_NAME=memphis</codeblock></li>
        <li>Change to the home directory and source the <codeph>stackrc</codeph> file:
          <codeblock>cd ~/
source stackrc</codeblock></li>
        <li>Execute the following command on any controller node to create the external network:
            <codeblock>neutron net-create &lt;external-network-name> --router:external</codeblock><p>Where
              <codeph>&lt;external-network-name></codeph> is a descriptive name for the
            network.</p><p>For
            example:</p><codeblock>neutron net-create ext-net  --router:external </codeblock><p>Where
              <codeph>&lt;external-network-name></codeph> is a descriptive name for the
          network.</p></li>
        <li>Execute the following command to create a subnet for the external network:
            <codeblock>neutron subnet-create &lt;external-network-name> &lt;subnet-ip> --name &lt;name></codeblock><p>Where:
              <ul id="ul_frm_3ml_zs">
              <li>
                <codeph>&lt;external-network-name></codeph> is the name of the external network you
                created;</li>
              <li><codeph>&lt;subnet-ip></codeph> is the IP address range to assign </li>
              <li><codeph>&lt;name></codeph> is a descriptive name for the subnet. Make note of this
                name, as you will use it during the installation.</li>
            </ul></p><p>For
          example:</p><codeblock>neutron subnet-create ext-net 10.200.70.64/26 --name extsub1 </codeblock></li>
        <li>Use the following command to create a virtual router:
            <codeblock>neutron router-create &lt;name></codeblock><p>Where:
              <codeph>&lt;name></codeph> is a descriptive name for the router.</p></li>
        <li>Enter <codeph>exit</codeph> to leave the root shell.</li>
      </ol></section>
    <section>
      <title>Add VLAN to VRS-G network device</title>
      <p>VRS‐G configuration includes setting up the port and VLAN ranges that are to be available
        to access the VSD. By default, the VRS‐G boots with all its ports configured as the network
        type, and therefore it has no VLANs available. </p>
      <p>Use the following command to change <codeph>eth0</codeph> from network to access and allow
        VLANs.</p>
      <ol>
        <li>From the HLM VM, SSH to the VRS-G nodes using the default username and password.<p>
            <codeblock>User Name: cghelion
Password: cghelion</codeblock>
          </p></li>
        <li>Execute the following command to switch to the root
          user:<codeblock>sudo su -</codeblock></li>
        <li>Execute the following command, using your VLAN ID.
            <codeblock>nuage-vlan-config mod eth0 Access &lt;vlad-id-range></codeblock><p>For
            example, if your VLAN ID is
          1508:</p><codeblock>nuage-vlan-config mod eth0 Access 1500-2000</codeblock></li>
      </ol>
    </section>
    <section id="vrsg-config">
      <title>Configure the VRS-G </title>
      <p>The HP Helion OpenStack Carrier Grade installation creates the required VRS-G gateway. You
        need to use the HP DCN Virtualized Services Directory Architect (VSD) interface to add the
        gateway and configure the <codeph>eth0</codeph> port.</p>
      <p><i>For more information on performing this task, refer to the latest DCN documentation
        </i></p>
      <ol>
        <li>Launch the VSD dashboard:<ul id="ul_ylj_thz_xs">
            <li>Using Chrome or Safari, navigate to <codeph>https://&lt;VSD_address>:8443/</codeph>
              and enter user name, password and organization. See the DCN documentation if you need
              to locate the IP address for the VSD.<p>
                <codeblock>User Name: Csproot 
Password: csproot 
Org: csp 
VSD Server : auto </codeblock>
              </p></li>
            <li>Click the arrow icon to log in.</li>
          </ul></li>
        <li>Click the <b>Gateways</b> tab. Two new gateways appear in the <b>Pending Gateways</b>
          list on the left.</li>
        <li>Click the blue arrow next to one of the new gateways to manage that gateway.<p><image
              href="../../media/CGH-install-nuage-gateway.png" id="image_rgy_5fz_xs" width="500">
              <alt>Discover the Gateway</alt>
            </image></p></li>
        <li>Add a port for the gateway by clicking <b>Create a Port</b> in the second panel from the
          left.</li>
        <li>Enter the information as required, specifying <codeph>eth0</codeph> in the <b>Physical
            Name</b> field and entering the VLAN ID for the external network. Also, change the
            <b>Type</b> field to <codeph>Access</codeph>.<p><image
              href="../../media/CGH-install-gateway-port.png" id="image_w3p_sgz_xs" width="250">
              <alt>Gateway Port</alt>
            </image></p></li>
        <li>Click <b>Create</b>.</li>
        <li>Click the blue arrow next to the second new gateway.</li>
        <li>Perform the same steps to add a port entering the same information. Both gateways must
          be configured in an identical manner.<p>The VLAN and other ports will appear similar to
            the following:</p><p><image href="../../media/CGH-install-vcenter-vlan.png"
              id="image_sgn_mh1_1t"/></p></li>
      </ol>
    </section>
    <section><title>Create a redundancy group for the VRS-G</title>
      <i>For more information on performing this task, refer to the latest DCN documentation </i>
      <p>In order to achieve high availability for the VRS-G, you must create a redundancy group
        using the VSD dashboard.</p><p>A redundancy group will designate one of the two gateways as
        the authoritative (active) gateway and the other as the secondary (passive) gateway. You can
        change which gateway is authoritative either at that point or at a later tine, by selecting
        the group and clicking the pencil icon.</p><p> Redundancy groups can be created out of two
        instantiated gateways of the VRS-G type. Both instances must either:</p><ul>
        <li>Come from the same template</li>
        <li>Be instantiated </li>
      </ul><p>In addition, the two gateways that make up a redundancy group must have exactly the
        same port and VLAN configuration. </p><p>To create a redundancy group, using the VSD
        dashboard: </p><ol>
        <li>Select the two peers and click on the redundancy group icon.<p><image
              href="../../media/CGH-install-vsd-redundancy.png" id="image_q5q_5j1_1t"/></p></li>
      </ol>
    </section>
    <section><title>Assign permissions to the VRS-G</title> There are two types of permissions,
        <b>Use Permissions</b> and <b>Extend Permissions</b>. Use Permissions allow members in the
      assigned group to use an object. Extend Permissions allow the members of an assigned group to
      use an object and to allow other groups to use the object. For this installation, you can
      select <b>Use Permission</b>. <p><i>For more information on performing this task, refer to the
          latest DCN documentation </i></p><p>To assign permissions:</p><ol id="ol_x4q_vl1_1t">
        <li>Select the redundancy group you created and click <b>Add a Permission</b>.</li>
        <li>Select a group and click <b>Select</b>. Members of this group will be able to use the
          gateway.</li>
      </ol></section>
    <section>
      <title>Retrieve the subnet floating IP name from the VSD</title>
      <p>You will need the floating IP subnet name during the installation when you create and edit
        a cURL script. If you do not have the name of the subnet you created, use the following
        steps:</p>
      <p><i>For more information on performing these tasks, refer to the DCN documentation </i></p>
      <ol>
        <li>In the VSD dashboard, click <b>Open Data Center Configuration</b>, then <b>Shared
            Network</b> tab. <p><image href="../../media/CGH-install-vsd-fip.png"
              id="image_okn_sjz_xs">
              <alt>Gateway Port</alt>
            </image></p></li>
        <li>For the floating IP subnet, right-click and select <b>Edit</b> to get the name. <p>This
            is the same name you assigned when you created the subnet. Make note of this name, as
            you will use it to tun the cURL script in the following step.</p></li>
      </ol>
    </section>
    <section>
      <title>Run a cURL script to enable floating IPs on the VRS-G</title>
    </section>
    <section>
      <p>This script creates the uplink on the VRS-G, which is a requirement for the EXT-NET to
        function with HP Helion OpenStack Carrier Grade. </p>
      <ol id="ol_ks4_zhj_1t">
        <li>From the HLM host, copy the following code to create a cURL script.</li>
        <li>Modify the cURL script for your environment:<ul id="ul_bzs_r1c_1t">
            <li>FIP_NAME is the one that is retrieved from previous step.</li>
            <li>GW_NAME is the redundant gateway name.</li>
            <li>All the UPLINK info are from the core switch on their specific testbed</li>
          </ul>
          <codeblock>
#############################################################
#!/bin/bash
set -x
#
# Starting with VSP 3.0R2, there is an officially supported way to enable FIP using
# a VSG or VRS-G uplink port
#
            
# Parameters
VLAN="209"
VSD_IP="10.20.5.21"
FIP_NAME="eec14785-be36-4975-9ef8-2bf7edc5590e" # unique name show in VSD created for external network floating ip pool
#GW_ID="3aa23ffa-7530-46eb-ab3b-eff9fe65b0f6"
#GW_NAME="10.20.6.106" # use the ip of the VRSG node created on VSD / VSC
GW_NAME="LR4-TB2-VRSG-Cluster" # use the ip of the VRSG node created on VSD / VSC
PORT_NAME="eth0" # pyshical nic use don VRSG node for trunk/ tagged external trafic
             
#
# IANA has reserved 192.0.0.0/29 for DS-lite transition
#
UPLINK_SUBNET="10.20.9.0"
UPLINK_MASK="255.255.255.192"
VRSG_IP="10.20.9.2"  #Modified :P
UPLINK_GW="10.20.9.1"                         # ROUTER IP ON SWITCH main switch 10.1.64.21
UPLINK_GW_MAC="bc:ea:fa:1d:b0:80"       # VLAN 1209 MAC address not VRSG is from main switch 10.1.64.21
if [ $# -eq 1 ]; then # remote install
echo "Performing remote install to root@'$1' (requires PermitRootLogin=yes in sshd config)..."
ssh root@$1 'bash -s ' &lt; $0
exit 0
elif [ $# -ne 0  ]; then
cat &lt;&lt;END
Usage (as root) $0 [remote IP]
END
exit -1
fi

# Install required software packages, if not already
if [ -e /usr/bin/yum ]; then
[[ `which jq` != "" ]] || yum install -y jq
QEMU_KVM="/usr/libexec/qemu-kvm"
QEMU_USR="qemu:qemu"
LIBVIRTD="libvirtd"
else
[[ `which jq` != "" ]] || apt-get install -y jq
QEMU_KVM="/usr/bin/kvm"
QEMU_USR="libvirt-qemu:kvm"
LIBVIRTD="libvirt-bin"
fi
              
# Determine Domain name for the Floating IP pool ( based on pool name )
APIKEY=`curl -ks -H "X-Nuage-Organization: csp" -H "Content-Type: application/json" -H "Authorization: XREST Y3Nwcm9vdDpjc3Byb290" https://$VSD_IP:8443/nuage/api/v3_0/me | jq -r '.[0].APIKey'`
TOKEN=`echo -n "csproot:$APIKEY" | base64`
ZONE_ID=`curl -ks -H "X-Nuage-Organization: CSP" -H "X-Nuage-Filter: name=='$FIP_NAME'" -H "Content-Type: application/json" -H "Authorization: XREST $TOKEN" \
https://$VSD_IP:8443/nuage/api/v3_0/sharednetworkresources | jq -r '.[0].parentID'`
if [ "$ZONE_ID" == "" ]; then
echo "Error: Floating IP pool named '$FIP_NAME' not found"
exit 1
fi

# Lookup VLAN to use
GW_ID=`curl -ks -H "X-Nuage-Filter: name=='$GW_NAME'" -H "X-Nuage-Organization: CSP" -H "Content-Type: application/json" -H "Authorization: XREST $TOKEN" \
https://$VSD_IP:8443/nuage/api/v3_0/redundancygroups | jq -r '.[0].ID'`
PORT_ID=`curl -ks -H "X-Nuage-Filter: name=='$PORT_NAME'" -H "X-Nuage-Organization: CSP" -H "Content-Type: application/json" -H "Authorization: XREST $TOKEN" \
https://$VSD_IP:8443/nuage/api/v3_0/redundancygroups/$GW_ID/ports | jq -r '.[0].ID'`
VLAN_ID=`curl -ks -H "X-Nuage-Filter: value==$VLAN" -H "X-Nuage-Organization: CSP" -H "Content-Type: application/json" -H "Authorization: XREST $TOKEN" \
https://$VSD_IP:8443/nuage/api/v3_0/ports/$PORT_ID/vlans | jq -r '.[0].ID'`
if [ "$VLAN_ID" == "" ]; then
echo "Error: VLAN on redundancygroup '$GW_NAME' with port '$PORT_NAME' and value '$VLAN' not found"
exit 1
fi
echo "VLAN $VLAN ID: $VLAN_ID"
# Get/Create uplink subnet
SUBNET_ID=`curl -ks -H "X-Nuage-Filter: type=='UPLINK_SUBNET'" -H "X-Nuage-Organization: CSP" -H "Content-Type: application/json" \
-H "Authorization: XREST $TOKEN" https://$VSD_IP:8443/nuage/api/v3_0/sharednetworkresources | jq -r '.[0].ID'`
if [ "$SUBNET_ID" == "" ]; then
echo "Creating new FIP uplink subnet in ZONE $ZONE_ID"
curl -ks -H "X-Nuage-Organization: CSP" -H "Content-Type: application/json" -H "Authorization: XREST $TOKEN" \
https://$VSD_IP:8443/nuage/api/v3_0/sharednetworkresources -d "{ \
\"name\": \"FIP uplink subnet\", \
\"description\": \"uplink subnet\", \
\"address\": \"$UPLINK_SUBNET\", \
\"netmask\": \"$UPLINK_MASK\", \
\"gateway\": \"$VRSG_IP\", \
\"type\": \"UPLINK_SUBNET\", \
\"uplinkInterfaceIP\" : \"$UPLINK_GW\", \
\"uplinkInterfaceMAC\" : \"$UPLINK_GW_MAC\", \
\"sharedResourceParentID\" : \"$ZONE_ID\", \
\"uplinkGWVlanAttachmentID\" : \"$VLAN_ID\", \
\"uplinkVPortName\" : \"uplink vport1\" \
}"
else
echo "FIP uplink subnet already exists: $SUBNET_ID"
fi
#############################################################          </codeblock></li>
        <li>Save and close the script.</li>
        <li>Execute the cURL script <codeblock>bash +x &lt;script_name>.sh</codeblock><p>Where:
            &lt;script_name> is the name of the cURL script you created.</p></li>
        <li>Use the VSD dashboard to confirm that uplink port appears in the <b>Monitoring</b> tab
          under VRS-G node. <p><b><i> You might see the VSC in red when the VRS-G is in HA mode,
                rather that green. This is expected behavior.</i></b></p><p><image
              href="../../media/CGH-install-curl.png" id="image_d2h_mcc_1t" width="700"/></p></li>
      </ol>
    </section>
    <section>
      <title>Configure ingress and egress security policies</title>
      <p>Use the following steps to configure these policies to allow SSH and PING access to the
        VRS-G.</p>
      <p><i>For more information on performing these tasks, refer to the DCN installation guide and
          user guide.</i><ol id="ol_lf3_dp1_ys">
          <li>In the VSD dashboard, click <b>Domains</b>, then <b>Ingress Security Policies</b> in
            the ribbon.</li>
          <li>Right-click the default ingress security policy and click <b>Edit</b>.</li>
          <li>Select <b>Forward IP Traffic by Default</b>. Make sure <b>Make This Policy Active</b>
            is selected. <p><image href="../../media/CGH-install-ingress.png" id="image_fkf_yp1_ys"
                width="650">
                <alt>VSD Ingress Rule</alt>
              </image></p></li>
          <li>Click <b>Update</b>.</li>
          <li>Click <b>Egress Security Policies</b> in the ribbon.</li>
          <li>Right-click the default egress security policy and click <b>Edit</b>.</li>
          <li>Select <b>Deploy Implicit Rules</b> and <b>Forward IP Traffic by Default</b>. Make
            sure <b>Make This Policy Active</b> is selected.</li>
          <li>Click <b>Update</b>.</li>
        </ol></p>
    </section>
    <section>
      <title>Enable GRE in iptables</title>
      <!-- CG-1265 -->
      <p>If an environment uses more than one VRS-G node, you must enable Generic Routing
        Encapsulation (GRE) in the IP tables. The following commands show you how to use the UFW
        tool to configure the IP tables.</p>
      <ol>
        <li>On each VRS-G node, navigate to the <codeph>/etc/ufw/before.rules</codeph> file.</li>
        <li>Add the following lines to the file, before the <codeph>COMMIT</codeph> line:
          <codeblock>-A ufw-before-input -p 47 -j ACCEPT</codeblock></li>
        <li>Save and close the file.</li>
        <li>Execute the following command to restart the UFW service.</li>
      </ol>
    </section>
    <section>
      <title>Upload a Glance VMDK Image</title>
      <p>In order to create VM instances in HP Helion OpenStack Carrier Grade, you need to supply at
        least on ISO image file, and import that file into Horizon. </p>
      <ol id="ol_yd1_ckj_1t">
        <li>Launch the Horizon dashboard.</li>
        <li>Switch to <b>memphis</b> region. </li>
        <li>Click the <b>Images</b> link on the <b>Admin</b> panel. </li>
        <li>In the <b>Images</b> screen, click <b>Create Image</b>.</li>
        <li>In the <b>Create an Image</b> screen, fill out the fields as appropriate: <ol
            id="ol_zd1_ckj_1t">
            <li>From the <b>Image Source</b> list, select <b>Image File</b>.</li>
            <li>From the <b>Image File</b> list that appears, navigate to the VMDK file.</li>
            <li>From the <b>Format</b> list, select <codeph>VMDK</codeph>.
              <p><image href="../../media/CGH-install-create-vmdk-image.png" width="400"
                    id="image_qym_mlj_1t"/>
              </p></li>
          </ol></li>
        <li>Click <b>Create Image</b>.</li>
        <li>For the image, click <b>More</b> then click <b>Edit</b> to update the metadata for the
              image.<p><image href="../../media/CGH-install-create-image.png" width="400"
              id="image_etd_dlj_1t"/></p></li>
        <li> Add <codeph>vmware_adaptertype</codeph> and <codeph>vmware_disktype</codeph>
          properties. For Ubuntu, Rhel images you can change the <codeph>vmware_adaptertype</codeph>
          to <codeph>lsiLogic</codeph>.</li>
        <li>When complete, click <b>Update Image</b>. </li>
      </ol>
      <p>Once the images are uploaded, you are ready to launch the instance with different flavors.
      </p>
    </section>
    <section id="next-step">
      <title>Next Step</title>
      <p>
        <xref href="carrier-grade-install-kvm-cloud-GA.dita">Deploying the KVM Region</xref>
      </p>
    </section>
  </body>
</topic>
