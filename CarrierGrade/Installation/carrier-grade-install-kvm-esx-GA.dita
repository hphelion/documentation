<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic10581">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 1.1: Installing the KVM + ESX<tm
      tmtype="reg"/> Deployment</title>
  <prolog>
    <metadata>
      <othermeta name="layout" content="default"/>
      <othermeta name="product-version" content="HP Helion Openstack Carreir Grade 1.1"/>
      <othermeta name="role" content="Storage Administrator"/>
      <othermeta name="role" content="Storage Architect"/>
      <othermeta name="role" content="Michael B"/>
      <othermeta name="product-version1" content="HP Helion Openstack Carreir Grade 1.1"/>
    </metadata>
  </prolog>
  <body>
    <p>After the <xref href="carrier-grade-install-hlm-vm.dita#topic10581">Helion Lifecycle
        Management (HLM) VM is installed</xref>, the next task in installing the <xref
        href="carrier-grade-install-pb-overview.dita#topic1925/install-option">KVM + ESX
        deployment</xref> is to deploy the HP Helion OpenStack cloud and install the HP Distributed
      Cloud Networking (DCN) components. You must install the VMware ESX<tm tmtype="reg"/> compute
      proxy. </p>
    <p><b>Note:</b> If you are installing the KVM-only deployment, see <xref
        href="carrier-grade-install-kvm.dita#topic10581">Installing the KVM Deployment</xref>.</p>
    <section id="prereqs">
      <p>Before starting the HP Helion OpenStack Carrier Grade installation, the VMware vSphere
        application and HP Distributed Cloud Networking components must be fully installed and
        functioning:</p>
      <p><b>DCN Requirements</b></p>
      <p>HP Helion Distributed Cloud Networking (DCN) must be deployed before starting the cloud
        deployment. Please refer to DCN documentation for details. </p>
      <p>For a quick overview of the DCN components, see <xref
          href="carrier-grade-technical-overview-esx.dita#topic3485/dcn">KVM + ESX Deployment
          Architecture Reference</xref></p>
      <p>In production cloud deployments:</p>
      <ul id="ul_hnw_pry_zs">
        <li>DCN should be deployed in HA mode (with 2 VSCs, 2 VRS-Gs).</li>
        <li>VSD should be deployed in HA mode (3+1 VSDs). </li>
        <li>Configure appropriate entries for VSD into DNS so that the SRV record for
            <codeph>_xmpp-client</codeph> resolves. This is required for DCN Cloud. Refer to  the
          DCN documentation examples of <codeph>BIND</codeph> commands and instructions for testing
          the DNS configuration. </li>
        <li>VSD should be assigned a DCM IP accessible for cloud deployment. Refer to the <xref
            href="carrier-grade-technical-overview-esx.dita#topic3485">architecture
          diagram</xref>.</li>
        <li>Verify that the IP address and domain name of VSD can be accessed using PING and DIG
          commands. </li>
        <li>Make sure all the VSD services are in PASS state.</li>
        <li><xref href="#topic10581/vsd" format="dita">Apply the VSD license</xref> based on single
          or clustered VSD setup. </li>
        <li><xref href="#topic10581" format="dita">Create the required VSD users</xref>.</li>
      </ul>
      <p>Optionally, you can integrate LDAP into the DCN environment.</p>
    </section>
    <section>
      <p><b>ESXi requirements</b></p>
      <p>ESX should be deployed before starting the cloud deployment. For more information, refer to the VMware product
        documentation.</p>
      <p>In production cloud deployments:</p>
      <ul id="ul_ejv_35y_zs">
        <li>ESX Cluster with two (2) or more hosts installed with ESXi 5.5 U2 and above</li>
        <li>ESX Cluster should have HA, DRS &amp; Vmotion Enabled</li>
        <li>ESX Cluster should have Shared Storag</li>
        <li>The CLM and TUL networks must be available to all ESX hosts VMNICs</li>
      </ul>
      <p><b>ESX Network Configuration Requriements</b></p>
      <p>In production cloud deployments, you will need to create the following three (3) port
        groups.</p>
      <ul id="ul_mdl_r5y_zs">
        <li>Cloud Lab Management (CLM VLAN)</li>
        <li>DVRS Datapath (TUL VLAN)</li>
        <li>Trunk Network (All VLANS)</li>
      </ul>
      <p>The names are case sensitive.</p>
      <p>In the VMware vCenter client, the port group list should appear as follows:</p>
      <p><image href="../../media/CGH-install-ESX-ports.png" id="image_os2_2vy_zs"/></p>
    </section>
    <section id="vmware">
      <title>Deploy the VMware vSphere Distributed Switch</title>
      <p>The VMware vSphere Distributed Switch you obtained from VMware must be installed. For more
        information, see <xref href="carrier-grade-install-vsphere-switch.dita#topic10581">Deploying
          the VMware vSphere Distributed Switch</xref>. </p>
    </section>
    <section>
      <title>Deploy the VRSvApp</title>
      <p>The VRSvApp you obtained from DCN must be installed. For more information, see <xref
          href="carrier-grade-install-vsvapp.dita#topic10581">Deploying the VRS vApp</xref>. </p>
    </section>
    <section id="vsd">
      <title>Verify VSD is running</title>
      <p>Make sure the VSD node is installed by logging into the VSD VM using SSH and running the
        following command:</p>
      <codeblock>service vsd status</codeblock>
      <p>You should see the status as below from VSD VM.</p>
      <p>
        <image href="../../media/CGH-install-virsh-vsd.png" width="400"/></p>
    </section>
    <section><title>Apply the VSD License</title>You should have recevied a license file when you
      purchased DCN. You need to apply that license on the HLM host. For more information, see <xref
        href="carrier-grade-install-vsd-license.dita#topic10581"/>.</section>
    <section id="create-user-for-plugin-login">
      <title>Create an OSadmin user for VSD</title>
      <p>You must create an administrative user called OSadmin and add it to CMS Group. For more
        information, see <xref href="carrier-grade-install-vsd-user.dita#topic10581">Create an
          OSadmin User for VSD</xref>.</p>
    </section>
    
    <section id="configure-a-json-file-for-installation">
      <title>Launch the HP Helion OpenStack cloud</title>
      <p>Use the following steps on the HLM host to log on to the HLM VM created in <xref
        href="carrier-grade-install-hlm-vm.dita#topic10581">Bootstrapping the HP Helion Lifecycle Management Virtual Machine and Installation Services</xref>:</p>
      <ol>
        <li>Login to HLM VM. <codeblock>ssh &lt;HLM_VM_IP></codeblock><p>where: HLM_VM_IP is the CLM
            IP of the HLM VM. Locate the IP address for the HLM VM in the /root/infra-ansible-playbooks/group_vars/all file under hlm_clmstaticip field.</p><p>Use the default credentials: </p><p>
            <codeblock>User Name: cghelion
Password: cghelion</codeblock>
          </p><p><b>Important:</b> After logging in with the default password, make sure you change
            the password for the <codeph>cghelion</codeph> user. </p></li>
        <li>Execute the following command to switch to the root
          user:<codeblock>sudo su -</codeblock></li>
        <li>Change to the home directory.<codeblock>cd ~</codeblock></li>
      
        <li>Execute the following command to provision and configure the HP Helion OpenStack
            cloud.<codeblock>hnewcloud  &lt;cloudname&gt; memphis</codeblock><p>Where::</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create.<p>The cloud
                name can be any combination of up to 63 digits, letters, and the <codeph>-</codeph>
                  chracter:<ul id="ul_lz5_rbf_bt">
                  <li>
                    <p>Cannot start or end with the <codeph>-</codeph> chracter;</p>
                  </li>
                  <li>
                    <p>Must be greater than 0 characters;</p>
                  </li>
                  <li>
                    <p>No restriction on the number of letters or digits (0 or more).</p>
                  </li>
                </ul></p></li>
            <li><codeph>memphis</codeph> is the name of the template to use. The installation kit
              includes a template called <codeph>memphis</codeph> designed to install HP Helion
              OpenStack, specific to the KVM + ESX deployment.</li>
          </ul><p>The command creates the <codeph>&lt;cloudname&gt;</codeph> directory, which
            contains several JSON template files. </p></li>
      </ol>
    </section>
    <section id="configure-node-provision-json">
      <title>Configure the node-provision JSON file</title>
      <p>Modify the <codeph>node-provision.json</codeph> file in the <codeph>&lt;cloudname></codeph>
        directory of the HLM VM. This file supplies input values to the <codeph>hprovision</codeph>
        script, later in the installation. </p>
      <p><b>Note:</b> To see a sample <codeph>node-provision.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-node-json.dita">Sample node-provision.json File for
          Installing the KVM + ESX Topology</xref>.</p>
      <ol id="ol_ub2_xfl_zs">
        <li>Edit <codeph>node-provision.json</codeph> file to change only the following fields: 
          <table>
            <tgroup cols="2">
              <colspec colname="col1" colsep="1" rowsep="1"/>
              <colspec colname="col2" colsep="1" rowsep="1"/>
              <thead>
                <row>
                  <entry colsep="1" rowsep="1">Field</entry>
                  <entry colsep="1" rowsep="1">Description</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>Pxe-mac-address</entry>
                  <entry>The MAC address of the interface you want to PXE boot onto. This is not
                    same as iLO* MAC address.</entry>
                </row>
                <row>
                  <entry>pm_ip</entry>
                  <entry>The power management IP address (iLO IP address).</entry>
                </row>
                <row>
                  <entry>pm_user</entry>
                  <entry>The power management user name (iLO user name).</entry>
                </row>
                <row>
                  <entry>pm_pass</entry>
                  <entry>The power management password (iLO password).</entry>
                </row>
                <row>
                  <entry>failure_zone, vendor, model, os_partition_size, data_partition_size</entry>
                  <entry>The values in these fields are added to the <codeph>nodes.json</codeph>
                    file used during cloud deployment. </entry>
                </row>
              </tbody>
            </tgroup>
          </table><p>*iLO is HP Integrated Lights-Out, a server management tool embedded with the
            ProLiant servers. Consult your iLO documentation for information on how to locate the
            iLO IP address, user name, and password.</p></li>
        <li>Save and close the file.</li>
      </ol>
    </section>
    <p>
      <b>Configure PXE boot</b>
    </p>
    <p>After you edit the <codeph>node-provision.json</codeph> file, you must enable one-time PXE
      boot on the servers to set the correct boot order. Execute the following on the HLM VM:</p>
    <ol>
      <li>Copy the <codeph>ilopxebootonce.py</codeph> from the
          <systemoutput>/root/cg-hlm/dev-tools/ilopxebootonce.py</systemoutput> to the
          <codeph>&lt;cloudname></codeph> directory where you have the
          <codeph>node-provision.json</codeph> file.</li>
      <li>Execute the following script:
        <codeblock>python ilopxebootonce.py node-provision.json</codeblock></li>
    </ol>
    <p>After the script is run, the <codeph>Current One-Time Boot Option</codeph> is set to
        <codeph>Network Device 1</codeph> on all the servers listed in
        <codeph>node-provision.json</codeph> file.</p>
    <section id="configure-def-json">
      <title>Configure the definition.json file</title>
      <p>Modify the <codeph>definition.json</codeph> file in the <codeph>&lt;cloudname></codeph>
        directory of the HLM VM to define the number of ESX compute proxies required in your
        environment. You need one proxy per vCenter; make sure this value is set to
        2<!--set this value to the number of vCenters you have.-->, as shown in the following
        example.</p>
      <p><b>Note:</b> To see a sample <codeph>definition.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-def-json.dita#topic4797">Sample definition.json File
          for Installing the KVM + ESX Topology</xref>.<ol id="ol_zzr_khl_zs">
          <li>Set the number of compute systems to 2 for  an HA VRS-G
            installation.<codeblock>"file": ".hos/ccp-vrsg.json",
"count": 2</codeblock></li>
        </ol></p>
    </section>
    <section id="configure-env-json">
      <title>Configure the environment.json file</title>
      <p>Modify the <codeph>environment.json</codeph> file in the <codeph>&lt;cloudname></codeph>
        directory of the HLM VM to configure the VLANs and network addresses as appropriate for your
        environment. Configure the CLM (management), CAN (api), and BLS (blockstore) networks.</p>
      <p><b>Note:</b> To see a sample <codeph>environment.json</codeph> file, see one of the
        following: <ul id="ul_smq_yqt_bt">
          <li><xref href="carrier-grade-install-pb-kvm-env-json.dita">For an Unbonded NIC
              Environment</xref>. <p>For each network provide:
            </p><codeblock>{
              "name": "management",
              "type": "vlan",
              "segment-id": "1551",
              "network-address": {
              "cidr": "10.200.51.0/24",
              "start-address": "10.200.51.100",
              "gateway": "10.200.51.1"
              }
              },        </codeblock></li>
          <li><xref href="carrier-grade-install-pb-kvm-env-json-bonded.dita#topic4797">For a Bonded
              NIC Environment</xref></li>
        </ul></p>
    </section>
    <section id="configure-ansible-json">
      <title>Configure the ansible.json file</title>
      <p>Modify the <codeph>ansible.json</codeph> file in the <codeph>&lt;cloudname>/vars</codeph>
        directory of the HLM VM.</p>
      <p>To see a sample <codeph>ansible.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-ansible-json.dita">Sample ansible.json File for
          Installing the KVM + ESX Topology</xref>.</p>
    </section>
    <section id="configure-esx-json">
      <title>Configure the esx.json file</title>
      <p>Modify the <codeph>esx.json</codeph> file in the <codeph>&lt;cloudname>/vars</codeph>
        directory on the HLM VM. This file is called by the script that installs the HP Helion
        OpenStack cloud, later in the installation. </p>
      <p>To see a sample <codeph>esx.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-esx-json.dita">Sample esx.json File for Installing the
          KVM + ESX Topology</xref>.</p>
    </section>
    <section id="configure-ldap-json">
      <title>Configure the ldap.json file</title>
      <p>Modify the <codeph>ldap.json</codeph> file in the <codeph>&lt;cloudname>/vars</codeph>
        directory of the HLM VM.</p>
      <p>To see a sample <codeph>ldap.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-ldap-json.dita">Sample ldap.json File for Installing
          the KVM + ESX Topology</xref>. </p>
    </section>
    <section id="configure-wr-json">
      <title>Configure the wr.json file</title>
      <p>Modify the <codeph>wr.json</codeph> file in the <codeph>&lt;cloudname>/vars</codeph>
        directory on the HLM VM.</p>
      <p>To see a sample <codeph>wr.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-wr-json.dita">Sample wr.json File for Installing the
          KVM + ESX Topology</xref>. </p>
    </section>
    <section>
      <title>Configure the dcn.json file</title>
      <p>Modify the <codeph>dcn.json</codeph> file in the
          <codeph>/opt/share/hlm/1/site/services/dcn.json</codeph> directory of the HLM VM to remove
          <b>both</b> instances of the following code:
        <codeblock>      {
          "description": "Ingress 47 GRE",
          "firewall": {
            "direction": "in",
            "protocol": "gre"
            },
           "port": 47,
            "scope": "private"
      },    </codeblock></p>
    </section>
    <section id="configure-mach-json">
      <title>Configure the machine_architecture.json file</title>
      <p><b>For bonded NIC environments</b>, modify the <codeph>machine_architecture.json</codeph>
        file in the <codeph>/opt/share/hlm/1/site/</codeph> directory of the HLM VM to fit your
        hardware model. You have to gather the correct PCI SLOT INFO and add ports accordingly on
        the hardware slots or cards used for networking.</p>
      <p><b>Note:</b> To see a sample <codeph>machine_architecture.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-env-json-bonded.dita#topic4797">Sample enviroment.json
          File for Installing the KVM + ESX Topology in an Bonded NIC Environment</xref>.</p>
    </section>
    <section>
      <title>Verify the JSON files</title>
      <p>After editing the JSON files, validate each JSON file to make sure there are no syntax
        errors using the tool of your preference. For example, using the Python json.tool: </p>
      <codeblock>python -m json.tool &lt;filename>.json</codeblock>
    </section>
    <section>
      <title>Configure the ESX Compute Proxy</title>
      <p>The HP Helion OpenStack Carrier Grade vCenter ESX compute proxy (compute proxy) is a driver
        that enables the Compute service to communicate with a VMware vCenter server. The HP Helion
        OpenStack Compute Service (Nova) requires this driver to interface with VMware ESX
        hypervisor APIs. You can download the proxy installation files from the <xref
          href="https://helion.hpwsportal.com/catalog.html#/Home/Show" format="html"
          scope="external">Helion Download Network</xref>.
        <!-- Review with Michael Duncan all JSON files and proxy re: bonded set up --></p>
      <p>For instructions on installing the ESX compute proxy, see <xref
          href="carrier-grade-install-esx-proxy.dita#topic10581">Deploy the ESX Compute
        Proxy</xref>.</p>
    </section>
    <section id="create-a-new-cloud-template-and-bring-the-cloud-nodes-up">
      <title>Provision the cloud nodes and bring the cloud nodes up</title>
      <ol>
        <li>On the HLM host, use the following script to start the provisioning of the HP Helion
          OpenStack cloud: <codeblock>hprovision &lt;cloudname&gt;          </codeblock><p>Where:
              <codeph>&lt;cloudname&gt;</codeph> is the name of the cloud you created. The script
            takes approximately 15 to 30 minutes.</p><p>This script prepares the bare metal servers
            for the HP Helion OpenStack Carrier Grade cloud installation including the hLinux
            operating system. The script also PXE boots the nodes specified in
              <codeph>node-provision.json</codeph> file and tracks the PXE boot completion process.
            The script also creates the <codeph>nodes.json</codeph> file in the directory.
            </p><p>You can log in to the iLO server management tool for each of the nodes to monitor
            the boot process. Consult yout iLO documentation for information on how to log into iLO.
          </p></li>
        <li>Make sure the nodes are booted up using iLO. </li>
        <li>Once the baremetal nodes are provisioned, make sure the <codeph>nodes.json</codeph> file
          is generated. The <codeph>nodes.json</codeph> file will have entries of 3 controllers, 2
          DCN Hosts, 2 VRS-G and the compute proxy node.</li>
        <li>Verify that each node in the <codeph>nodes.json</codeph> file is installed and active. <ol>
            <li>Ping proxy node from HLM VM with the IP using IP specified in
                <codeph>nodes.json</codeph> file.</li>
            <li>SSH to the compute proxy node from the HLM VM using IP specified in
                <codeph>nodes.json</codeph> file.</li>
          </ol></li>
        <li>Configure the back-end drivers to enable management of the OpenStack Block Storage
          volumes on vCenter-managed data stores and 3PAR and/or VSA storage arrays. The HP Block
          Storage (Cinder) service allows you to configure multiple storage back-ends. Use the
          following steps to configure back-end support:<ol id="ul_btr_l52_xs">
            <li>Change to the <codeph>/cinder/blocks</codeph> directory:
              <codeblock>cd ~/&lt;cloudname>/services/cinder/blocks</codeblock> Where
                <codeph>&lt;cloudname></codeph> is the name you assigned to the cloud. The directory
              contains several sample Cinder configuration files that you can edit, depending upon
              which storage method(s) you are using.</li>
            <li>Depending upon the type of storage you are using, edit the following file: <ul>
                <li><codeph>cinder_conf.multiBackendSample</codeph> - this sample file provides all
                  the variables needed to use ESX in the non-KVM region and HP StoreVirtual VSA
                  and/or HP StoreServ (3PAR) attched to the KVM region;</li>
              </ul><p>Use the <codeph>enabled_backends</codeph> variable to list each of the
                back-ends you are using. You must specify at least one back-end for the non-KVM
                region and one or more for the KVM region.</p><table>
                <tgroup cols="3">
                  <colspec colname="col1" colsep="1" rowsep="1"/>
                  <colspec colname="col2" colsep="1" rowsep="1"/>
                  <colspec colname="col3" colsep="1" rowsep="1"/>
                  <thead>
                    <row>
                      <entry colsep="1" rowsep="1">Region/Hypervisor</entry>
                      <entry colsep="1" rowsep="1">Volume Backend</entry>
                      <entry colsep="1" rowsep="1">Backend Name</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>KVM</entry>
                      <entry>3PAR </entry>
                      <entry>hp3par</entry>
                    </row>
                    <row>
                      <entry>KVM</entry>
                      <entry>VSA</entry>
                      <entry>hplefthand</entry>
                    </row>
                    <row>
                      <entry>Non-KVM </entry>
                      <entry>ESX Datastores</entry>
                      <entry>vmdk </entry>
                    </row>
                  </tbody>
                </tgroup>
              </table><p><b>Note:</b> You can use either 3PAR and VSA in the KVM region or select
                both if you have respective storage arrays </p><p id="storage">A typical
                  <codeph>cinder_conf</codeph> that enables all three back ends appears similar to
                the following
              example:</p><codeblock>[DEFAULT]
enabled_backends=vmdk, hp3par, hplefthand
...                                
[vmdk]
volume_backend_name = &lt;mybackendname3>
vmware_host_ip = &lt;hostip>
vmware_host_username = &lt;username>
vmware_host_password = &lt;password>
vmware_volume_folder = &lt;volumes_folder>
vmware_image_transfer_timeout_secs = 7200
vmware_task_poll_interval = 0.5
vmware_max_objects_retrieval = 100
volume_driver = cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver
          
[hp3par]
volume_backend_name=&lt;mybackendname1>
hp3par_api_url=https://&lt;management_ip>:8080/api/v1
hp3par_username=&lt;username>
hp3par_password=&lt;password>
hp3par_cpg=&lt;cpg>
san_ip=&lt;san_ip>
san_login=&lt;username>
san_password=&lt;password>
hp3par_iscsi_ips=&lt;iscsi_target_ips seperated by ,>
volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
hp3par_debug=False
hp3par_iscsi_chap_enabled=false
hp3par_snapshot_retention=48
hp3par_snapshot_expiration=72
                            
[hplefthand]
volume_backend_name=&lt;mybackendname2>
hplefthand_username = &lt;username>
hplefthand_password = &lt;password>
hplefthand_clustername = &lt;Cluster Name>
hplefthand_api_url = https://&lt;Iscsi Virtual IP address>/lhos </codeblock></li>
            <li>Save the file to <codeph>cinder_conf</codeph>.</li>
          </ol></li>
      </ol></section>
        
        <section id="proxy-patch">
      <title>Apply the ESX compute proxy patch</title>
      <p>This patch fixes an intermittent issue where the ESX compute proxy is not configured after
        the <codeph>hprovision</codeph> completes successfully. The patch increase the timeout to
        allow enough time for the proxy VM to boot. </p>
      <p>For more information, see the README in the download or <xref
          href="carrier-grade-install-esx-proxy-patch.dita#topic10581">Apply the ESX Compute Proxy
          Patch</xref>.</p>
    </section>
        <section>
      <title>Apply the Generic Routing Encapsulation (GRE) patch</title>
      <p>This patch contains three files. The first file fixes an issue with the IP tables role
        failing during installation while enabling Generic Routing Encapsulation (GRE) in firewall
        in VRS-G nodes. The other two files enable GRE on VRS-G nodes in the IP tables.  </p>
      <p>For more information, see the README in the download or <xref
          href="carrier-grade-install-gre-patch.dita#topic10581">Applying the Generic Routing
          Encapsulation (GRE) Patch</xref>.</p>
      <title>Apply the RabbitMQ Random Password patch</title>
      <p>For information, see the README in the download or <xref
          href="carrier-grade-install-rabbitmq-patch.dita#topic10581">Applying the Random RabbitMQ
          Password Patch</xref>.</p>
    </section>
        
        <section><title>Apply the Reverse Path Filter Patch</title>After deploying the HP OpenStack
      cloud, you might not be able to access the Horizon interface and the OpenStack services cannot
      be accessed over the CAN network. <p>To prevent this problem, install a patch to configure the
        kernel to set the reverse path filter (<codeph>rp_filter</codeph>) to 1 in the
          <codeph>all</codeph> and <codeph>default</codeph> configuration files. </p><p>For more
        information, see the README in the download or <xref
          href="carrier-grade-install-rp-filter.dita#topic10581">Applying the Reverse Path Filter
          Patch</xref>.</p></section>
        
        <section><title>Apply the Helion Dynamic Password Patch</title>The HP Helion OpenStack cloud
      deployment creates a default password to log into the Helion interface. You can deploy a patch
      to configure the installer to generate a dynamic user name and password for the Helion
      interface. The user name can password can be retrieved from a configuration files. <p>For more
        information, see the README in the download or <xref
          href="carrier-grade-install-helion-password-patch.dita#topic10581">Applying the Helion
          Dynamic Password Patch</xref>.</p></section>
        
        <section><title>Apply the NIC bonding patch</title><p>HP Helion OpenStack Carrier Grade environments using NIC bonding, see the README in the download
        or <xref href="carrier-grade-install-nic-patch.dita#topic10581">Applying the NIC Bonding
          Patch</xref> to install a required patch.</p></section>
        
    <section><title>Deploy the HP Helion OpenStack cloud</title>
      <ol>
        <li>Run the HP Helion OpenStack Configuration Processor:
            <codeblock>hcfgproc -d definition.json</codeblock><p>The HP Helion OpenStack
            Configuration Processor is a script, called <codeph>hcfgproc</codeph>, that is
            incorporated into the installation environment. This command generates the necessary
            configuration for the cloud. </p><p>When the command completes, you will see the
            following:
            message:</p><codeblock>#################################################################################
The configuration processor completed successfully.
################################################################################</codeblock><p>To
            view the complete output of the command, see <xref
              href="carrier-grade-install-hcfgproc-output.dita#topic10581"> Output from the hcfgproc
              command</xref>.</p></li>
        <li>Review the CloudDiagram, <codeph>hosts.hf</codeph>, and
            <codeph>net/interfaces.d/eth.cfg</codeph> files to make sure the network settings are
          correct.</li>
        <li>Initialize network interfaces on all the cloud nodes using the following command:
            <codeblock><codeph>hnetinit &lt;cloudname&gt;</codeph></codeblock><p>Where:
              <codeph>&lt;cloudname&gt;</codeph> is the name of the cloud you created.</p><p>After
            this command completes, all cloud nodes and CLM network interfaces are configured. The
            output for the command should appear similar to the following:</p><p><image
              href="../../media/CGH-install-hnetinit-output.png" id="image_tft_r11_1t"/></p></li></ol>
    </section>
    <section><title>Deploy HP Helion OpenStack</title>
      <ol>
        <li>Use the following command to deploy the HP Helion OpenStack cloud:
            <codeblock><codeph>hdeploy &lt;cloudname&gt;</codeph></codeblock><p>This command takes a
            significant period of time. When this command is complete, the non-KVM cloud
            installation is complete. Use the following sections to configure the Horizon interface,
            configure networking, and configure the ESX environment. </p></li>
        <li>Enter <codeph>exit</codeph> to leave the root shell.</li>
      </ol>
    </section>
    <section id="patch-after">
      <title>Configure LDAP to enable CLI and use Keystone v3</title>
      <p>By default, the HP Helion OpenStack Carrier Grade services are configured to use Keystone
        v2 authorization. The services need to be modified to use Keystone v3.  Users will not be
        able to execute OpenStack CLI commands until the specific changes are made on all three
        controller nodes in the non-KVM region. </p>
      <p>For information, see <xref href="carrier-grade-install-config-ldap3.dita#topic10581">Configuring LDAP CLI Support</xref></p>
    </section>
    <section><title>Applying the HLM Port Security Patch</title>This patch secures all ports on the
      HLM VM. The standard protocol ports will allow inbound access only and the only the cloud
      nodes will have inbound access to the other ports of the HLM VM.<p>For more information, see
        the README in the download or <xref
          href="carrier-grade-install-secure-port-patch.dita#topic10581">Applying the HLM Port
          Security Patch</xref>. </p></section>
    <section>
      <title>Disable the Root User</title>
      <p>For security purposes, we strongly recommend disabling root. After the
          <codeph>root</codeph> user is disabled, you can use another default user account,
          <codeph>cghelion</codeph> to access the HLM VM and any actions that require root access
        should be done using <codeph>sudo</codeph>.</p>
      <p>For more information, see <xref href="carrier-grade-install-disable-root.dita#topic10581"
          >Disabling the Root User</xref>.</p>
    </section>
    
    <section id="next-step">
      <title>Next Step</title>
      <p><xref href="carrier-grade-install-launch-horizon.dita#topic10581">Launching the Horizon Interface</xref></p>
      <!--<p><xref href="carrier-grade-install-kvm-cloud-GA.dita">Deploying the KVM Region</xref></p>-->
    </section>
  </body>
</topic>
