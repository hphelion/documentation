<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic10581">
<title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 1.1: Deploying the Non-KVM
    Cloud</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
</metadata>
</prolog>
<body>
<p>
<!--UNDER REVISION-->
 <!--./CarrierGrade/Installation/carrier-grade-install-pb-hlm-vm.md-->
 <!--permalink: /helion/openstack/carrier/install/pb/hlm-vm/--></p>
  <p>The first phase of the HP Helion Openstack Carrier Grade installation involves creating a virtual machine for the Helion Lifecycle Management (HLM) and then deploying the HP Helion Openstack cloud.</p>
  <p>HLM consists of the ongoing operations/maintenance as well as the initial deployment of the HP Helion OpenStack Carrier Grade system.</p>

<section id="logging"> <title>About Installation Logging</title>
<p>For all scripts that are executed (including <codeph>hlm-build</codeph> and <codeph>h*</codeph> scripts below), separate log files are generated on <codeph>/var/log/hlm/</codeph> folder.</p>
<p>For example, if you run the <codeph>hprovsion</codeph> command, all command line output will also be logged in <codeph>/var/log/hlm/hprovision.log</codeph> file.</p>
<p>The following is the list of scripts for which logging is implemented:</p>
<ul>
<li>hlm_updatepackages.sh</li>
<li>hlm_initcobbler.sh</li>
<li>hlm_importiso.sh</li>
<li>hlm_prepareenv.sh</li>
<li>hnewcloud</li>
<li>hprovision</li>
<li>hcfgproc</li>
<li>hnetinit</li>
</ul>
</section>
<section id="prepare"> <title>Prepare the system for deployment</title>
<p>Use the following steps to prepare the server on which the HLM VM will be deployed (the HLM host):</p>
<ol>
<li>Download and copy the installer files and Ansible playbooks to KVM host.<ol id="ol_lnw_nn2_ns">
            <li>Copy the <codeph>cg-hlm.qcow2</codeph> file to <codeph>/home/images</codeph>.</li>
            <li>Copy the <codeph>cg-hos-dcn-vsd.tar.gz.gpg</codeph> to the
                <codeph>/home/images</codeph> directory.</li>
          
      <li>Decrypt and untar the PGP file using password <codeph>cghelion</codeph> when prompted.  
        <codeblock>gpg -d cg-hos-dcn.tar.gz.gpg | tar -xzvf</codeblock></li>
      <li>Copy the <codeph>infra-ansible-playbooks</codeph> file to <codeph>/root</codeph>.</li> 
      <li>Decrypt and untar the file using password <codeph>cghelion</codeph> when prompted.</li>
      <li>Copy the <codeph>cg-hos.tar.gz.gpg</codeph> file to <codeph>/root/cg</codeph>. Create the directory if it does not exist.</li>
      <li>Copy the <codeph>cg-hos-dcn-vsc.tar.gz.gpg</codeph> file to <codeph>/root/cg</codeph>.</li>
        <li>Copy the following files from the <codeph>Patches/build-33</codeph> folder<codeblock>
      hnewcloud.sh
ansible.cfg
ilopxebootonce.py
venom-patch.tar.gz
</codeblock></li>
  <li>Extract the <codeph>venom-patch.tar.gz</codeph> and follow the readme file. The <codeph>cg-hlm.qcow2</codeph> and <codeph>VSD-3.0.0_HP_r3.0_16.qcow2</codeph> are in the <codeph>/home/images</codeph> folder. Refer to the <codeph>group_vars/all</codeph> file on the HLM host if you need the location of the <codeph>/home/images</codeph> file.</li></ol></li>

  <li>Edit the <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file for your environment.
          For information on each variable, refer to the comments in the file with each variable.</li>
<li>Check the hosts file <codeph>/root/infra-ansible-playbooks/hosts</codeph> file and make sure it
          appears similar to the following. Make sure the VSD IP (DCM network) is
          correct. Also, update the vsd CLM IP in this file.</li>
</ol>
<codeblock>  
        [vsd]
        10.200.50.21 ansible_ssh_user=root ansible_ssh_pass=Alcateldc

        [hlm]

        [hlm_kvm_host]
        192.168.122.1 
</codeblock>
</section>
<section id="deploy-the-hlm-and-vsd-vm"> <title>Deploy the HLM Cloud and VSD VM</title>
  <p>Use the following steps to bring up the HLM cloud and create a VM for the HP Virtualized Services Directory (VSD) on the HLM Host using Ansible playbooks.</p>
<ol>
<li>Make sure the Ansible playbook file is not in executable mode.</li>
<li>Execute the following command:<codeblock>ansible-playbook -i hosts setup_hlm_onBM.yml</codeblock>
  The command will do the following:<ul>
    <li>Copy both installation files (tar balls) to your HLM server, decrypt, and extract the files.</li>
    <li>Execute the <codeph>updatepackages</codeph> command.</li>
    <li>Execute the <codeph>prepareenv</codeph> command.</li>
    <li>Execute the <codeph>Init cobbler</codeph> command.</li>
    <li>Execute the <codeph>Importiso</codeph> command.</li></ul>

You will see similar message when the playbook is run successful.</li>
</ol>
<p><image href="../../media/CGH-Install-Ansible.png"/></p>
</section>

<section id="apply-the-patch-to-fix-qemu-venom-vulnerability"> <title>Apply the patch to fix QEMU Venom vulnerability</title>
<p>There is a <xref href="https://bugs.launchpad.net/openstack-ansible/+bug/1454677" scope="external" format="html" >known vulnerability</xref> in <xref href="http://wiki.qemu.org/Main_Page" scope="external" format="html" >QEMU</xref>.</p>
<p>Use the following steps to patch the vulnerability:</p>
<ol>
<li>Copy the <codeph>venom-patch.tar.gz</codeph> file to the HLM VM and extract the files. Follow
          instructions in Readme.txt file that is in the extracted files.</li>
<li>Copy the <codeph>packages-patch.tar.gz.gpg</codeph> to <codeph>/root</codeph> directory on HLM
          VM.</li>
<li>Decrypt and extract the <codeph>packages-patch.tar.gz.gpg</codeph> file, using the password
            <codeph>cghelion</codeph> when
          prompted:<codeblock><codeph>gpg -d packages-patch.tar.gz.gpg | tar -xzvf -
</codeph></codeblock></li>
<li>Execute the following command to update the package list on the HLM
          VM:<codeblock><codeph>rm /root/packages/repo/debian/qemu*2.1.3*.deb
apt-get update
apt-get -y --force-yes install dpkg-dev
cd /root/packages/repo/debian
dpkg-scanpackages . /dev/null | gzip -9c &gt; Packages.gz
</codeph></codeblock></li>
<li>Execute the following command to update the package repository on the HLM
          VM:<codeblock><codeph>cd /root/cg-hlm/hlm-build
./hlm_updatepackages.sh
</codeph></codeblock></li>
</ol>
</section>
  
<section> <title>Coniguring the VSD node, creating user and applying License</title>
  <p>During the <i>Deploy the HLM Cloud and VSD VM</i> section, a vritual machine is created to host VSD. </p>
<p>You need to log into the VSD node (using SSH), then create a default user and apply the required license.</p>
  
<p>After logging into the VDS node, execute the following command to launch a console window:</p>
<codeblock>virsh console vsd</codeblock>
<p>You should see the status as below from VSD VM.</p>
<p>
  <image href="../../media/CGH-install-virsh-vsd.png"/>
</p>
</section>
<section id="vsd-performance-workaround"> <title>VSD Performance workaround</title>
<p>By default VSD has only 8G memory. For better performance behavior, you can update the VSD memory to 16G.</p>
<ol>
<li>From HLM host, execute the following command to power down the VSD node:
          VM:<codeblock>virsh destroy vsd</codeblock></li>
<li>Execute the following command to edit the memory setting in the VSD XML
          file:<codeblock><codeph>virsh edit vsd 

Current value 
    &lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;8388608&lt;/currentMemory&gt;

Change to 
    &lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;16777216&lt;/currentMemory&gt;
</codeph></codeblock></li>
<li>Save the value<codeblock><codeph>virsh save vsd
</codeph></codeblock></li>
<li>Use the following command to start the
            VM:<codeblock><codeph>virsh start vsd
</codeph></codeblock><p>It can take 15 minutes or
            more to sync with NTP and to get all the other VSD services up.</p></li>
</ol>
</section>
<section id="launch-vsd-dashboard"> <title>Launch VSD Dashboard</title>
<p>On the HLM host:</p>
<ol>
<li>Using an browser such as Chrome or FireFox, navigate to the DCM IP of VSD.</li>
<li>In the log in page, enter the default
          credentials:<codeblock><codeph>User Name: Csproot 
Password: csproot 
Org: csp 
VSD Server : auto 
</codeph></codeblock></li>
</ol>
</section>
<section id="applying-the-license"> <title>Applying the License</title>
<p>To install the required DCN license:</p>
<ol>
<li>From VSD Dashboard, click the <b>Open VSP Configuration</b> tab on the top right corner of the
          dashboard.</li>
<li>Click the <b>Licenses</b> tab and click <b>+</b>.</li>
<li>Copy and paste the license that you have receive into the screen that displays.</li>
</ol>
</section>
<section id="create-user-for-plugin-login"> <title>Create User for Plugin Login</title>
<p>You must create a user and add it to CMS Group.</p>
<ol>
<li>From VSD Dashboard, click the <b>Open VSP Configuration</b> tab on the top right corner of the
          dashboard.</li>
<li>Click the <b>CSP Users</b> tab and click <b>+</b>.</li>
<li>Create a user named <codeph>OSadmin</codeph> with the password <codeph>OSadmin</codeph>.</li>
<li>Add the user to the <codeph>CMS Group</codeph>.</li>
</ol>
</section>
<section id="configure-a-json-file-for-installation"> <title>Configure a JSON file for installation</title>
<p>The HP Helion OpenStack VM deployment requires a JSON file. Use the following steps to install and edit the file.</p>
<ol>
<li>On the HLM host, change to the home directory.<codeblock>cd ~
</codeblock></li>
<li>Provision and configure your HP Helion OpenStack VM.<codeblock>hnewcloud  &lt;cloudname&gt; &lt;cloud_template&gt;
</codeblock><p>Where:</p><ul>
            <li>cloudname is the name of the cloud to create</li>
            <li>cloud_template is the name of the template to use.</li>
          </ul><p>The command creates the <codeph>&lt;cloudname&gt;</codeph> directory, which will
            contain a JSON template file <codeph>node-provision.json</codeph>. This template
            supplies input values to the <codeph>hprovision</codeph> script, later in the
            installation.</p></li>
<li>Edit <codeph>node-provision.json</codeph> file based on following guidelines:<table>
            <tgroup cols="2">
              <colspec colname="col1" colsep="1" rowsep="1"/>
              <colspec colname="col2" colsep="1" rowsep="1"/>
              <thead>
                <row>
                  <entry colsep="1" rowsep="1">Field</entry>
                  <entry colsep="1" rowsep="1">Baremetal</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>name</entry>
                  <entry>Name of the system you want to add</entry>
                </row>
                <row>
                  <entry>Pxe-mac-address</entry>
                  <entry>MAC address of the interface you want to PXE boot onto. This is not same as
                    iLO MAC address,</entry>
                </row>
                <row>
                  <entry>Pxe-interface</entry>
                  <entry>The name of the interface on which PXE boot should occur. For example: <codeph>eth0</codeph></entry>
                </row>
                <row>
                  <entry>pm_type</entry>
                  <entry>ipmilan </entry>
                </row>
                <row>
                  <entry>pm_ip</entry>
                  <entry>Power management IP (ilo ip)</entry>
                </row>
                <row>
                  <entry>pm_user</entry>
                  <entry>Power management user (ilo username)</entry>
                </row>
                <row>
                  <entry>pm_pass</entry>
                  <entry>Power management password (ilo password)</entry>
                </row>
                <row>
                  <entry>node_group</entry>
                  <entry>Enter the same value as <codeph>node-type</codeph> in the <codeph>nodes.json</codeph>
                    file used during cloud deployment. For example: `CCN-001-001`.</entry>
                </row>
                <row>
                  <entry>failure_zone, vendor, model, os_partition_size, data_partition_size</entry>
                  <entry>Enter the same value as for these fields an in the <codeph>nodes.json</codeph> file used
                    during cloud deployment</entry>
                </row>
              </tbody>
            </tgroup>
          </table><p>To see a sample <codeph>node-provision.json</codeph> file, see <xref
              href="../../CarrierGrade/Installation/carrier-grade-install-pb-hlm-vm-json.dita">Create the HLM Virtual Machine</xref>.</p></li>
</ol>
</section>
<section id="pxe-boot"> <title>Configure PXE boot</title>
<p>After you edit the <codeph>node-provision.json</codeph> file, you must enable one-time PXE boot on cloud nodes to set the correct boot order. Execute the following on the HLM VM</p>
<ol>
<li>Use the following command to install the <codeph>python-hpilo</codeph> module on HLM VM:<codeblock><codeph>pip install python-hpilo
</codeph></codeblock><p>
            <codeph>python-hpilo</codeph> is a python library and command-line tool for
          iLO.</p></li>
<li>Copy the <codeph>ilopxebootonce.py</codeph> from the <codeph>Patches/build-33 folder</codeph>
          you <xref type="section" href="#topic10581/prepare">downloaded earlier</xref> to directory
          where you have <codeph>node-provision.json</codeph>.</li>
<li>Execute the following
          script<codeblock><codeph>python ilopxebootonce.py node-provision.json
</codeph></codeblock></li>
</ol>
<p>After the script is run, the <codeph>Current One-Time Boot Option</codeph> is set to <codeph>Network Device 1</codeph> on all the servers listed in <codeph>node-provision.json</codeph> file.</p>
</section>
<section id="create-a-new-cloud-template-and-bring-the-cloud-nodes-up"> <title>Create new cloud template and bring the cloud nodes up</title>
<ol>
<li>Use the following script to start the provisioning of the HLM VM:<p>hprovision
              <codeph>&lt;cloudname&gt;</codeph>
          </p><p>Where:</p><ul>
            <li>cloudname is the name of the cloud to create</li>
          </ul><p>This script will PXE boot the nodes specified in
              <codeph>node-provision.json</codeph> file. The script alsos track the PXE boot
            completion process and will create the <codeph>nodes.json</codeph> file in the
            directory. </p></li>
  <li>Update the <codeph>node-provision.json</codeph> file used in the <xref type="section" href="#topic10581/pxe-boot">previous step</xref>.<p>a. Change to
            the <codeph>&lt;cloudname&gt;</codeph>
            directory:</p><codeblock><codeph>cd ~/&lt;cloudname&gt;
</codeph></codeblock><p>b.
            Verify that the each PXE-booted node has an IP address that matches the IP address
            specified in the <codeph>node-provision.json</codeph> file.</p></li>
<li>Modify the <codeph>environment.json</codeph> file to configure the VLANs and network addresses
          as appropriate for your environment.<codeblock>"cidr": "start-address": </codeblock>
            
         The three controller nodes should have CLM, CAN, EXT, BLS on eth0 and TUL on
                eth1.
           <!--Hiding for RC0 
              <p>The two compute nodes should have CLM, EXT, BLS on eth0 and TUL on eth1.</p>
           -->
          <p>
            <b>Example:</b>
          </p><codeblock>"cidr": "192.168.101.0/24",
"start-address": "192.168.101.100"
</codeblock><p>
            <b>NOTE:</b> The Helion Configuration Processor assigns the first address of the CLM
            address range to itself for serving python and debian repositories. Make sure that you
            set the first IP address of the CLM range for the eth2 (CLM) address of the HLM
            node.</p></li>
<li>Modify the <codeph>definition.json</codeph> file:
  
  <ol><li>Set the number of compute systems to 2.

    <codeblock><codeph>"count": 2, //number of computes in the resource pool</codeph></codeblock>.</li>

    <li>Update `ansible-vars` section with all the information based on your setup.</li>

    <li>ake sure you have two NTP entries at the end of this `definition.json` file as seen in the snapshot. If you have only one NTP server in your environment, specify the same NTP server twice.</li></ol>
</li>
<li>Once you have correctly edited all the json "Cloud Model" files, run the HP  Helion OpenStack Configuration
          Processor on
            these<codeblock><codeph>cd /usr/local/bin

hcfgproc -d definition.json
</codeph></codeblock><p>The
              <codeph>hcfgproc</codeph> script gets installed in <codeph>/usr/local/bin</codeph> by
            the <codeph>prepare-env</codeph> script. The script generates a <codeph>clouds/</codeph>
            directory within the  directory. </p></li>
<li>Review the CloudDiagram, <codeph>hosts.hf</codeph>, and
            <codeph>net/interfaces.d/eth.cfg</codeph> files to make sure the network settings are
          correct.</li>
<li>Initialize network interfaces on all the cloud nodes using the following
            command:<codeblock><codeph>hnetinit &lt;cloudname&gt; 
</codeph></codeblock><p>You can
            run this command from any directory.</p></li>
</ol>
<p>After this command completes, all cloud nodes and CLM network interfaces should be set correctly.</p>
</section>
<section id="next-step"> <title>Next Step</title>
<p>
  <xref href="../../CarrierGrade/Installation/carrier-grade-install-pb-workarounds.dita" >HLM Post-Installation Tasks</xref>
</p>
<p>
  <xref type="section" href="#topic10581"> Return to Top </xref>
</p>
<!-- ===================== horizontal rule ===================== -->
</section>
</body>
</topic>
