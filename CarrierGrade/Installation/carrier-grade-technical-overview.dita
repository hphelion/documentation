<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic3485">
<title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 1.1: Technical Overview</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HP Helion Openstack"/>
<othermeta name="product-version" content="HP Helion Openstack 1.0"/>
<othermeta name="product-version" content="HP Helion Openstack 1.0.1"/>
<othermeta name="product-version" content="HP Helion Openstack 1.1"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Sameer V, Pranoy R, Michael B"/>
<othermeta name="product-version1" content="HP Helion Openstack"/>
<othermeta name="product-version2" content="HP Helion Openstack 1.0"/>
<othermeta name="product-version3" content="HP Helion Openstack 1.0.1"/>
<othermeta name="product-version4" content="HP Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--UNDER REVISION-->
 <!--./CarrierGrade/Installation/carrier-grade-technical-overview.md-->
 <!--permalink: /helion/openstack/carrier/technical-overview/--></p>
<p> <xref href="../../CarrierGrade/Installation/carrier-grade-install-playbook-overview.dita" >▲ Installation Overview</xref> | <xref href="../../CarrierGrade/Installation/carrier-grade-support-matrix.dita" >Support Matrix ▶</xref>
</p>
<p>This page introduces you to HP Helion OpenStack Carrier Grade, a carrier-grade distribution of OpenStack the leading open source cloud computing platform.</p>
<ul>
<li>
<xref type="section" href="#topic3485/Helion-services">HP Helion OpenStack services</xref>
</li>
<li>
<xref type="section" href="#topic3485/deploy-arch">Deployment architecture</xref>
</li>
<li>
<xref type="section" href="#topic3485/networkarch">Network architecture</xref>
</li>
<li>
<xref type="section" href="#topic3485/next">Hardware requirement</xref>
</li>
</ul>
<section id="Helion-services"> <title>HP Helion OpenStack Carrier Grade services- functional overview</title>
<p>The following table outlines the functionality of HP Helion OpenStack  Carrier Grade services based on the type of users - Users and Administrators. For a complete description of these services, see the <xref href="../../CarrierGrade/Overview/carrier-grade.services-overview.dita" >Services Overview</xref> page.</p>
</section>
<section id="deploy-arch"> <title>Deployment architecture</title>
<p>You can deploy HP Helion OpenStack Carrier Grade on a <xref href="http://www.linux-kvm.org/page/Main_Page" scope="external" format="html" >KVM hypervisor</xref>.</p>
<p>The following diagram depicts a simplified deployment scenario using KVM.</p>
<!-- This image is for beta release only. Replace for beta -->
<p>
<xref format="html" href="javascript:window.open('/media/CGH-architecture-rc0.png','_self','toolbar=no,menubar=no,resizable=yes,scrollbars=yes')" >
<image href="../../media/CGH-architecture-rc0.png"/>
</xref>
<!--A BR tag was used here in the original source.-->Click to view larger image; click <b>Back</b> to return.</p>
</section>
<section id="networkarch"> <title>Network architecture</title>
<p>The following information describes the network configuration for the <xref type="section" href="#topic3485/physical">physical networks</xref> and <xref type="section" href="#topic3485/virtual">virtual networks</xref>, which must be configured by the network administrator.</p>
<table>
<tgroup cols="2">
<colspec colname="col1"/>
<colspec colname="col2"/>
<thead>
<row>
<entry> Network </entry>
<entry> Description </entry>
</row>
</thead>
<tbody>
<row>
<entry> CLM / OBS</entry>
<entry> Cloud Management and Object Store Network -- Untagged (Private)</entry>
</row>
<row>
<entry> PXE </entry>
<entry> Boot/initial configuration network. Untagged </entry>
</row>
<row>
<entry> CAN </entry>
<entry> Consumer Access Network (shared with WR - OAM)</entry>
</row>
<row>
<entry> CTL </entry>
<entry> IPMI/iLO network (shared with WR) </entry>
</row>
<row>
<entry> BLS </entry>
<entry>Block Storage Network; can be on a separate interface (`intf*`) </entry>
</row>
<row>
<entry> DCM </entry>
<entry>Data Center Management network (accessible to WR region) and route across multi-DC </entry>
</row>
<row>
<entry> WR-TUL/SR-IOV (Swift) </entry>
<entry> Tenant Underlay Network from WR region </entry>
</row>
<row>
<entry> WR-PXE </entry>
<entry> Boot/Cloud Management network for WR Cloud/Region -- Untagged</entry>
</row>
<row>
<entry> WR-EXT </entry>
<entry> External network (FIP network for WR region) </entry>
</row>
</tbody>
</tgroup>
</table>

  <!-- <row>
<row>
<entry> WR-BLS </entry>
<entry> WR Cloud BLock Storage Network; can be on a separate interface (intf*) </entry>
</row>

<row>
    <entry> EXT </entry>
    <entry> External network (FIP network for DCN region); can be on a separate interface (`intf*`)</entry>
  </row>
  
  <entry> TUL (Swift) </entry>
<entry> Tenant Underlay Network(s); routed across multi-DC for VxLAN 12 extension  </entry>
</row>
-->

</section>
<section id="interfaces"> <title>Interfaces</title>
<p>The following are the interfaces being used, based on the technical architecture diagram. You can use more than two interfaces and specific networks to physical networks.</p>
<ul>
<li>Intf0 is mapped to Port1/Bonded Pair</li>
<li>Intf1 is mapped to Port2/Bonded Pair</li>
<li>Intf<i>n</i> are Multiple Provider Networks or SR-IOV interfaces.</li>
</ul>
</section>
<section id="routing-acls"> <title>Routing ACLs</title>
<p>Configure the following routing access lists.</p>
<table>
<tgroup cols="3">
<colspec colname="col1"/>
<colspec colname="col2"/>
<colspec colname="col3"/>
<thead>
<row>
<entry> From </entry>
<entry> To </entry>
<entry> Reason </entry>
</row>
</thead>
<tbody>
<row>
<entry> CLM </entry>
<entry> DCM </entry>
<entry> Access NTP, DNS, LDAP, StoreVirtual APIs, and so forth </entry>
</row>
<row>
<entry> CLM </entry>
<entry> CTL </entry>
<entry> Access iLO network for managing the lifecycle of the node </entry>
</row>
</tbody>
</tgroup>
</table>

  <!-- <row>
<entry> CLM </entry>
<entry> EXT </entry>
<entry> Access external networks, for example to download patches </entry>
</row>
<row>
<entry> CLM </entry>
<entry> CAN </entry>
<entry> Accessing OpenStack APIs </entry>
</row>
<row>
<entry> DCM </entry>
<entry> CLM </entry>
<entry> VSC IP to HLM CLM VM IP (only during deployment) </entry>
</row>
<row>
<entry> TUL1 </entry>
<entry> TUL2 </entry>
<entry> Inter DC communication (VPN tunnel, BGP, MPLS) </entry>
</row>
<row>
<entry> DCM1 </entry>
<entry> DCM2 </entry>
<entry> Inter DC communication (VPN tunnel, BGP, MPLS) </entry>
</row>
 -->
  
  <p>CLM, PXE, OBS, BLS, WR-PXE, WR-INFRA, WR-TUL will use RFC 1918 non-routable IPs to prevent access to the CLM network from DCM, CLT, or EXT.</p>
</section>
<section id="next"> <title>Next step</title>
<p>For more information of Hardware configurations see the <xref href="../../CarrierGrade/Installation/carrier-grade-support-matrix.dita" >Support Matrix</xref>.</p>
<p>
  <xref type="section" href="#topic3485"> Return to Top </xref>
</p>

  <section id="ethernet-interfaces"> <title>Ethernet Interfaces</title>
    <p>All hosts in the server server connect to at least the internal management network using an Ethernet interface. The ports used for this connection must support network booting and must be configured to be used as the primary booting device for normal operations.</p>
    <p>Typically this means that they must be on-board ports, since in most BIOS/UEFI implementations only on-board ports can be configured for network booting. You can use ports on a 10 GB NIC instead, if these ports fulfill these requirements.</p>
    <p>The following table illustrates the number and type of Ethernet ports required in two different installation scenarios. It assumes that the ports used to connect to the internal management network are on-board 1 GB ports.</p>
    <p>
      <b>NOTE:</b> The following table assumes that each interface is connected to a single network. An Ethernet interface can be shared by more than one network.</p>
    <table>
      <tgroup cols="3">
        <thead>
          <row>
            <entry>Personality</entry>
            <entry>Basic Scenario</entry>
            <entry>LAG Scenario</entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry>
              Controller Node</entry>
            <entry> <ul><li> One 1G on-board interface (Internal management network)</li> 
              <li>One 1G interface (OAM)</li>
              <li>One optional 1G or 10G interface (Infrastructure network)</li></ul></entry>
            <entry><ul><li>Two 1G on-board interfaces (Internalmanagement network)</li>
              <li>Two 1G interfaces (OAM)</li>
              <li>Two optional 1G or 10G interfaces (Infrastructure network)
                NOTE: The controller-0 and controller-1 port configurations must be identical.</li></ul></entry>
            <entry>
            </entry>
          </row>
          <row>
            <entry>Compute Node</entry>
            <entry><ul><li>One 1G on-board interface (Internal management network)</li>
              <li>One 1G (Intel i350) or 10G (Intel 82599) interface per additional Provider Network</li></ul></entry>
            <entry><ul><li>Two 1G on-board interfaces (Internal management network)</li>
              <li>Two 1G (Intel i350) or 10G (Intel 82599) interfaces per additional Provider Network</li></ul></entry>
          </row>
        </tbody>
      </tgroup>
    </table>
    <p>In the basic scenario, a single Ethernet port is used to attach the host to each of the networks. In the LAG scenario, two Ethernet ports are used for each connection.</p>
  </section>
  <section id="board-management-modules"> <title>Board Management Modules</title>
    <p>For out-of-band reset and power-on/power-off capabilities, HP360 or HP380 servers equipped with HP iLO (Integrated Lights Out) board management modules are required. Each module must be connected using port-based VLAN to a switch that has access to the internal management network.</p>
  </section>
  <section id="usb-interface"> <title>USB Interface</title>
    <p>For the controller, a USB interface is required for backup and restore operations, and for software installation if a DVD is not available.</p>
  </section>
  <section id="net"> <title>Network Requirements</title>
    <p>The networking environment of the Titanium Server incorporates up to five types of network:</p>
    <ul>
      <li>the internal management network</li>
      <li>the OAM network</li>
      <li>one or more provider networks</li>
      <li>an optional infrastructure network</li>
      <li>an optional board management network. </li>
    </ul>
    <p>Operational requirements for each network are described in the following sections.</p>
  </section>
  <section id="internal-management-network"> <title>Internal Management Network</title>
    <p>The internal management network must be implemented as a single, dedicated, Layer 2 broadcast
      domain for the exclusive use of each server cluster. Sharing of this network by more
      than one server cluster is not a supported configuration.</p>
    <p>During the server software installation process, several network services such as BOOTP, DHCP,
      and PXE, are expected to run over the internal management network. These services
      are used to bring up the different hosts to an operational state. Therefore, it is
      mandatory that this network be operational and available in advance, to ensure a
      successful installation.</p>
    <p>On each host, the internal management network can be implemented using a 1Gb or 10 Gb Ethernet port. In either
      case, requirements for this port are:</p>
    <ul>
      <li>must be capable of PXE-booting</li>
      <li>can be used by the motherboard as a primary boot device</li>
    </ul>
  </section>
  <section id="infrastructure-network"> <title>Infrastructure Network</title>
    <p>This is an optional network.</p>
    <p>As with the internal management network, the infrastructure network must be implemented as a
      single, dedicated, Layer 2 broadcast domain for the exclusive use of each server
      cluster.</p>
    <p>Sharing of this network by more than one server cluster is not a supported configuration.</p>
    <p>The infrastructure network can be implemented as a 1Gb or 10 Gb Ethernet network. In its absence, all infrastructure traffic is carried over the internal management network.</p>
  </section>
  <section id="oam-network"> <title>OAM Network</title>
    <p>You should ensure that the following services are available on the OAM Network:</p>
    <ul>
      <li>
        <p>DNS Service - Needed to facilitate the name resolution of servers reachable on the OAM Network.</p>
        
        <p>The server can operate without a configured DNS service. However, a DNS service should be in
          place to ensure that links to external references in the current and future
          versions of the web administration interface work as expected.</p>
      </li>
      <li>
        <p>NTP Service - The Network Time Protocol (NTP) can be optionally used by the server controller
          nodes to synchronize their local clocks with a reliable external time
          reference. However, it is strongly suggested that this service be available,
          among other things, to ensure that system-wide log reports present a unified
          view of the day-to-day operations.</p>
      </li>
    </ul>
    <p>The server compute nodes always use the controller nodes as the de-facto time server for the
      entire cluster.</p>
  </section>
  <section id="provider-network"> <title>Provider Network</title>
    <p>There are no specific requirements for network services to be available on the provider network. However, you must ensure that all network services required by the guests running in the compute nodes are available. For configuration purposes, the compute nodes themselves are entirely served by the services provided by the controller nodes over the internal management network.</p>
  </section>
  

<!--
### Backup and Restore nodes {#backup-restore-nodes}

Backup and restore scripts and procedures are provided for the seed VM, undercloud, overcloud management controller (running singleton services like Sherpa), and the MySQL database deployed in the controller cluster. These scripts are to be used by administrators managing the OpenStack Cloud.

Backup and Restore of VM instances/snapshots and volumes/snapshots for workloads in the cloud is supported using the Object Storage service.

For more information see [HP Helion OpenStack Back Up and Restore](/helion/openstack/1.1/backup.restore/)

<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top </a>
-->
<!-- ===================== horizontal rule ===================== -->
</section>
</body>
</topic>
