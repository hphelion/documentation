
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xml:lang="en-us" lang="en-us">
<head><meta name="description" content="We have gathered some of the common issues that occur during installation and organized them by when they occur during the installation. These sections will coincide with the steps labeled in the ..."></meta><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><meta name="DC.Type" content="topic"></meta><meta name="DC.Title" content="HPE Helion OpenStack 2.0: Troubleshooting the Installation"></meta><meta name="prodname" content="HPE Helion"></meta><meta name="prodname" content="HPE Helion"></meta><meta name="version" content="4.1.0"></meta><meta name="version" content="4.1.0"></meta><meta name="copyright" content="HPE Helion 2015" type="primary"></meta><meta name="DC.Rights.Owner" content="HPE Helion 2015" type="primary"></meta><meta name="copyright" content="HPE Helion 2015" type="primary"></meta><meta name="DC.Rights.Owner" content="HPE Helion 2015" type="primary"></meta><meta name="DC.Format" content="XHTML"></meta><meta name="DC.Identifier" content="troubleshooting_installation"></meta><link rel="stylesheet" type="text/css" href="../../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>HPE Helion OpenStack 2.0: Troubleshooting the Installation</title><!--  Generated with Oxygen version 17.0, build number 2015072912.  --><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><link rel="stylesheet" type="text/css" href="../../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../../oxygen-webhelp/resources/skins/skin.css"><!----></link><script type="text/javascript"><!--
          
          var prefix = "../../index.html";
          
          --></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery-1.8.2.min.js"><!----></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script></head>
<body onload="highlightSearchTerm()" class="frmBody" id="troubleshooting_installation">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td width="75%"></td><td><div class="navheader"></div></td></tr></tbody></table>

  <h1 class="title topictitle1">HPE Helion OpenStack<sup>Â®</sup> 2.0: Troubleshooting the Installation</h1>

  <div class="body">
    <div class="section" id="troubleshooting_installation__about">
      <p class="p">We have gathered some of the common issues that occur during installation and organized
        them by when they occur during the installation. These sections will coincide with the steps
        labeled in the installation instructions.</p>

      <ul class="ul">
        <li class="li"><a class="xref" href="installation_troubleshooting.html#troubleshooting_installation__deployer_setup">Issues during Lifecycle-manager Setup</a></li>

        <li class="li"><a class="xref" href="installation_troubleshooting.html#troubleshooting_installation__deploy_cobbler">Issues while Provisioning your Baremetal Nodes</a></li>

        <li class="li"><a class="xref" href="installation_troubleshooting.html#troubleshooting_installation__config_processor">Issues while Updating Configuration Files</a></li>

        <li class="li"><a class="xref" href="installation_troubleshooting.html#troubleshooting_installation__deploy_cloud">Issues while Deploying the Cloud</a></li>

        <li class="li"><a class="xref" href="blockstorageconfig_troubleshooting.html">Issues during Block Storage Backend Configuration</a></li>

      </ul>

    </div>

    <div class="section" id="troubleshooting_installation__deployer_setup"><h2 class="title sectiontitle">Issues during Lifecycle-manager Setup</h2>
      <p class="p"><strong class="ph b">Issue: Running the <samp class="ph codeph">~/hos-2.0.0/hos-init.bash</samp> script when configuring
          your deployer does not complete</strong></p>

      <p class="p">Part of what the <samp class="ph codeph">hos-init.bash</samp> script does is install git and so if your
        DNS nameserver(s) is not specified in your <samp class="ph codeph">/etc/resolv.conf</samp> file, is not
        valid, or is not functioning properly on your lifecycle-manager node then it won't be able
        to complete.</p>

      <p class="p">To resolve this issue, double check your nameserver in your
          <samp class="ph codeph">/etc/resolv.conf</samp> file and then re-run the script.</p>

    </div>

    <div class="section" id="troubleshooting_installation__deploy_cobbler"><h2 class="title sectiontitle">Issues while Provisioning your Baremetal Nodes</h2>
      <p class="p"><strong class="ph b">Issue: Configuration changes needed after Cobbler deploy</strong></p>

      <p class="p">If you've made a mistake or wish to change your
          <samp class="ph codeph">~/helion/my_cloud/definition/data/servers.yml</samp> configuration file after
        you've already run the <samp class="ph codeph">cobbler-deploy.yml</samp> playbook, follow these steps to
        ensure Cobbler gets updated with the new server information:</p>

      <ol class="ol">
        <li class="li">Ensure your <samp class="ph codeph">servers.yml</samp> file is updated</li>

        <li class="li">Commit your changes to git:
          <pre class="pre codeblock">cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</pre>
</li>

        <li class="li">Determine which nodes you have entered into Cobbler by using:
          <pre class="pre codeblock">sudo cobbler system list</pre>
</li>

        <li class="li">Remove the nodes that had the old information from Cobbler using:
          <pre class="pre codeblock">sudo cobbler system remove --name &lt;nodename&gt;</pre>
</li>

        <li class="li">Re-run the <samp class="ph codeph">cobbler-deploy.yml</samp> playbook to update the new node
          definitions to Cobbler: <pre class="pre codeblock">cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre>

          <div class="note note"><span class="notetitle">Note:</span> If you've also already run the <samp class="ph codeph">bm-reimage.yml</samp> playbook then read
            the <a class="xref" href="#troubleshooting_installation__reimage">How to re-image an existing
              node</a> section below for how to ensure your nodes get re-imaged when re-running
            this playbook.</div>

        </li>

      </ol>

      <p class="p"><strong class="ph b">Issue: bm-reimage.yml playbook doesn't find any nodes to image</strong></p>

      <p class="p">You may receive this error when running the <samp class="ph codeph">bm-reimage.yml</samp> playbook:</p>

      <pre class="pre codeblock">TASK: [cobbler | get-nodelist | Check we have targets] ************************
failed: [localhost] =&gt; {"failed": true}
msg: There is no default set of nodes for this command, use -e nodelist
        
FATAL: all hosts have already failed -- aborting</pre>

      <p class="p">This behavior occurs when you don't specify the <samp class="ph codeph">-e nodelist</samp> switch to your
        command and all of your nodes are marked as <samp class="ph codeph">netboot-enabled: false</samp> in
        Cobbler. By default, without the <samp class="ph codeph">-e nodelist</samp> switch, the
          <samp class="ph codeph">bm-reimage.yml</samp> playbook will only reimage the nodes marked as
          <samp class="ph codeph">netboot-enabled: True</samp>, which you can verify which nodes you have that are
        marked as such with this command:</p>

      <pre class="pre codeblock">sudo cobbler system find --netboot-enabled=1</pre>

      <p class="p">If this is on a fresh install and none of your nodes have been imaged with the HPE Helion
        OpenStack ISO, then you should be able to remove all of your nodes from Cobbler and re-run
        the <samp class="ph codeph">cobbler-deploy.yml</samp> playbook. You can do so with these commands:</p>

      <ol class="ol">
        <li class="li">Get a list of your nodes in Cobbler:
          <pre class="pre codeblock">sudo cobbler system list</pre>
</li>

        <li class="li">Remove each of them with this command:
          <pre class="pre codeblock">sudo cobbler system remove --name SYSTEM_NAME</pre>
</li>

        <li class="li">Confirm they are all removed in Cobbler:
          <pre class="pre codeblock">sudo cobbler system list</pre>
</li>

        <li class="li">Re-run <samp class="ph codeph">cobbler-deploy.yml</samp>:
          <pre class="pre codeblock">cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre>
</li>

        <li class="li">Confirm they are all now set to <samp class="ph codeph">netboot-enabled: True</samp>:
          <pre class="pre codeblock">sudo cobbler system find --netboot-enabled=1</pre>
</li>

      </ol>

      <p class="p"><strong class="ph b">Issue: The <samp class="ph codeph">--limit</samp> switch does not do anything</strong></p>

      <p class="p">If the <samp class="ph codeph">cobbler-deploy.yml</samp> playbook fails, it makes a mention that to retry
        you should use the <samp class="ph codeph">--limit</samp> switch, as seen below:</p>

      <pre class="pre codeblock">to retry, use: --limit @/home/headmin/cobbler-deploy.retry</pre>

      <p class="p">This is a standard Ansible message but it is not needed in this context. Please use our
        instructions described above to remove your systems, if necessary, from Cobbler before
        re-running the playbook as the <samp class="ph codeph">--limit</samp> is not needed. Note that using the
          <samp class="ph codeph">--limit</samp> switch does not cause any harm and won't prevent the playbook
        from completing, it's just not a necessary step.</p>

      <p class="p" id="troubleshooting_installation__install_fail"><strong class="ph b">Issue: Dealing With Nodes that Fail to Install</strong></p>

      <p class="p">The <samp class="ph codeph">bm-reimage.yml</samp> playbook will take every node as far as it can through
        the baremetal install process. Nodes that fail will not prevent the others from continuing
        to completion. At the end of the run you will get a list of the nodes (if any) that failed
        to install.</p>

      <p class="p">If you run <samp class="ph codeph">bm-reimage.yml</samp> a second time, by default it will target only
        the failed nodes the second time round (because the others are already marked as
        "completed"). Alternatively you can target specific nodes for reimage using <samp class="ph codeph">-e
          nodelist</samp> as described in the section below.</p>

      <p class="p">The places where you are most likely to see a node fail is timeout in the "wait for
        shutdown" step, which means that the node did not successfully install an operating system
        (e.g. it could be stuck in POST) or a timeout in "wait for ssh" at the end of the baremetal
        install. This means that the node did not come back up after being powered on. To fix the
        issues you'll need to connect to the nodes' consoles and investigate.</p>

      <p class="p" id="troubleshooting_installation__reimage"><strong class="ph b">How to re-image an existing node</strong></p>

      <p class="p">Once Linux for HPE Helion has been successfully installed on a node, that node will be
        marked as "installed" and subsequent runs of <samp class="ph codeph">bm-reimage.yml</samp> (and related
        playbooks) will not target it. This is deliberate so that you can't reimage the node by
        accident. If you do need to reimage existing nodes you will need to use the <samp class="ph codeph">-e
          nodelist</samp> option to target them specifically. For example:</p>

      <pre class="pre codeblock">ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=cpn-0044,cpn-0045</pre>

      <div class="note note"><span class="notetitle">Note:</span> You can target all nodes with <samp class="ph codeph">-e nodelist=all</samp></div>

      <p class="p">This will power cycle the specified nodes and reinstall the operating system on them, using
        the existing settings stored in Cobbler.</p>

      <p class="p">If you want to change settings for a node in the configuration files, see the <a class="xref" href="#troubleshooting_installation__deploy_cobbler">Configuration changes needed after
          Cobbler deploy</a> section above.</p>

      <p class="p"><strong class="ph b">Issue: Wait for SSH phase in bm-reimage.yml hangs </strong></p>

      <div class="p">This issue has been observed during deployment to systems configured with QLogic based
        BCM578XX network adapters utilizing the bnx2x driver and is currently under investigation.
        The symptom manifests following a cold boot during deployment at the "wait for SSH" phase in
        bm-reimage.yml which will result in a hang and eventually timeout, causing the baremetal
        install to fail. The presence of this particular issue can be further confirmed by
        connecting to the remote console of the server via iLO or by checking the server's dmesg
        output for the presence of bnx2_panic_dump messages, similar to the following:
        <pre class="pre codeblock">
bnx2x: [bnx2x_prev_unload_common:10433(eth%d)]Failed to empty BRB, hope for the best  ...
bnx2x: [bnx2x_stats_update:1268(eth0)]storm stats were not updated for 3 times
bnx2x: [bnx2x_stats_update:1269(eth0)]driver assert
bnx2x: [bnx2x_panic_dump:929(eth0)]begin crash dump -----------------  .....
bnx2x: [bnx2x_panic_dump:1163(eth0)]end crash dump -----------------</pre>

        This workaround is completed by rebooting the server.</div>

      <p class="p"><strong class="ph b">Issue: Soft lockup at imaging</strong></p>

      <p class="p">When imaging your nodes, if you see a kernel error of the type "BUG: soft lockup - CPU#10
        stuck...." you should reset the node and make sure it is imaged properly next time. Note
        that depending on when you get the error, you may have to rerun
          <samp class="ph codeph">cobbler-deploy.yml</samp>. If the bm-reimage playbook says it failed to image
        the node, then cobbler knows this has occurred and you can reset the node. If not, you can
        follow the instructions above for <a class="xref" href="#troubleshooting_installation__reimage">reimaging the node</a>.</p>

      <p class="p"><strong class="ph b">Blank Screen Seen When Monitoring the Imaging Step</strong></p>

      <p class="p">If you are watching the <samp class="ph codeph">os-install</samp> process on a node via the console
        output, there can be a pause between 2-3 minutes in length where nothing gets reported to
        the console screen. This is just after the grub menu has been displayed on a UEFI-based
        system.</p>

      <p class="p">This is normal and nothing to be concerned with. The imaging process will continue on after
        this pause.</p>

      <p class="p"><strong class="ph b">Issue: The <samp class="ph codeph">--limit</samp> switch does not do anything</strong></p>

      <p class="p">If the <samp class="ph codeph">bm-reimage.yml</samp> playbook fails, it makes a mention that to retry you
        should use the <samp class="ph codeph">--limit</samp> switch, as seen below:</p>

      <pre class="pre codeblock">to retry, use: --limit @/home/stack/bm-reimage.retry</pre>

      <p class="p">This is a standard Ansible message but it is not needed in this context. Note that using
        the <samp class="ph codeph">--limit</samp> switch does not cause any harm and won't prevent the playbook
        from completing, it's just not a necessary step.</p>

    </div>


    <div class="section" id="troubleshooting_installation__config_processor"><h2 class="title sectiontitle">Issues while Updating Configuration Files</h2>
      <p class="p">If you have made corrections to your configuration files and need to re-run the
        Configuration Proceessor, the only thing you need to do is commit your changes to your local
        git:</p>

      <pre class="pre codeblock">cd ~/helion/hos/ansible
git add -A
git commit -m "commit message"</pre>

      <p class="p">You can then re-run the Configuration Processor:</p>

      <pre class="pre codeblock">cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre>

    </div>


    <div class="section" id="troubleshooting_installation__deploy_cloud"><h2 class="title sectiontitle">Issues while Deploying the Cloud</h2>
      <p class="p"><strong class="ph b">Issue: If the <samp class="ph codeph">site.yml</samp> playbook fails, you can query the log for the
          reason</strong></p>

      <p class="p">Ansible is good about outputting the errors into the command line output, however if you'd
        like to view the full log for any reason the location is:</p>

      <div class="p">
        <pre class="pre codeblock">~/.ansible/ansible.log</pre>

      </div>

      <p class="p">This log is updated real time as you run ansible playbooks.</p>

      <div class="note tip"><span class="tiptitle">Tip:</span> Use grep to parse through the log. Usage: <samp class="ph codeph">grep &lt;text&gt;
          ~/.ansible/ansible.log</samp></div>

      <p class="p" id="troubleshooting_installation__wipe"><strong class="ph b">Issue: How to Wipe the Disks of your Machines</strong></p>

      <p class="p">If you have re-run the <samp class="ph codeph">site.yml</samp> playbook, you may need to wipe the disks
        of your nodes</p>

      <p class="p">You would generally run the playbook below after re-running the
          <samp class="ph codeph">bm-reimage.yml</samp> playbook but before you re-run the
          <samp class="ph codeph">site.yml</samp> playbook.</p>

      <pre class="pre codeblock">cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre>

      <p class="p">The playbook will show you the disks to be wiped in the output and allow you to confirm
        that you want to complete this action or abort it if you do not want to proceed. You can
        optionally use the <samp class="ph codeph">--limit &lt;NODE_NAME&gt;</samp> switch on this playbook to
        restrict it to specific nodes.</p>

      <p class="p">If you receive an error stating that <samp class="ph codeph">osconfig</samp> has already run on your
        nodes then you will need to remove the <samp class="ph codeph">/etc/hos/osconfig-ran</samp> file on each
        of the nodes you want to wipe with this command:</p>

      <pre class="pre codeblock">sudo rm /etc/hos/osconfig-ran</pre>

      <p class="p">That will clear this flag and allow the disk to be wiped.</p>

      <p class="p"><strong class="ph b">Issue: Freezer installation fails if an independent network is used for the
          External_API.</strong></p>

      <p class="p"> Currently the Freezer installation fails if an independent network is used for the
        External_API. If you intend to deploy the External API  on an independent
        network, the following changes need to be made: </p>

      <div class="p">In <samp class="ph codeph">roles/freezer-agent/defaults/main.yml</samp> add the following line:
        <pre class="pre codeblock">backup_freezer_api_url: "{{ FRE_API | item('advertises.vips.private[0].url', default=' ') }}"</pre>

        In <samp class="ph codeph">roles/freezer-agent/templates/backup.osrc.j2</samp> add the following line:
        <pre class="pre codeblock">export OS_FREEZER_URL={{ backup_freezer_api_url }}</pre>

      </div>

      <p class="p"><strong class="ph b">Error Received if Root Logical Volume is Too Small</strong></p>

      <p class="p">When running the <samp class="ph codeph">site.yml</samp> playbook, you may receive the error below if
        your root logical-volume is too small:</p>

      <pre class="pre codeblock">2015-09-29 15:54:02,751 p=26345 u=stack |  TASK: [osconfig | disk config | Extend root LV] *******************************
2015-09-29 15:54:03,021 p=26345 u=stack |  failed: [helion-ccp-swpac-m1-mgmt] =&gt; (item=({'physical_volumes': ['/dev/sda_root'], 'consumer': {'name': 'os'},
'name': 'hlm-vg'}, {'mount': '/', 'fstype': 'ext4', 'name': 'root', 'size': '10%'})) =&gt; {"changed": true, "cmd": ["lvextend", "-l", "10%VG", "/dev/hlm-
vg/root"], "delta": "0:00:00.022983", "end": "2015-09-29 10:54:18.925855", "failed": true, "failed_when_result": true, "item": [{"consumer": {"name": 
"os"}, "name": "hlm-vg", "physical_volumes": ["/dev/sda_root"]}, {"fstype": "ext4", "mount": "/", "name": "root", "size": "10%"}], "rc": 3, "start": "2015-
09-29 10:54:18.902872", "stdout_lines": [], "warnings": []}
2015-09-29 15:54:03,022 p=26345 u=stack |  stderr:   New size given (7128 extents) not larger than existing size (7629 extents)</pre>

      <p class="p">The specific part of this error to parse out and resolve is:</p>

      <pre class="pre codeblock">stderr:   New size given (7128 extents) not larger than existing size (7629 extents)</pre>

      <p class="p">The error also references the root volume:</p>

      <pre class="pre codeblock">"name": "root", "size": "10%"</pre>

      <p class="p">The problem is that the root logical-volume, as specified in the
          <samp class="ph codeph">disks_controller.yml</samp> file, is set to <samp class="ph codeph">10%</samp> of the overall
        physical volume and this value is too small.</p>

      <p class="p">To resolve this issue you need to ensure that the percentage is set properly for the size
        of your logical-volume. The default values in the configuration files is based on a 500GB
        disk, so if your logical-volumes are smaller you may need to increase the percentage so
        there is enough room.</p>

    </div>

  </div>

<div class="navfooter"><!----></div><div class="footer">WebHelp output generated by<a class="oxyFooter" href="http://www.oxygenxml.com" target="_blank"><span class="oXygenLogo"><img src="../../oxygen-webhelp/resources/img/LogoOxygen100x22.png" alt="Oxygen"></img></span><span class="xmlauthor">XML Author</span></a></div>
</body>
</html>