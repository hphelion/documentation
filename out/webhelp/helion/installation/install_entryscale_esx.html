
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xml:lang="en-us" lang="en-us">
<head><meta name="description" content="This document describes the procedure for the deployment of an ESX cloud using input model and adding more ESX hosts to an already activated cluster. It contains the following topics: Prerequisites ..."></meta><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><meta name="DC.Type" content="topic"></meta><meta name="DC.Title" content="HPE Helion OpenStack 2.0: Installation for Helion Entry-scale Cloud with ESX"></meta><meta name="prodname" content="HPE Helion"></meta><meta name="version" content="4.1.0"></meta><meta name="copyright" content="HPE Helion 2015" type="primary"></meta><meta name="DC.Rights.Owner" content="HPE Helion 2015" type="primary"></meta><meta name="DC.Format" content="XHTML"></meta><meta name="DC.Identifier" content="install_esx"></meta><link rel="stylesheet" type="text/css" href="../../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>HPE Helion OpenStack 2.0: Installation for Helion Entry-scale Cloud with ESX</title><!--  Generated with Oxygen version 17.0, build number 2015072912.  --><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><link rel="stylesheet" type="text/css" href="../../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../../oxygen-webhelp/resources/skins/skin.css"><!----></link><script type="text/javascript"><!--
          
          var prefix = "../../index.html";
          
          --></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery-1.8.2.min.js"><!----></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script></head>
<body onload="highlightSearchTerm()" class="frmBody" id="install_esx">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td width="75%"></td><td><div class="navheader"></div></td></tr></tbody></table>

  <h1 class="title topictitle1">HPE Helion OpenStack<sup>®</sup> 2.0: Installation for Helion Entry-scale Cloud
    with ESX</h1>

  <div class="body">
    
    <p class="p">This document describes the procedure for the deployment of an ESX cloud using input model and
      adding more ESX hosts to an already activated cluster.</p>

    <div class="p"> It contains the following topics:<ul class="ul" id="install_esx__ul_wrs_dwq_lt">
        <li class="li"><a class="xref" href="#install_esx__prereqs">Prerequisites</a></li>

        <li class="li"><a class="xref" href="#install_esx__deployCloud">Deploy ESX Cloud</a></li>

        <li class="li"><a class="xref" href="#install_esx__prepAndDeploy">Prepare and Deploy ESX Computes
            and OVSvAPPs</a></li>

      </ul>
</div>

    <div class="section">
      <div class="note important"><span class="importanttitle">Important:</span> Before you start your ESX cloud deployment ensure that you read the
        following instructions carefully.</div>

    </div>

    <div class="section"><h2 class="title sectiontitle">Important Notes</h2>
      <ul class="ul">
        <li class="li">If you are looking for information about when to use the GUI installer and when to use
          the CLI, see the <a class="xref" href="installation_overview.html#install_overview">Installation
            Overview</a>.</li>

        <li class="li">Review the <a class="xref" href="../hardware.html">recommended minimum hardware
            requirements</a> that we have listed.</li>

        <li class="li">The installation process can occur in different phases. For example, you can install the
          control plane only and then add Compute nodes afterwards if you would like.</li>

        
        <li class="li">If you run into issues during installation, we have put together a list of <a class="xref" href="installation_troubleshooting.html">Installation Troubleshooting Steps</a> you
          can reference.</li>

        <li class="li">Make sure all disks on the system(s) are wiped before you begin the install. (For Swift,
          refer to <a class="xref" href="../objectstorage/swift_device_groups.html">Swift Requirements for
            Device Group Drives</a>)</li>

        <li class="li">There is no requirement to have a dedicated network for OS-install and system
          deployment, this can be shared with the management network. More information can be found
          on the <a class="xref" href="../example_configurations.html">Example Configuration</a> page.</li>

      </ul>

    </div>

    <div class="section"><h2 class="title sectiontitle">Before You Start</h2>
      
      <p class="p">We have put together a <a class="xref" href="preinstall_checklist.html">Pre-Installation
          Checklist</a> that should help with the recommended pre-installation tasks.</p>

    </div>

    <div class="section" id="install_esx__prereqs"><h2 class="title sectiontitle">Prerequisite</h2>
      
      <p class="p">ESX/vCenter integration is not fully automatic, vCenter administrators are advised of the
        following responsibilities to ensure secure operation:</p>

      <div class="p">
        <ul class="ul" id="install_esx__ul_shv_s4b_2t">
          <li class="li">The VMware administrator is responsible for administration of the vCenter servers and
            the ESX nodes using the VMware administration tools. These responsibilities include: <ul class="ul" id="install_esx__ul_py3_j5l_ft">
              <li class="li">Installing and configuring vCenter Server</li>

              <li class="li">Installing and configuring ESX server and ESX cluster</li>

              <li class="li">Installing and configuring shared datastores</li>

              <li class="li">Establishing network connectivity between the ESX network and the HPE Helion
                management network</li>

            </ul>
</li>

          <li class="li">The VMware administration staff is responsible for the review of vCenter logs. These
            logs are not automatically included in Helion centralized logging.</li>

          <li class="li">Logging levels for vCenter should be set appropriately to prevent logging of the
            password for the Helion message queue.</li>

          <li class="li">The vCenter cluster and ESX Compute nodes must be appropriately backed up.</li>

          <li class="li">Backup procedures for vCenter should ensure that the file containing the Helion
            configuration as part of Nova and Cinder volume services is backed up and the backups
            are protected appropriately.</li>

          <li class="li">Since the file containing the Helion message queue password could appear in the swap
            area of a vCenter server, appropriate controls should be applied to the vCenter cluster
            to prevent discovery of the password via snooping of the swap area or memory dumps.</li>

          <li class="li">It is recommended to have a common shared storage for all the ESXi hosts in a
            particular cluster. </li>

          <li class="li">Ensure that you have enabled HA (High Availability) and DRS (Distributed Resource
            Scheduler) settings in a cluster configuration before running the installation. DRS/HA
            is disabled only for OVSvApp. This is done so that it does not move to a different host.
            If you do not enable DRS/HA prior to installation then you will not be able to disable
            it only for OVSvApp. As a result DRS/HA can migrate OVSvApp to different host, which
            will create a network loop.<div class="p">
              <div class="note note"><span class="notetitle">Note:</span> No two clusters should have the same name across datacenters in a given
                vCenter.</div>

            </div>
</li>

        </ul>

      </div>

    </div>

    <div class="section" id="install_esx__deployCloud"><strong class="ph b">Deploy ESX Cloud</strong><p class="p">At a high level, here are the steps to
        configure and deploy ESX cloud:</p>
<p class="p"><img class="image" id="install_esx__image_kjt_zlm_ft" src="../../media/esx/esx_deploy.jpg"></img></p>
</div>

    <div class="section"><h2 class="title sectiontitle">Procedure to Deploy ESX cloud</h2>
      
    </div>

    <p class="p">The following topics in this section explain how to deploy ESX cloud.</p>

    <div class="section"><h2 class="title sectiontitle">Set up the Lifecycle-manager</h2>
      
      <p class="p"><strong class="ph b">Installing the Lifecycle-manager</strong></p>

      <p class="p">The lifecycle-manager will contain the installation scripts and configuration files to
        deploy your cloud. You can set up the lifecycle-manager on a dedicated node or you do so on
        your first controller node. The default choice is to use the first controller node as the
        lifecycle-manager.</p>

      <ol class="ol">
        <li class="li">Download the HPE Helion OpenStack 2.0.0 product from the <a class="xref" href="https://helion.hpwsportal.com/catalog.html#/Category/%7B%22categoryId%22%3A10389%7D/Show" target="_blank">Helion Downloads</a> page after signing in.</li>

        <li class="li">Boot your deployer from the ISO contained in the download.</li>

        <li class="li">Enter "install" to start installation. <div class="note note"><span class="notetitle">Note:</span> "install" is all lower case</div>
</li>

        <li class="li">Select the language. Note that only the English language selection is currently
          supported.</li>

        <li class="li">Select the location.</li>

        <li class="li">Select the keyboard layout.</li>

        <li class="li">Select the primary network interface, if prompted:<ul class="ul">
            <li class="li">Assign IP address, subnet mask, and default gateway</li>

          </ul>
</li>

        <li class="li">Create new account:<ul class="ul">
            <li class="li">Enter a username.</li>

            <li class="li">Enter a password.</li>

            <li class="li">Enter time zone.</li>

          </ul>

        </li>

      </ol>

      <p class="p">Once the initial installation is finished, complete the lifecycle-manager setup with these
        steps:</p>

      <ol class="ol">
        <li class="li">Ensure your lifecycle-manager node has a valid DNS nameserver specified in
            <samp class="ph codeph">/etc/resolv.conf</samp>.</li>

        <li class="li">Set the environment variable LC_ALL: <samp class="ph codeph">export LC_ALL=C</samp>
          <div class="note note"><span class="notetitle">Note:</span> This can be added to <samp class="ph codeph">~stack/.bashrc</samp> or
              <samp class="ph codeph">/etc/bash.bashrc</samp>.</div>
</li>

      </ol>

      <p class="p">At the end of this section you should have a node set up with Linux for HPE Helion on
        it.</p>

      <p class="p"><strong class="ph b">Configure and Run the Lifecycle-manager</strong></p>

      <div class="note important"><span class="importanttitle">Important:</span> It is critical that you don't run any of the commands below as the
          <samp class="ph codeph">root</samp> user or use <samp class="ph codeph">sudo</samp>, unless it is stated explicitly in
        the steps. Run then as the user you just created (or <samp class="ph codeph">stack</samp> if you left the
        default of "stack").</div>

      <ol class="ol">
        <li class="li">Log into your lifecycle-manager node as the user you created and mount the install media
          at <samp class="ph codeph">/media/cdrom</samp>. It may be necessary to use <samp class="ph codeph">wget</samp> or
          another file transfer method to transfer the install media to the deployer before
          completing this step. Here is the command to mount the media:
          <pre class="pre codeblock">sudo mount Helion-OpenStack-2.0.0.iso /media/cdrom</pre>
</li>

        <li class="li">Unpack the tarball that is in the <samp class="ph codeph">/media/cdrom/hos2.0.0/</samp> directory:
          <pre class="pre codeblock">tar xvf /media/cdrom/hos/hos-2.0.0-20151022T082820Z.tar</pre>
</li>

        <li class="li">Run the following included script: <pre class="pre codeblock">~/hos-2.0.0/hos-init.bash</pre>

          <p class="p">You will be prompted to enter an optional SSH passphrase when running
              <samp class="ph codeph">hos-init.bash</samp>. This passphrase is used to protect the key used by
            Ansible when connecting to its client nodes. If you do not want to use a passphrase then
            just press return at the prompt.</p>

          <p class="p">For automated installation (e.g. CI) it is possible to disable SSH passphrase prompting
            by setting the <samp class="ph codeph">HOS_INIT_AUTO</samp> environment variable before running
              <samp class="ph codeph">hos-init.bash</samp>, like this:</p>

          <pre class="pre codeblock">export HOST_INIT_AUTO=y</pre>
</li>

      </ol>

      <p class="p">If you have protected the SSH key with a passphrase then execute the following commands to
        avoid having to enter the passphrase on every attempt by Ansible to connect to its client
        nodes:</p>

      <pre class="pre codeblock">eval $(ssh-agent)
ssh-add ~/.ssh/id_rsa</pre>

      <p class="p">At the end of this section you should have a local directory structure, as described
        below:</p>

      <pre class="pre codeblock">helion/                        Top level directory
helion/examples/               Directory contains the config input files of the example clouds
helion/my_cloud/definition/    Directory contains the config input files
helion/my_cloud/config/        Directory contains .j2 files which are symlinks to the /hos/ansible directory
helion/hos/                    Directory contains files used by the installer
helion/tech-preview            Directory contains the config input files of the tech-preview clouds</pre>

      <p class="p">For any troubleshooting information regarding these steps, see <a class="xref" href="installation_troubleshooting.html#troubleshooting_installation__deployer_setup">Troubleshooting Your Installation</a></p>

    </div>

    <div class="section" id="install_esx__Configure"><h2 class="title sectiontitle">Prepare and Deploy Cloud Controllers</h2>
      
      <ol class="ol" id="install_esx__ol_c1w_pfl_ft">
        <li class="li">See a sample set of configuration files in the
            <samp class="ph codeph">~/helion/examples/entry-scale-esx</samp> directory. The accompanying README.md
          file explains the contents of each of the configuration files. </li>

        <li class="li">Copy the example configuration files into the required setup directory and edit them as
          required:
          <pre class="pre codeblock">cp -r ~/helion/examples/entry-scale-esx/* ~/helion/my_cloud/definition/</pre>
</li>

        
        <li class="li">Modify <samp class="ph codeph">~/helion/hos/ansible/hlm-deploy.yml</samp> to uncomment the line
          containing <samp class="ph codeph">eon-deploy.yml</samp>. You must comment the line containing
            <samp class="ph codeph">ceph-deploy.yml</samp>, <samp class="ph codeph">vsa-deploy.yml</samp>, and
            <samp class="ph codeph">cmc-deploy.yml</samp>.
            <p class="p"></p>
</li>

        <li class="li" id="install_esx__optional-eon">(<strong class="ph b">Optional</strong>) To enable centralised logging for EON service, do
          the following:<ol class="ol" type="a" id="install_esx__ol_ykv_czv_st">
            <li class="li">Change your directory to
              <pre class="pre codeblock">~/helion/my_cloud/config/logging/vars</pre>
</li>

            <li class="li">Edit <samp class="ph codeph">eon-api-clr.yml</samp> file to set the value of
                <strong class="ph b">centralized_logging</strong> to <strong class="ph b">true</strong> as shown in the following
              sample:<pre class="pre codeblock"> - centralized_logging:
        enabled: true
        format: rawjson
      files:
      - /var/log/eon/eon-api-json.log
      log_rotate:
      - daily
      - compress
      - missingok
      - notifempty
      - copytruncate
      - maxsize 45M
      - rotate 5
      - create 640 eon eon </pre>
</li>

            <li class="li">Edit <samp class="ph codeph">eon-conductor-clr.yml</samp> file to set the value of
                <strong class="ph b">centralized_logging</strong> to <strong class="ph b">true</strong> as shown in the following
              sample:<pre class="pre codeblock"> - centralized_logging:
        enabled: true
        format: rawjson
      files:
      - /var/log/eon/eon-conductor-json.log
      log_rotate:
      - daily
      - compress
      - missingok
      - notifempty
      - copytruncate
      - maxsize 45M
      - rotate 5
      - create 640 eon eon </pre>
</li>

          </ol>
</li>

        <li class="li">Commit your cloud deploy configuration to the<a class="xref" href="using_git.html"> local git
            repo</a>, as follows: <pre class="pre codeblock">cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</pre>

          <div class="note note"><span class="notetitle">Note:</span> This step needs to be repeated any time you make changes to your configuration files
            before you move onto the following steps. See <a class="xref" href="using_git.html">Using Git for
              Configuration Management</a> for more information.</div>
</li>

      </ol>

    </div>

    <p class="p">Then you need to run the following commands to complete your configuration. These commands
      also verify your configuration is correct.</p>

    <ol class="ol" id="install_esx__ol_og4_jy4_st">
      <li class="li">Run the following playbook which confirms that there is iLo connectivity for each of your
        nodes so that they are accessible to be re-imaged in a later step:
        <pre class="pre codeblock">cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-power-status.yml</pre>
</li>

      <li class="li">Run the configuration processor, as follows:
        <pre class="pre codeblock">cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre>
</li>

    </ol>

    <p class="p">If you receive an error during either of these steps then there is an issue with one or more
      of your configuration files. We recommend that you verify that all of the information in each
      of your configuration files is correct for your environment and then commit those changes to
      git using the instructions above.</p>


    <div class="section"><h2 class="title sectiontitle">Provision Your Baremetal Nodes</h2>
      <p class="p">To provision the baremetal nodes in your cloud deployment you can either use the automated
        operating system installation process provided by HPE Helion OpenStack or you can use the 3rd
        party installation tooling of your choice. We will outline both methods below:</p>

      <p class="p" id="install_esx__d283e403"><strong class="ph b">Using 3rd Party Baremetal Installers</strong></p>

      <p class="p">If you do not wish to use the automated operating system installation tooling included with
        HPE Helion OpenStack 2.0 then the requirements that have to be met using the installation
        tooling of your choice are:</p>

      <ul class="ul">
        <li class="li">The operating system must be installed via the HPE Linux for HPE Helion OpenStack ISO
          provided on the <a class="xref" href="https://helion.hpwsportal.com/catalog.html#/Category/%7B%22categoryId%22%3A10389%7D/Show" target="_blank">Helion Downloads</a> page.</li>

        <li class="li">Each node must have SSH keys in place that allow the user who will be doing the
          deployment to SSH to each node without a password.</li>

        <li class="li">Passwordless sudo needs to be enabled for the user.</li>

        <li class="li">There should be a LVM logical volume as <samp class="ph codeph">/root</samp> on each node.</li>

        <li class="li">If the LVM volume group name for the volume group holding the "root" LVM logical volume
          is hlm-vg then it will align with the disk input models in the examples.</li>

        <li class="li">Ensure that <samp class="ph codeph">openssh-server</samp>, <samp class="ph codeph">python</samp>, and
            <samp class="ph codeph">rsync</samp> are installed.</li>

      </ul>

      <p class="p">If you chose this method for installing your baremetal hardware, skip forward to the <a class="xref" href="install_entryscale_kvm.html#install_kvm__config_processor">Run the Configuration
          Processor</a> step.</p>

      <p class="p">If you would like to use the automated operating system installation tools provided by HP
        Helion OpenStack 2.0 then complete all of the steps below.</p>

      <p class="p"><strong class="ph b">Using the Automated Operating System Installation Provided by HPE Helion
        OpenStack</strong></p>

      <p class="p"><strong class="ph b">Part One: Deploy Cobbler</strong></p>

      <p class="p">This phase of the install process takes the baremetal information that was provided in
          <samp class="ph codeph">servers.yml</samp> and installs the Cobbler provisioning tool and loads this
        information into Cobbler. This sets each node to <samp class="ph codeph">netboot-enabled: true</samp> in
        Cobbler. Each node will be automatically marked as <samp class="ph codeph">netboot-enabled: false</samp>
        when it completes its operating system install successfully. Even if the node tries to PXE
        boot subsequently, Cobbler will not serve it. This is deliberate so that you can't reimage a
        live node by accident.</p>

      <p class="p">The <samp class="ph codeph">cobbler-deploy.yml</samp> playbook prompts for a password - this is the
        password that will be encrypted and stored in Cobbler, which is associated with the user
        running the command on the deployer, that you will use to log in to the nodes via their
        consoles after install. The username is the same as the user set up in the initial dialogue
        when installing the deployer from the iso, and is the same user that is running the
        cobbler-deploy play.</p>

      <ol class="ol">
        <li class="li">Run the following playbook which confirms that there is iLo connectivity for each of
          your nodes so that they are accessible to be re-imaged in a later step:
          <pre class="pre codeblock">cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-power-status.yml</pre>
</li>

        <li class="li">Run the following playbook to deploy Cobbler:
          <pre class="pre codeblock">cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre>
</li>

      </ol>

      <p class="p"><strong class="ph b">Part Two: Image the Nodes</strong></p>

      <div class="p">This phase of the install process goes through a number of distinct steps: <ol class="ol">
          <li class="li">Powers down the nodes to be installed</li>

          <li class="li">Sets the nodes hardware boot order so that the first option is a network boot.</li>

          <li class="li">Powers on the nodes. (The nodes will then boot from the network and be installed using
            infrastructure set up in the previous phase)</li>

          <li class="li">Waits for the nodes to power themselves down (this indicates a success install). This
            can take some time.</li>

          <li class="li">Sets the boot order to hard disk and powers on the nodes.</li>

          <li class="li">Waits for the nodes to be ssh-able and verifies that they have the signature
            expected.</li>

        </ol>

      </div>

      <p class="p">The reimage command is:</p>

      <pre class="pre codeblock">cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml [-e nodelist=node1,node2,node3]</pre>

      <p class="p">If a nodelist is not specified then the set of nodes in cobbler with
          <samp class="ph codeph">netboot-enabled: True</samp> is selected. The playbook pauses at the start to
        give you a chance to review the set of nodes that it is targeting and to confirm that it's
        correct.</p>

      <p class="p">You can use the command below which will list all of your nodes with the
          <samp class="ph codeph">netboot-enabled: True</samp> flag set:</p>

      <pre class="pre codeblock">sudo cobbler system find --netboot-enabled=1</pre>

      <p class="p">For any troubleshooting information regarding these steps, see <a class="xref" href="installation_troubleshooting.html#troubleshooting_installation__deploy_cobbler">Troubleshooting Your Installation</a></p>

    </div>

    <div class="section"><h2 class="title sectiontitle">Deploy the Cloud</h2>
      <ol class="ol">
        <li class="li">Use the playbook below to create a deployment directory:
          <pre class="pre codeblock">cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre>
</li>

        <li class="li">[OPTIONAL] - Run the <samp class="ph codeph">wipe_disks.yml</samp> playbook to ensure all of your
          partitions on your nodes are completely wiped before continuing with the installation. If
          you are using fresh machines this step may not be necessary.
          <pre class="pre codeblock">cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre>
</li>

        <li class="li">Run the <samp class="ph codeph">site.yml</samp> playbook using the command below. The <samp class="ph codeph">-e
            elasticsearch_cluster_name</samp> switch is used to set a unique name for your
          elasticsearch cluster. This should be set to a value other than
            <samp class="ph codeph">elasticsearch</samp>. <pre class="pre codeblock">cd ~/scratch/ansible/next/hos/ansible 
ansible-playbook -i hosts/verb_hosts site.yml -e elasticsearch_cluster_name=&lt;name&gt;</pre>

          <div class="p">If you have used an encryption password when running the configuration processor use
            the command below and enter the encryption password when prompted: 
            <pre class="pre codeblock">ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</pre>

          </div>

          <div class="note note"><span class="notetitle">Note:</span> The step above runs <samp class="ph codeph">osconfig</samp> to configure the cloud and
              <samp class="ph codeph">hlm-deploy</samp> to deploy the cloud. Therefore, this step may run for a
            while, perhaps 45 minutes or more, depending on the number of nodes in your
            environment.</div>

        </li>

        <li class="li">Verify that the network is working correctly. Ping each IP in the
            <samp class="ph codeph">/etc/hosts</samp> file from one of the controller nodes.</li>

      </ol>

      <p class="p">For any troubleshooting information regarding these steps, see <a class="xref" href="installation_troubleshooting.html#troubleshooting_installation__deploy_cloud">Troubleshooting Your Installation</a></p>

    </div>

    <div class="section" id="install_esx__prepAndDeploy"><h2 class="title sectiontitle">Prepare and Deploy ESX Computes and OVSvAPPs </h2>
      
      <p class="p">The following sections describe the procedure to install and configure ESX compute and
        OVSvAPPs on vCenter.</p>

      <ul class="ul" id="install_esx__ul_lns_fjl_ft">
        <li class="li"><a class="xref" href="#install_esx__deploy_template">Deploy Helion Linux Shell VM
            Template</a></li>

        <li class="li"><a class="xref" href="#install_esx__prepare_esx_cloud_deployment">Preparation for ESX
            Cloud Deployment</a></li>

      </ul>

    </div>

    <div class="section" id="install_esx__deploy_template"><strong class="ph b">Deploy Helion Linux Shell VM Template</strong><p class="p">The first step in
        deploying the ESX compute proxy and OVSvAPPs is to create a VM template that will make it
        easier to deploy the ESX compute proxy for each Cluster and OVSvAPPs on each ESX server.
        </p>
<div class="p">Perform the following steps to deploy a template:<ol class="ol" id="install_esx__ol_qdt_ljs_ft">
          <li class="li">Import the <samp class="ph codeph">hlm-shell-vm.ova</samp> in the vCenter using the vSphere client.
            The <samp class="ph codeph">hlm-shell-vm.ova</samp> template is present in the following
            location:<pre class="pre codeblock">location /media/cdrom/hos-2.0.0/ hlm-shell-vm.ova</pre>
</li>

          <li class="li">In the vSphere Client, click <strong class="ph b">File</strong> and then click <strong class="ph b">Deploy OVF
            Template</strong></li>

          <li class="li">Follow the instructions in the wizard to specify the data center, cluster, and node to
            install. Refer to the VMWare vSphere documentation as needed.</li>

        </ol>
</div>
</div>

    <div class="section" id="install_esx__prepare_esx_cloud_deployment"><strong class="ph b">Preparation for ESX Cloud Deployment</strong><p class="p">This
        section describes the procedures to prepare and deploy the ESX computes and OVSvAPPs for
        deployment. </p>
<div class="p">
        <ol class="ol" id="install_esx__ol_xpc_zqs_ft">
          <li class="li">Login to the deployer/lifecycle-manager node.</li>

          <li class="li">Source <samp class="ph codeph">service.osrc</samp>.</li>

          <li class="li"><a class="xref" href="#install_esx__register-vcenter">Register a vCenter
              Server</a></li>

          <li class="li"><a class="xref" href="#install_esx__register-network">Register ESX Cloud Network
              Configuration</a></li>

          <li class="li"><a class="xref" href="#install_esx__import-cluster">Import Clusters</a></li>

          <li class="li"><a class="xref" href="#install_esx__activate-cluster">Activate Clusters</a><ol class="ol" type="a" id="install_esx__ol_isb_wcc_vt">
              <li class="li"><a class="xref" href="#install_esx__modify-volume-config">Modify the Volume
                  Configuration File</a></li>

              <li class="li"><a class="xref" href="#install_esx__commit-your-cloud">Commit your Cloud
                  Definition</a></li>

              <li class="li"><a class="xref" href="#install_esx__deploy-compute-proxy-ovsvapps">Deploy ESX
                  Compute Proxy and OVSvApps</a></li>

            </ol>
</li>

        </ol>

      </div>
</div>

    <p class="p"><strong class="ph b">Manage vCenters and Clusters</strong></p>

    <p class="p">The following section describes the detailed procedure on managing the vCenters and
      clusters.</p>

    <p class="p" id="install_esx__register-vcenter"><strong class="ph b">Register a vCenter Server</strong></p>

    <div class="p">vCenter provides centralized management of virtual host and virtual machines from a single
        console.<ol class="ol" id="install_esx__ol_xdj_5js_ft">
        <li class="li">Add a vCenter using EON python
            client.<pre class="pre codeblock"><samp class="ph codeph"># eon vcenter-add --name &lt;vCenter Name&gt; --ip-address &lt;vCenter IP address&gt; --username &lt;vCenter Username&gt; --password &lt;vCenter Password&gt; --port &lt;vCenter Port&gt;</samp></pre>
<div class="p">where:
              <ul class="ul" id="install_esx__ul_bp3_yjs_ft">
              <li class="li">vCenter Name - the identical name of the vCenter server.</li>

              <li class="li">vCenter IP address - the IP address of the vCenter server.</li>

              <li class="li">vCenter Username - the admin privilege username for the vCenter.</li>

              <li class="li">vCenter Password - the password for the above username.</li>

              <li class="li">vCenter Port - the vCenter server port. By default it is 443. <div class="note important"><span class="importanttitle">Important:</span> Please do not change the vCenter Port unless you are certain it is required to do so.</div>
</li>

            </ul>
</div>
<div class="p"><strong class="ph b">Sample
            Output:</strong><pre class="pre codeblock"><samp class="ph codeph"># eon vcenter-add --name vc01 --ip-address 10.1.200.41 --username administrator@vsphere.local --password password --port 443</samp>
+------------+--------------------------------------+
| Property   | Value                                |
+------------+--------------------------------------+
| created_at | 2015-08-20T12:08:09.000000           |
| deleted    | False                                |
| deleted_at | None                                 |
| id         | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| ip_address | 10.1.200.41                          |
| name       | vc01                                 |
| password   | &lt;SANITIZED&gt;                          |
| port       | 443                                  |
| type       | vcenter                              |
| updated_at | 2015-08-20T12:08:09.000000           |
| username   | administrator@vsphere.local          |
+------------+--------------------------------------+</pre>
</div>
</li>

      </ol>
</div>

    <div class="section"><strong class="ph b">Show vCenter</strong><ol class="ol" id="install_esx__ol_wh5_rsb_lt">
        <li class="li">Show vCenter using EON python
              client.<pre class="pre codeblock"># eon vcenter-show &lt;vCenter ID&gt;</pre>
<div class="p"><strong class="ph b">Sample
              Output:</strong><pre class="pre codeblock"><samp class="ph codeph"># eon vcenter-show BC9DED4E-1639-481D-B190-2B54A2BF5674 </samp>
+------------+--------------------------------------+
| Property   | Value                                |
+------------+--------------------------------------+
| created_at | 2015-08-20T12:08:09.000000           |
| datacenters| DC1                                  |
| deleted    | False                                |
| deleted_at | None                                 |
| id         | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| ip_address | 10.1.200.41                          |
| name       | vc01                                 |
| password   | &lt;SANITIZED&gt;                          |
| port       | 443                                  |
| type       | vcenter                              |
| updated_at | 2015-08-20T12:08:09.000000           |
| username   | administrator@vsphere.local          |
+------------+--------------------------------------+</pre>
</div>
</li>

      </ol>
</div>

    <div class="section" id="install_esx__register-network"><strong class="ph b">Register ESX Cloud Network Configuration</strong>
      <p class="p">This involves getting a sample network information template. Fill the details of the
        template and use that template to register the cloud network configuration for the vCenter.</p>

      <div class="p">
        <ol class="ol">
          <li class="li">Execute the following command to get the network information
              template:<pre class="pre codeblock"><samp class="ph codeph"># eon get-network-info-template --filename &lt;<strong class="ph b">NETWORK_CONF_FILENAME</strong>&gt;</samp></pre>
<div class="p">For
              example:<pre class="pre codeblock"># eon get-network-info-template --filename net_conf.json</pre>
</div>
<div class="p">Sample
              file of <samp class="ph codeph">net_conf.json</samp> is shown
              below:<pre class="pre codeblock">{
    "network": {
        # Deployer Network details
        # This network should be reachable from the Deployer node
        "deployer_network": {
            #Deployer Portgroup Name. If already exists then we will use it. 	     #If not then we will create it.
            "deployer_pg_name": "hlm-Deployer-PG",

            #VLAN id for Deployer Portgroup
            "deployer_vlan": "33",

#Enable DHCP for Deployer network ?
            "enable_deployer_dhcp": "no",

#CIDR and gateway for deployer network only when enable_deployer_dhcp is no
            "deployer_cidr": "10.20.18.0/23",
            "deployer_gateway_ip": "10.20.18.1",

            #Deployer Node's PXE IP Address
            "deployer_node_ip": "10.20.16.2"
        },

        #Management Network details
        "management_network": {
#Mgmt DVS name. If already exists then we will use it. If not #then we will create it.
            "mgmt_dvs_name": "hlm-Mgmt",

#Physical NIC name(s) for Mgmt DVS. Make sure that this Physical #NIC(s) is/are free/not used across all hosts in that cluster.
            "mgmt_nic_name": "vmnic3",

#Mgmt Portgroup Name. If already exists then we will use it. 	     #If not then we will create it.
            "mgmt_pg_name": "hlm-Mgmt-PG",

            #Interface order: Example eth1
            "mgmt_interface_order": "eth1",

#Provided physical NIC name(s) will be configured as active #uplink(s) from mgmt_dvs_name. If not provided then the first #entry in mgmt_dvs_name will be configured as active and the #remaining as standby.
            "active_nics": "",





            #Load Balancing. Please choose the corresponding number

#1 -&gt; Route based on the originating virtual port (Choose an            #uplink based on the virtual port where the traffic entered the #virtual switch)

#2 -&gt; Route based on IP hash (Choose an uplink based on a hash #of the source and destination IP addresses of each packet. For #non-IP packets, whatever is at those offsets is used to compute #the hash)

#3 -&gt; Route based on source MAC hash (Choose an uplink based on #a hash of the source Ethernet.)

#4 -&gt; Route based on physical NIC load (Choose an uplink based on #the current loads of physical NICs)

#5 -&gt; Use explicit failover order (Always use the highest order    #uplink, from the list of Active adapters, which passes failover #detection criteria)

	    "load_balancing": "1",

#Network Failover Detection. Please choose the corresponding number
# 1 -&gt; Link Status. Relies solely on the link status that the #network adapter provides. This option detects failures, such as #cable pulls and physical switch power failures, but not #configuration errors, such as a physical switch port being #blocked by spanning tree or that is misconfigured to the wrong #VLAN or cable pulls on the other side of a physical switch.

# 2-&gt; Beacon Probing. Sends out and listens for beacon probes on all #NICs in the team and uses this information, in addition to link #status, to determine link failure. This detects many of the #failures previously mentioned that are not detected by link #status alone.

            "network_failover_detection": "1",

	    #Notify Switches(yes/no).
#If you select Yes, whenever a virtual NIC is connected to the          #Switch or whenever that virtual NIC’s traffic would be routed #over a different physical NIC in the team because of a failover #event, a notification is sent out over the network to update the #lookup tables on physical switches. In almost all cases, this #process is desirable for the lowest latency of failover #occurrences and migrations with vMotion.

            "notify_switches": "yes"
        },

        "data_network": {
            #Tenant network type. Only vlan is supported for HOS 2.0
            "tenant_network_type": "vlan",

#Data/Uplink DVS name. If already exists then we will use it. If #not then we will create it.
            "data_dvs_name": "hlm-Data",

#Physical NIC name(s) for Data/Uplink DVS. Make sure that this #Physical NIC(s) is/are free/not used across all hosts in that #cluster.
            "data_nic_name": "vmnic2, vmnic1",

#Data Portgroup Name. If already exists then we will use it. If      #not then we will create it.
            "data_pg_name": "hlm-Data-PG",

            #Interface order: Example eth2
            "data_interface_order": "eth2",

#Provided physical NIC name(s) will be configured as active #uplink(s) from data_dvs_name. If not provided then the first #entry in data_dvs_name will be configured as active and the #remaining as standby.

            "active_nics": "vmnic1",

	    #Load Balancing. Please choose the corresponding number

#1 -&gt; Route based on the originating virtual port (Choose an            #uplink based on the virtual port where the traffic entered the #virtual switch)

#2 -&gt; Route based on IP hash (Choose an uplink based on a hash #of the source and destination IP addresses of each packet. For #non-IP packets, whatever is at those offsets is used to compute #the hash)

#3 -&gt; Route based on source MAC hash (Choose an uplink based on #a hash of the source Ethernet.)

#4 -&gt; Route based on physical NIC load (Choose an uplink based on #the current loads of physical NICs)

#5 -&gt; Use explicit failover order (Always use the highest order    #uplink, from the list of Active adapters, which passes failover #detection criteria)
            
            "load_balancing": "1",


#Network Failover Detection. Please choose the corresponding number
# 1 -&gt; Link Status. Relies solely on the link status that the #network adapter provides. This option detects failures, such as #cable pulls and physical switch power failures, but not #configuration errors, such as a physical switch port being #blocked by spanning tree or that is misconfigured to the wrong #VLAN or cable pulls on the other side of a physical switch.

# 2-&gt; Beacon Probing. Sends out and listens for beacon probes on all #NICs in the team and uses this information, in addition to link #status, to determine link failure. This detects many of the #failures previously mentioned that are not detected by link #status alone.
            
            "network_failover_detection": "1",

	    #Notify Switches(yes/no).
#If you select Yes, whenever a virtual NIC is connected to the          #Switch or whenever that virtual NIC’s traffic would be routed #over a different physical NIC in the team because of a failover #event, a notification is sent out over the network to update the #lookup tables on physical switches. In almost all cases, this #process is desirable for the lowest latency of failover #occurrences and migrations with vMotion.

            “notify_switches”: “yes”
        },

        “hpvcn_trunk_network”: {
#Trunk DVS name. If already exists then we will use it. If not #then we will create it.
            "trunk_dvs_name": "hlm-Trunk",

#Trunk portgroup name. If already exists then we will use it. If #not then we will create it.
            "trunk_pg_name": "hlm-Trunk-PG",

            #Interface order: Example eth3
            "trunk_interface_order": "eth3"
        },

#VLAN Range for Data &amp; Trunk port group. Please provide the range #separated by a hyphen (vlan-vlan). Multiple vlan or vlan ranges should #be provided as comma separated value.
        "vlan_range": "1-4094"
    },

    "template": {
#Provide the template name that will be used for cloning Computeproxy #and OVSvApp VMs
        "template_name": "hlm-template"
    },

    "vmconfig": {
        #Number of CPUs for OVSvApp/Computeproxy VM
        "cpu": "4",

        #Amount of RAM for OVSvApp/Computeproxy VM(in Mega Byte)
        "memory_in_mb": "4096",

#SSH public key content for OVSvAPP/Computeproxy password less login. #Carefully copy the public key and paste it within the double quotes.
        "ssh_key": "&lt;deployer-ssh-pub-key-contents&gt;"
    }
}</pre>
</div>
</li>

          <li class="li">Modify the template (json file) as per your
              environment.<pre class="pre codeblock">vi &lt;<strong class="ph b">NETWORK_CONF_FILENAME</strong>&gt;</pre>
<div class="p">For
              example:<pre class="pre codeblock">vi net_conf.json</pre>
</div>
</li>

          <li class="li">Use the template to register Cloud Network Configuration. This sets the network
            information for a vCenter which is used to deploy and configure compute proxy and
            OVSvAPP VMs during the cluster activation.
              <pre class="pre codeblock"><samp class="ph codeph"># eon set-network-info --vcenter-id &lt;vCenter ID&gt; --datacenter-name &lt;datacenter name&gt; --config-json &lt;NETWORK_CONF_FILENAME&gt;</samp></pre>
<div class="p">For
              example:
              <pre class="pre codeblock"># eon set-network-info --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674 --datacenter DC1 --config-json net_conf.json</pre>
</div>
<div class="p">
              <div class="note note"><span class="notetitle">Note:</span> The vcenter ID is generated when you register a vCenter.</div>

            </div>
</li>

          <li class="li">Execute the following command to view the list of clusters for the given
              vCenter.<pre class="pre codeblock"><samp class="ph codeph"># eon cluster-list --vcenter-id &lt;vCenter ID&gt;</samp></pre>
<strong class="ph b">Sample
              Output</strong><pre class="pre codeblock"># eon cluster-list --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674
+------------+----------+------------+---------------+
| MOID       | Name     | Datacenter | Import Status |
+------------+----------+------------+---------------+
| domain-c21 | Cluster1 | DC1        | not_imported  |
+------------+----------+------------+---------------+</pre>
</li>

        </ol>

      </div>
</div>

    <div class="section" id="install_esx__import-cluster"><strong class="ph b">Import Cluster </strong><p class="p">You can use one or more ESX clusters for
        ESX Cloud Deployment. When an Import Cluster is invoked, required ESX Compute Proxy and
        OVSvApp nodes are deployed.</p>
<p class="p">vCenter can have multiple clusters, but the current
        release of HPE Helion OpenStack supports only sequential import of clusters (one-by-one).
        Therefore, you can import only one cluster at a time using the <samp class="ph codeph">eon
          cluster-import</samp> command.</p>
<div class="p">
        <ol class="ol">
          <li class="li"> Import the cluster for the EON database under the given vCenter. <pre class="pre codeblock"><samp class="ph codeph"># eon cluster-import --vcenter-id &lt;vCenter ID&gt; --cluster-name &lt;Cluster Name&gt; --cluster-moid &lt;Cluster Moid&gt;</samp></pre>
where:<div class="p">
              <ul class="ul" id="install_esx__ul_r4g_rjs_ft">
                <li class="li">vCenter ID - ID of the vcenter containing the cluster.</li>

                <li class="li">Cluster Name - the name of the cluster that needs to be imported.</li>

                <li class="li">Cluster Moid - Moid of the cluster that needs to be imported.</li>

              </ul>

            </div>
<div class="p"><strong class="ph b">Sample
              Output</strong><pre class="pre codeblock"># eon cluster-import --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674 --cluster-name Cluster1 --cluster-moid domain-c21
+--------------+-----------+
| Property     | Value     |
+--------------+-----------+
| cpu_free     | 83071.73  |
| cpu_total    | 83072     |
| cpu_used     | 0.27      |
| datacenter   | DC1       |
| disk_free    | 1022.79   |
| disk_total   | 1023.75   |
| errors       | []        |
| memory_free  | 496.82    |
| memory_total | 511.76    |
| memory_used  | 14.94     |
| name         | Cluster1  |
| state        | importing |
| switches     | []        |
+--------------+-----------+</pre>
</div>
<p class="p">One
              vCenter can have multiple clusters. But it allows you to import only one cluster at a
              time.</p>
</li>

          <li class="li"> Execute the following command to view the list of clusters for the given
              vCenter.<pre class="pre codeblock"><samp class="ph codeph"># eon cluster-list --vcenter-id &lt;vCenter ID&gt;</samp></pre>
<strong class="ph b">Sample
              Output</strong><pre class="pre codeblock"># eon cluster-list --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674
+------------+----------+------------+---------------+
| MOID       | Name     | Datacenter | Import Status |
+------------+----------+------------+---------------+
| domain-c22 | Cluster2 | DC1        | imported      |
+------------+----------+------------+---------------+</pre>
</li>

        </ol>

      </div>
</div>

    <div class="section" id="install_esx__activate-cluster"><strong class="ph b">Activate Clusters</strong><div class="note note"><span class="notetitle">Note:</span> You can activate the cluster only
        after the import status of the cluster is changed to <strong class="ph b">imported</strong>.</div>

      <p class="p">When you execute the active cluster command, the <samp class="ph codeph">server.yml</samp> of the input
        model is updated with IP Addresses of compute proxy and OVSvApp.
        </p>

      <div class="p">
        <ol class="ol">
          <li class="li">Activate the cluster for the selected
                vCenter.<pre class="pre codeblock"><samp class="ph codeph"># eon cluster-activate --vcenter-id &lt;vCenter ID&gt; --cluster-moid &lt;Cluster Moid&gt; </samp></pre>
<p class="p"><strong class="ph b">Sample
                Output</strong></p>
<div class="p">
              <pre class="pre codeblock"># eon cluster-activate --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674 --cluster-moid domain-c22 
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Property      | Value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| node_info     | {u'computeproxy': {u'pxe-mac-addr': u'00:50:56:b6:ce:1b', u'pxe-ip-addr': u'172.170.2.4', u'name': u'COMPUTEPROXY_Cluster1', u'cluster-moid': u'domain-c21'}, u'network_driver': {u'cluster_dvs_mapping': u'DC1/host/Cluster1:hlm-Trunk', u'Cluster1': [{u'host-moid': u'host-29', u'pxe-ip-addr': u'172.170.2.3', u'esx_hostname': u'10.1.200.33', u'ovsvapp_node': u'ovsvapp-10-1-200-33', u'pxe-mac-addr': u'00:50:56:b6:5e:9a'}, {u'host-moid': u'host-25', u'pxe-ip-addr': u'172.170.2.2', u'esx_hostname': u'10.1.200.66', u'ovsvapp_node': u'ovsvapp-10-1-200-66', u'pxe-mac-addr': u'00:50:56:b6:56:e6'}]}} |
| resource_moid | domain-c21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| resource_name | Cluster1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| state         | activated                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</pre>

            </div>
</li>

        </ol>

      </div>
</div>

    <div class="section" id="install_esx__modify-volume-config"><strong class="ph b">Modify the Volume Configuration File</strong>
      <p class="p">Once the cluster is activated you must configure the volume.</p>
<p class="p">Perform the following
        steps to modify the volume configuration files:</p>
<ol class="ol" id="install_esx__ol_bhx_n5p_st">
        <li class="li">Change the directory. The <samp class="ph codeph">cinder.conf.j2</samp> is present in following
          directories
            :<pre class="pre codeblock">cd /home/stack/helion/hos/ansible/roles/_CND-CMN/templates</pre>
<div class="p">OR<pre class="pre codeblock">cd /home/stack/helion/my_cloud/config/cinder</pre>
</div>
<p class="p">It
            is recommended to modify the <samp class="ph codeph">cinder.conf.j2</samp> present in
              <samp class="ph codeph">/home/stack/helion/my_cloud/config/cinder</samp></p>
</li>

        <li class="li">Modify the <samp class="ph codeph">cinder.conf.j2</samp> as follows:
          <pre class="pre codeblock"># Configure the enabled backends
enabled_backends=&lt;unique-section-name&gt;

# Start of section for VMDK block storage
#
# If you have configured VMDK Block storage for cinder you must
# uncomment this section, and replace all strings in angle brackets
# with the correct values for vCenter you have configured. You
# must also add the section name to the list of values in the
# 'enabled_backends' variable above. You must provide unique section
# each time you configure a new backend.

#[&lt;unique-section-name&gt;]
#vmware_api_retry_count = 10
#vmware_tmp_dir = /tmp
#vmware_image_transfer_timeout_secs = 7200
#vmware_task_poll_interval = 0.5
#vmware_max_objects_retrieval = 100
#vmware_volume_folder = cinder-volumes
#volume_driver = cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver
#vmware_host_ip = &lt;ip_address_of_vcenter&gt;
#vmware_host_username = &lt;vcenter_username&gt;
#vmware_host_password = &lt;password&gt;
#
#volume_backend_name = &lt;vmdk-backend-name&gt;
#
# End of section for VMDK block storage</pre>
</li>

      </ol>
</div>

    <div class="section" id="install_esx__commit-your-cloud"><strong class="ph b">Commit your Cloud Definition</strong>
      <div class="p">
        <ol class="ol">
          <li class="li"> Add the cloud deployment definition to git
            :<pre class="pre codeblock">cd /home/stack/helion/hos/ansible;
git add -A;
git commit -m 'Adding ESX Configurations or other commit message';</pre>
</li>

          <li class="li">Prepare your environment for deployment:
            <pre class="pre codeblock">ansible-playbook -i hosts/localhost config-processor-run.yml;
ansible-playbook -i hosts/localhost ready-deployment.yml;
cd /home/stack/scratch/ansible/next/hos/ansible;</pre>
</li>

        </ol>

      </div>
</div>

    <div class="section" id="install_esx__deploy-compute-proxy-ovsvapps"><strong class="ph b">Deploy ESX Compute Proxy and OVSvApps</strong>
      <div class="p">Execute the following command to deploy an esx compute and
        OVSvApps:<pre class="pre codeblock">ansible-playbook -i hosts/verb_hosts guard-deployment.yml
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit '*<strong class="ph b">esx-ovsvapp:*esx-compute</strong>' 
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml --limit NOV-ESX:NEU-OVSVAPP
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</pre>
</div>

      <div class="note note"><span class="notetitle">Note:</span> The variable <strong class="ph b">esx-ovsvapp</strong> and <strong class="ph b">esx-compute</strong> must be taken from the
          <strong class="ph b">name</strong> key in the <samp class="ph codeph">resource-nodes</samp> section in the
          <samp class="ph codeph">data/control_plane.yml</samp> file
          (<samp class="ph codeph">/home/stack/helion/my_cloud/definition/data/control_plane.yml</samp>).
      </div>
</div>

  </div>

<div class="navfooter"><!----></div><div class="footer">WebHelp output generated by<a class="oxyFooter" href="http://www.oxygenxml.com" target="_blank"><span class="oXygenLogo"><img src="../../oxygen-webhelp/resources/img/LogoOxygen100x22.png" alt="Oxygen"></img></span><span class="xmlauthor">XML Author</span></a></div>
</body>
</html>