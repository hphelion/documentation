
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xml:lang="en-us" lang="en-us">
<head><meta name="description" content="This page covers the following topics: High Availability Concepts Overview Highly Available Cloud Infrastructure Highly Available Cloud-Aware Tenant Workloads Highly Available Cloud Infrastructure ..."></meta><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><meta name="DC.Type" content="topic"></meta><meta name="DC.Title" content="HPE Helion OpenStack 2.0: High Availability (HA) Concepts"></meta><meta name="prodname" content="HPE Helion"></meta><meta name="version" content="4.1.0"></meta><meta name="copyright" content="HPE Helion 2015" type="primary"></meta><meta name="DC.Rights.Owner" content="HPE Helion 2015" type="primary"></meta><meta name="DC.Format" content="XHTML"></meta><meta name="DC.Identifier" content="HP2.0HA"></meta><meta name="DC.Language" content="en-us"></meta><link rel="stylesheet" type="text/css" href="../../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>HPE Helion OpenStack 2.0: High Availability (HA) Concepts</title><!--  Generated with Oxygen version 17.0, build number 2015072912.  --><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><link rel="stylesheet" type="text/css" href="../../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../../oxygen-webhelp/resources/skins/skin.css"><!----></link><script type="text/javascript"><!--
          
          var prefix = "../../index.html";
          
          --></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery-1.8.2.min.js"><!----></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script></head>
<body onload="highlightSearchTerm()" class="frmBody" id="HP2.0HA">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td width="75%"></td><td><div class="navheader"></div></td></tr></tbody></table>

  <h1 class="title topictitle1">HPE Helion OpenStack<sup>Â®</sup> 2.0: High Availability (HA) Concepts</h1>

  <div class="body">
    <p class="p">This page covers the following topics:</p>

    <ul class="ul">
      <li class="li"><a class="xref" href="#HP2.0HA__concepts_overview">High Availability Concepts Overview</a>
        <ul class="ul">
          <li class="li"><a class="xref" href="#HP2.0HA__cloud_infrastructure">Highly Available Cloud Infrastructure</a></li>

          <li class="li"><a class="xref" href="#HP2.0HA__tenant_workloads">Highly Available Cloud-Aware Tenant Workloads</a></li>

        </ul>

      </li>

      <li class="li"><a class="xref" href="#HP2.0HA__highly_available_cloud_infrastructure">Highly Available Cloud Infrastructure</a></li>

      <li class="li"><a class="xref" href="#HP2.0HA__high_availablity_controllers">High Availablity of Controllers</a>
        <ul class="ul">
          <li class="li"><a class="xref" href="#HP2.0HA__api_request">API Request Message Flow</a></li>

          <li class="li"><a class="xref" href="#HP2.0HA__node_failure">Handling Node Failure</a></li>

          <li class="li"><a class="xref" href="#HP2.0HA__network_partitions">Handling Network Partitions</a></li>

          <li class="li"><a class="xref" href="#HP2.0HA__galera_cluster">MySQL Galera Cluster</a></li>

          <li class="li"><a class="xref" href="#HP2.0HA__singleton_services">Singleton Services</a>
            <ul class="ul">
              <li class="li"><a class="xref" href="#HP2.0HA__cinder_volume">Cinder-Volume</a></li>

              <li class="li"><a class="xref" href="#HP2.0HA__sherpa">Sherpa</a></li>

              <li class="li"><a class="xref" href="#HP2.0HA__nova_consoleauth">Nova consoleauth</a></li>

            </ul>

          </li>

          <li class="li"><a class="xref" href="#HP2.0HA__failed_controller_nodes">Rebuilding or Replacing failed Controller Nodes</a></li>

        </ul>

      </li>

      <li class="li"><a class="xref" href="#HP2.0HA__availability_zones">Availability Zones</a></li>

      <li class="li"><a class="xref" href="#HP2.0HA__compute_kvm">Compute with KVM</a></li>

      <li class="li"><a class="xref" href="#HP2.0HA__nova_availability_zones">Nova Availability Zones</a></li>

      <li class="li"><a class="xref" href="#HP2.0HA__compute_esx">Compute with ESX Hypervisor</a></li>

      <li class="li"><a class="xref" href="#HP2.0HA__block_storage_vsa">Block Storage with StoreVirtual VSA</a></li>

      <li class="li"><a class="xref" href="#HP2.0HA__cinder_availability_zones">Cinder Availability Zones</a></li>

      <li class="li"><a class="xref" href="#HP2.0HA__object_storage_swift">Object Storage with Swift</a></li>

      <li class="li"><a class="xref" href="#HP2.0HA__highly_available_app_workloads">Highly Available Cloud Applications and Workloads</a></li>

      <li class="li"><a class="xref" href="#HP2.0HA__what_not_ha">What is not Highly Available?</a>
        <ul class="ul">
          <li class="li"><a class="xref" href="#HP2.0HA__deployer">Deployer</a></li>

          <li class="li"><a class="xref" href="#HP2.0HA__control_plane">Control Plane</a></li>

        </ul>

      </li>

      <li class="li"><a class="xref" href="#HP2.0HA__more_information">More Information</a></li>

    </ul>



    <div class="section" id="HP2.0HA__concepts_overview"><h2 class="title sectiontitle">High Availability Concepts Overview</h2>
      
      <p class="p">Highly Available Cloud ensures that at least a minimum of cloud resources are always
        available on request, which results in uninterrupted operations for users.</p>

      <p class="p">In order to achieve this high availability of infrastructure and workloads, we define the
        scope of HA to be limited to protecting these only against single points of failure (SPOF).
        Single points of failure include:</p>

      <ul class="ul">
        <li class="li"><strong class="ph b">Hardware SPOFs</strong>: Hardware failures can take the form of server failures, memory going bad, power failures, 
          hypervisors crashing, hard disks dying, NIC cards breaking, switch ports failing, network cables loosening, 
          and so forth.</li>

        <li class="li"><strong class="ph b">Software SPOFs</strong>: Server processes can crash due to software defects, out-of-memory conditions, operating system kernel panic, and so forth.</li>

      </ul>

      <p class="p">By design, HPE Helion OpenStack strives to create a system architecture resilient to SPOFs,
        and does not attempt to automatically protect the system against multiple cascading levels
        of failures; such cascading failures will result in an unpredictable state.  Hence, the
        cloud operator is encouraged to recover and restore any failed component, as soon as the
        first level of failure occurs.</p>

 
      <div class="sectiondiv">
        <p class="p"><strong class="ph b">Highly Available Cloud Infrastructure</strong></p>

        <p class="p">Cloud users are able to provision and manage the compute, storage, and network
          infrastructure resources at any given point in time and the Horizon Dashboard and the
          OpenStack APIs must be reachable and be able to fulfill user requests.</p>

        <p class="p"><img class="image" id="HP2.0HA__CloudInfrastructure" src="../../media/ha/ha-resilient-cloud-infrastructure.png"></img></p>

        <p class="p">Once the Compute, Storage, and Network resources are deployed, users expect these resources to be 
          reliable in the following ways:</p>

        <ul class="ul">
          <li class="li">If the Nova-Compute KVM Hypervisors/servers hosting the project compute virtual
            machine (VM) dies and the compute VM is lost along with its local ephemeral storage, the
            re-launching of the dead compute VM succeeds because it launches on another Nova-Compute
            KVM Hypervisor/server.  If ephemeral storage loss is undesirable, the compute VM can be
            booted from the Cinder volume.</li>

          <li class="li">Data stored in Block Storage service volumes can be made highly-available by clustering <a class="xref" href="#HP2.0HA__block_storage_vsa">(Details below in VSA section below)</a></li>

          <li class="li">Data stored by the Object service is always available <a class="xref" href="#HP2.0HA__object_storage_swift">(Details in Swift section below)</a></li>

          <li class="li">Network resources such as routers, subnets, and floating IP addresses provisioned by the 
            Networking Operation service are made highly-available via Helion Control Plane redundancy and DVR.</li>

        </ul>

        <p class="p">The infrastructure that provides these features is called a <strong class="ph b">Highly Available Cloud Infrastructure</strong>.</p>

      </div>
      
      <div class="sectiondiv">
        <p class="p"><strong class="ph b">Highly Available Cloud-Aware Tenant Workloads</strong></p>

        <p class="p">HPE Helion OpenStack Compute hypervisors do not support transparent high availability for user 
          applications; as such, the project application provider is responsible for deploying their applications 
          in a redundant and highly available manner, using multiple VMs spread appropriately across availability 
          zones, routed through the load balancers and made highly available through clustering.</p>

        <p class="p">These are known as <strong class="ph b">Highly Available Cloud-Aware Tenant Workloads</strong>.</p>

      </div>
      
    </div>

    
    <div class="section" id="HP2.0HA__highly_available_cloud_infrastructure"><h2 class="title sectiontitle">Highly Available Cloud Infrastructure</h2>
      
      <p class="p">The highly available cloud infrastructure consists of the following:</p>

      <ul class="ul">
        <li class="li">High Availability of Controllers</li>

        <li class="li">Availability Zones</li>

        <li class="li">Compute with KVM</li>

        <li class="li">Nova Availability Zones</li>

        <li class="li">Compute with ESX</li>

        <li class="li">Clock Storage with StoreVirtual VSA</li>

        <li class="li">Object Storage with Swift</li>

      </ul>

    </div>

    
    <div class="section" id="HP2.0HA__high_availablity_controllers"><h2 class="title sectiontitle">High Availability of Controllers</h2>
      
      <p class="p">The HPE Helion OpenStack installer deploys highly available configurations of 
        OpenStack cloud services, resilient against single points of failure.</p>

      <p class="p">The high availability of the controller components comes in two main forms.</p>

      <ul class="ul">
        <li class="li">Many services are stateless and multiple instances are run across the control 
          plane in active-active mode. The API services (nova-api, cinder-api, etc.) are 
          accessed through the HA proxy load balancer whereas the internal services 
          (nova-scheduler, cinder-scheduler, etc.), are accessed through the message 
          broker. These services use the database cluster to persist any data.
          <div class="note note"><span class="notetitle">Note:</span> The HA proxy load balancer is also run in active-active mode and 
            keepalived (used for Virtual IP (VIP) Management) is run in active-active 
            mode, with only one keepalived instance holding the VIP at any one point 
            in time.</div>
</li>

        <li class="li">The high availability of the message queue service and the database service 
          is achieved by running these in a clustered mode across the three nodes of the 
          control plane: RabbitMQ cluster with Mirrored Queues and Percona MySQL Galera cluster.</li>

      </ul>

      <p class="p"><img class="image" id="HP2.0HA__ControlPlane1" src="../../media/ha/ControlPlaneHA2.0_1.png"></img></p>

      <p class="p">The above diagram illustrates the HA architecture with the focus on VIP management 
        and load balancing. It only shows a subset of active-active API instances and does 
        not show examples of other services such as nova-scheduler, cinder-scheduler, etc.</p>

      <p class="p">In the above diagram, requests from an OpenStack client to the API services are 
        sent to VIP and port combination; for example, 192.0.2.26:8774 for a Nova request. 
        The load balancer listens for requests on that VIP and port. When it receives a request, 
        it selects one of the controller nodes configured for handling Nova requests, in this 
        particular case, and then forwards the request to the IP of the selected controller 
        node on the same port.</p>

      <p class="p">The nova-api service list, which is listening for requests on the IP of its host 
        machine, then receives the request and deals with it accordingly. The database service 
        is also accessed through the load balancer . RabbitMQ, on the other hand, is not 
        currently accessed through VIP/HA proxy as the clients are configured with the set 
        of nodes in the RabbitMQ cluster and failover between cluster nodes is automatically 
        handled by the clients.</p>

      <p class="p">The sections below cover the following topics in detail:</p>

      <ul class="ul">
        <li class="li"><a class="xref" href="#HP2.0HA__api_request">API Request Message Flow</a></li>

        <li class="li"><a class="xref" href="#HP2.0HA__node_failure">Handling Node Failure</a></li>

        <li class="li"><a class="xref" href="#HP2.0HA__network_partitions">Handling Network Partitions</a></li>

        <li class="li"><a class="xref" href="#HP2.0HA__galera_cluster">MySQL Galera Cluster</a></li>

      </ul>
    
      
      <div class="sectiondiv">
        <p class="p" id="HP2.0HA__api_request"><strong class="ph b">API Request Message Flow</strong></p>

        <p class="p">The diagram below shows the flow for an API request in an HA deployment. All API 
          requests (internal and external) are sent through the VIP.</p>

        <p class="p"><img class="image" id="HP2.0HA__ControlPlane2" src="../../media/ha/ControlPlaneHA2.0_2.png"></img></p>

        <p class="p">The flow of a sample API request is explained below:</p>

        <ul class="ul">
          <li class="li">keepalived has currently configured the VIP on the Controller0 node; 
            client sends Nova request to VIP:8774</li>

          <li class="li">HA proxy (listening on VIP:8774) receives the request and selects Controller0 
            from the list of available nodes (Controller0, Controller1, Controller2). 
            The request is forwarded to the Controller0IP:8774</li>

          <li class="li">nova-api on Controller0 receives the request and determines that a 
            database change is required. It connects to the database using VIP:</li>

          <li class="li">HA proxy (listening on VIP:3306) receives the database connection request 
            and selects Controller1 from the list of available nodes (Controller0, 
            Controller1, Controller2). The connection request is forwarded to 
            Controller1IP:3306</li>

        </ul>

      </div>
      
      <div class="sectiondiv">
        <p class="p" id="HP2.0HA__node_failure"><strong class="ph b">Handling Node Failure</strong></p>

        <p class="p">With the above HA set up, loss of a controller node is handled as follows:</p>

        <p class="p">Assume that the Controller0, which is currently in control of the VIP, is lost, 
          as shown in the diagram below:</p>

        <p class="p"><img class="image" id="HP2.0HA__ControlPlane3" src="../../media/ha/ControlPlaneHA2.0_3.png"></img></p>

        <p class="p">When this occurs, keepalived immediately moves the VIP on to the Controller1 
          and can now receive API requests, which is load-balanced by HA proxy, as stated 
          earlier.</p>

        <div class="note note"><span class="notetitle">Note:</span> Although MySQL and RabbitMQ clusters have lost a node, they still continue 
          to be operational:</div>

        <p class="p"><img class="image" id="HP2.0HA__ControlPlane4" src="../../media/ha/ControlPlaneHA2.0_4.png"></img></p>

        <p class="p">Finally, when Controller0 comes back online, keepalived and HA proxy will 
          resume in standby/slave mode and be ready to take over, should there be a 
          failure of Controller1. The Controller0 rejoins the MySQL and RabbitMQ clusters.</p>

      </div>
      
      <div class="sectiondiv">
        <p class="p" id="HP2.0HA__network_partitions"><strong class="ph b">Handling Network Partitions</strong></p>

        <p class="p">It is important for the HA setup to tolerate network failures, specifically those 
          that result in a partition of the cluster, whereby one of the three nodes in the 
          control plane cannot communicate with the remaining two nodes of the cluster. 
          The description of network partition handling is separated into the main HA 
          components of the controller.</p>

      </div>
      
      <div class="sectiondiv">
        <p class="p" id="HP2.0HA__galera_cluster"><strong class="ph b">MySQL Galera Cluster</strong></p>

        <p class="p">The handling of network partitions is illustrated in the diagram below. 
          Galera has a quorum mechanism so when there is a partition in the cluster, 
          the primary or quorate partition can continue to operate as normal, whereas 
          the non-primary/minority partition cannot commit any requests. In the example 
          below, Controller0 is partitioned from the rest of the control plane. As a 
          result, requests can only be satisfied on Controller1 or Controller2. 
          Controller0 will continue to attempt to rejoin the cluster:</p>

        <p class="p"><img class="image" id="HP2.0HA__ControlPlane5" src="../../media/ha/ControlPlaneHA2.0_5.png"></img></p>

        <p class="p">When HA proxy detects the errors against the mysql instance on Controller0, 
          it removes that node from its pool for future database requests.</p>

      </div>
      <div class="sectiondiv">
        <p class="p"><strong class="ph b">Singletone Services</strong></p>

        
        <div class="sectiondiv">
          <p class="p" id="HP2.0HA__cinder_volume"><strong class="ph b">Cinder-Volume</strong></p>

          <p class="p">Due to the single threading required in both cinder-volume and the drivers, 
            the Cinder volume service is run as a singleton in the control plane. 
            <img class="image" id="HP2.0HA__CinderVolume" src="../../media/ha/ha-cinder-volume.png"></img></p>

          <p class="p">Cinder-volume is deployed on all three controller nodes, but kept active on 
            only one node at a time. By default, cinder-volume is kept active on the controller. 
            If the controller fails, you must enable and start the cinder-volume service on 
            one of the other controller nodes, until it is restored. Once the controller is 
            restored, you must shut down the Cinder volume service from all other nodes and 
            start it on the controller to ensure it runs as a singleton.</p>

          <p class="p">Since cinder.conf is kept synchronized across all the 3 nodes, Cinder volume 
            can be run on any of the nodes at any given time. Ensure that it is run on only 
            one node at a time.</p>

          <p class="p">Details of how to activate Cinder Volume after controller failure is documented in
              <a class="xref" href="../blockstorage/singleton_service_cinder.html#topic_l1p_vpy_jt">Managing
              Cinder Volume and Backup Services</a>.</p>

        </div>
        <div class="sectiondiv">
          <p class="p" id="HP2.0HA__sherpa"><strong class="ph b">Sherpa</strong></p>

          <p class="p">In Helion OpenStack 2.0, Sherpa is used by the Helion Development Platform and is not
            used for Helion OpenStack Upgrade.</p>

          <p class="p">You must take periodic backups of the Sherpa service since it does 
            maintain some state information on local disk storage on the controller. 
            If the controller fails, Sherpa becomes unavailable until you rebuild or 
            restore the controller. After restoring the controller, you should restore 
            the Sherpa state from its latest backup.</p>

          <div class="note note"><span class="notetitle">Note:</span> The Sherpa backup needs to be done manually. It's not backed up as 
            part of Control Plane Backup (i.e. Freezer)</div>

        </div>
        <div class="sectiondiv">
          <p class="p" id="HP2.0HA__nova_consoleauth"><strong class="ph b">Nova consoleauth</strong></p>

          <p class="p">If the controller fails, the Nova consoleauth service will become 
            unavailable and users will not be able to connect to their VM consoles 
            via VNC. The service will be restored once you restore the controller.</p>

        </div> 
      </div>
      <div class="sectiondiv">
        <p class="p" id="HP2.0HA__failed_controller_nodes"><strong class="ph b">Rebuilding or Replacing failed Controller Nodes</strong></p>

        <p class="p">As described above, the three node controller cluster provides a robust, highly 
          available control plane of OpenStack services. Either Controller1 or Controller2 
          servers can be shut down for a short duration for maintenance activities without 
          impacting cloud service availability. (Controller0 cannot be shut down without 
          affecting cloud service availability.)</p>

        <div class="note note"><span class="notetitle">Note:</span> The HA design is only robust against single points of failure and may not 
          protect you against multiple levels of failure. As soon as first-level failure 
          occurs, you must try to fix the symptom/root cause and recover from the failure, 
          as soon as possible.</div>

        <p class="p">In the unlikely event that one of the controller servers suffers an irreparable hardware
          failure, you can decommission and delete it from the cluster. You can then deploy the
          failed controller on a new server and connect it back into the original three node
          controller cluster. Learn more about <a class="xref" href="../operations/replace_controller.html#topic_ijj_45j_nt">Replacing/Rebuilding
            Controller Nodes</a>.</p>

      </div>
    </div>

    
    
    <div class="section" id="HP2.0HA__availability_zones"><h2 class="title sectiontitle">Availability Zones</h2>
      
      <p class="p"><img class="image" id="HP2.0HA__DeploymentZones" src="../../media/ha/DeploymentZonesHA2.0_1.png"></img></p>

      <p class="p">While planning your OpenStack deployment, you should decide on how to zone 
        various types of nodes - such as compute, block storage, and object storage. 
        For example, you may decide to place all servers in the same rack in the same 
        zone. For larger deployments, you may plan more elaborate redundancy schemes 
        for redundant power, network ISP connection, and even physical firewalling 
        between zones (<em class="ph i">this aspect is outside the scope of this document</em>).</p>

        <p class="p">HPE Helion OpenStack offers APIs, CLIs and Horizon UIs for the administrator 
          to define and user to consume, availability zones for Nova, Cinder and Swift 
          services. This section outlines the process to deploy specific types of nodes 
          to specific physical servers, and makes a statement of available support for 
          these types of availability zones in the current release.</p>

        <div class="note note"><span class="notetitle">Note:</span> By default, HPE Helion OpenStack is deployed in a single availability zone upon
        installation. Multiple availability zones can be configured by an administrator
        post-install, if required. Refer to the <a class="xref" href="http://docs.openstack.org/openstack-ops/content/scaling.html" target="_blank">Chapter 5: Scaling</a> (in the OpenStack Operations Guide for more
        information).</div>

    </div>

    
    
    <div class="section" id="HP2.0HA__compute_kvm"><h2 class="title sectiontitle">Compute with KVM</h2>
      
      <p class="p">You can deploy your KVM Nova-Compute nodes either during initial installation, 
        or by adding compute nodes post initial installation.</p>

      <p class="p">While adding compute nodes post initial installation, you can specify the 
        target physical servers for deploying the compute nodes.</p>

      <p class="p">Learn more about <a class="xref" href="../operations/add_node.html">Adding Compute
          Nodes after Initial Installation</a>.</p>

    </div>

    
    <div class="section" id="HP2.0HA__nova_availability_zones"><h2 class="title sectiontitle">Nova Availability Zones</h2>
      
      <p class="p">Nova host aggregates and  Nova availability zones can be used to segregate 
        Nova compute nodes across different failure zones.</p>

    </div>

    
    <div class="section" id="HP2.0HA__compute_esx"><h2 class="title sectiontitle">Compute with ESX Hypervisor</h2>
      
      <p class="p">Compute nodes deployed on ESX Hypervisor can be made highly available using 
        the HA feature of VMware ESX Clusters. For more information on VMware HA, 
        please refer to your VMware ESX documentation.</p>

    </div>

    
    <div class="section" id="HP2.0HA__block_storage_vsa"><h2 class="title sectiontitle">Block Storage with StoreVirtual VSA</h2>
      
      <p class="p">Highly available Cinder block storage volumes are provided by the network 
        RAID 10 implementation in the HPE StoreVirtual VSA software. You can deploy 
        the VSA nodes in three node cluster and specify Network RAID 10 protection 
        for Cinder volumes.</p>

      <p class="p">The underlying SAN/iQ operating system of the StoreVirtual VSA ensures that 
        the two-way replication maintains two mirrored copies of data for each volume.</p>

      <p class="p">This Network RAID 10 capability ensures that failure of any single server 
        does not cause data loss, and maintains data access to the clients.</p>

      <p class="p">Furthermore, each of the VSA nodes of the cluster can be strategically deployed in
        different zones of your data center for maximum redundancy and resiliency. For more
        information on how to deploy VSA nodes on desired target servers, refer to the <a class="xref" href="../installation/configure_vsa.html#config_vsa">Configuring for VSA Block Storage
          Backend</a> document.</p>

      <p class="p"><img class="image" id="HP2.0HA__BlockStorage" src="../../media/ha/ha-block-storage.png"></img></p>

    </div>

    
    <div class="section" id="HP2.0HA__cinder_availability_zones"><h2 class="title sectiontitle">Cinder Availability Zones</h2>
      
      <p class="p">Cinder availability zones are not supported for general consumption 
        in the current release.</p>

    </div>

    
    <div class="section" id="HP2.0HA__object_storage_swift"><h2 class="title sectiontitle">Object Storage with Swift</h2>
      
      <p class="p">High availability in Swift is achieved at two levels.</p>

      <p class="p"><strong class="ph b">Control Plane</strong></p>

      <p class="p">The Swift API is served by multiple Swift proxy nodes. Client requests 
        are directed to all Swift proxy nodes by  the HA Proxy load balancer in 
        round-robin fashion. The HA Proxy load balancer regularly checks the node 
        is responding, so that if it fails, traffic is directed to the remaining 
        nodes. The Swift service will continue to operate and respond to client 
        requests as long as at least one Swift proxy server is running.</p>

      <p class="p">If a Swift proxy node fails in the middle of a transaction, the transaction fails. However
        it is standard practice for Swift clients to retry operations. This is transparent to
        applications that use the python-swiftclient library.</p>

      <p class="p">The entry-scale example cloud models contain three Swift proxy nodes. 
        However, it is possible to add additional clusters with additional Swift 
        proxy nodes to handle a larger workload or to provide additional resiliency.</p>

      <p class="p"><strong class="ph b">Data</strong></p>

      <p class="p">Multiple replicas of all data is stored. This happens for account, 
        container and object data. The example cloud models recommend a replica 
        count of three. However, you may change this to a higher value if needed.</p>

      <p class="p">When Swift stores different replicas of the same item on disk, it ensures 
        that as far as possible, each replica is stored in a different zone, 
        server or drive. This means that if a single server of disk drives fails, 
        there should be two copies of the item on other servers or disk drives.</p>

      <p class="p">In this release, only a single zone is supported.</p>

      <p class="p">If a disk drive is failed, Swift will continue to store three replicas. 
        The replicas that would normally be stored on the failed drive are 
        âhanded offâ to another drive on the system. When the failed drive is 
        replaced, the data on that drive is reconstructed by the replication 
        process. The replication process re-creates the âmissingâ replicas by 
        copying them to the drive using one of the other remaining replicas.  
        While this is happening, Swift can continue to store and retrieve data.</p>

      
    </div>

    
    <div class="section" id="HP2.0HA__highly_available_app_workloads"><h2 class="title sectiontitle">Highly Available Cloud Applications and Workloads</h2>
      
      
      
      
      
      <p class="p">Projects writing applications to be deployed in the cloud must be aware 
        of the cloud architecture and potential points of failure and architect 
        their applications accordingly for high availability.</p>

      <p class="p">Some guidelines for consideration:</p>

      <ol class="ol">
        <li class="li">Assume intermittent failures and plan for retries
          <ul class="ul">
            <li class="li"><strong class="ph b">OpenStack Service APIs</strong>: invocations can fail - you should 
              carefully evaluate the response of each invocation, and retry in 
              case of failures.</li>

            <li class="li"><strong class="ph b">Compute</strong>: VMs can die - monitor and restart them</li>

            <li class="li"><strong class="ph b">Network</strong>: Network calls can fail - retry should be successful</li>

            <li class="li"><strong class="ph b">Storage</strong>: Storage connection can hiccup - retry should be successful</li>

          </ul>

        </li>

        <li class="li">Build redundancy into your application tiers
          <ul class="ul">
            <li class="li">
              <ul class="ul">
                <li class="li">Replicate VMs containing stateless services such as Web application 
                  tier or Web service API tier and put them behind load balancers 
                  (you must implement your own HA Proxy type load balancer in your 
                  application VMs until HPE Helion OpenStack delivers the LBaaS service).
                </li>

                <li class="li">Boot the replicated VMs into different Nova availability zones.</li>

                <li class="li">If your VM stores state information on its local disk (Ephemeral Storage), 
                  and you cannot afford to lose it, then boot the VM off a Cinder volume.</li>

                <li class="li">Take periodic snapshots of the VM which will back it up to Swift 
                  through Glance.</li>

                <li class="li">Your data on ephemeral may get corrupted (but not your backup 
                  data in Swift and not your data on Cinder volumes).</li>

                <li class="li">Take regular snapshots of Cinder volumes and also back up 
                  Cinder volumes or your data exports into Swift.</li>

              </ul>

            </li>

          </ul>

        </li>

        <li class="li">Instead of rolling your own highly available stateful services, use 
          readily available HPE Helion OpenStack platform services such as:
          <ul class="ul">
            <li class="li">Database as a Service (DBaaS)</li>

            <li class="li">DNS as a Service(DNSaaS)</li>

            <li class="li">Messaging as a Service (MSGaaS)</li>

          </ul>

        </li>

      </ol>

     </div>

    
    <div class="section" id="HP2.0HA__what_not_ha"><h2 class="title sectiontitle">What is not Highly Available?</h2>
      
      
      <div class="sectiondiv">
        <p class="p" id="HP2.0HA__deployer"><strong class="ph b">Deployer</strong></p>

        <p class="p">The deployer in Helion Openstack is not highly-available. The deployer state/data are all
          maintained in filesystem and are backed up by the Freezer backup. In case a
          deployer/lifecycle-manager node failure, the deployer state/data can be recovered from the
          backup.</p>

      </div>
      
      <div class="sectiondiv">
        <p class="p" id="HP2.0HA__control_plane"><strong class="ph b">Control Plane</strong></p>

        <p class="p">High availability is not supported for Stateful L3 Services 
          (Source-NAT), and Network Services (LBaaS, VPNaaS, FWaaS)</p>

      </div>
    </div>

    
    <div class="section" id="HP2.0HA__more_information"><h2 class="title sectiontitle">More Information</h2>
      
      <ul class="ul">
        <li class="li"><a class="xref" href="http://docs.openstack.org/high-availability-guide/content/ch-intro.html" target="_blank">OpenStack High-availability Guide</a></li>

        <li class="li"><a class="xref" href="http://12factor.net/" target="_blank">12-Factor Apps</a></li>

      </ul>

    </div>

  </div>

<div class="navfooter"><!----></div><div class="footer">WebHelp output generated by<a class="oxyFooter" href="http://www.oxygenxml.com" target="_blank"><span class="oXygenLogo"><img src="../../oxygen-webhelp/resources/img/LogoOxygen100x22.png" alt="Oxygen"></img></span><span class="xmlauthor">XML Author</span></a></div>
</body>
</html>