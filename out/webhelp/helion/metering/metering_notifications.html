
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xml:lang="en-us" lang="en-us">
<head><meta name="description" content="In HPE Helion OpenStack 2.0 we have adopted a strategy to reduce the amount of data that is sent to storage. The main reason is that we are currently using a SQL-based cluster, which is not highly ..."></meta><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><meta name="DC.Type" content="topic"></meta><meta name="DC.Title" content="HPE Helion OpenStack 2.0: Ceilometer Metering Service Notifications"></meta><meta name="prodname" content="HPE Helion"></meta><meta name="version" content="4.1.0"></meta><meta name="copyright" content="HPE Helion 2015" type="primary"></meta><meta name="DC.Rights.Owner" content="HPE Helion 2015" type="primary"></meta><meta name="DC.Format" content="XHTML"></meta><meta name="DC.Identifier" content="notifications"></meta><link rel="stylesheet" type="text/css" href="../../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>HPE Helion OpenStack 2.0: Ceilometer Metering Service Notifications</title><!--  Generated with Oxygen version 17.0, build number 2015072912.  --><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><link rel="stylesheet" type="text/css" href="../../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../../oxygen-webhelp/resources/skins/skin.css"><!----></link><script type="text/javascript"><!--
          
          var prefix = "../../index.html";
          
          --></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery-1.8.2.min.js"><!----></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script></head>
<body onload="highlightSearchTerm()" class="frmBody" id="notifications">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td width="75%"></td><td><div class="navheader"></div></td></tr></tbody></table>

  <h1 class="title topictitle1">HPE Helion OpenStack<sup>Â®</sup> 2.0: Ceilometer Metering Service Notifications</h1>

  <div class="body">
   
   <div class="section"> In HPE Helion OpenStack 2.0 we have adopted a strategy to reduce the amount of data that
      is sent to storage. The main reason is that we are currently using a SQL-based cluster, which
      is not highly indicated for big data. <p class="p">You can control the data Ceilometer collects using
        the pipeline configuration files. These configuration files are located in the <strong class="ph b">/etc</strong>
        folder of the respective components on all of the controller nodes. Ceilometer Central Agent
        and Ceilometer Notification Agent use different <strong class="ph b">pipeline.yml </strong>files to configure
        meters that are fetched via polling and using notifications. The files were created this way
        to prevent accidently polling for meters that can be retrieved by both the polling agent and
        the notification agent. For exampe, Glance image and Glance image.size are both meters that
        are of type pollster AND type notification.</p>
 For example, for notification white listing,
      you have to specify pipeline configuration for the Ceilometer Notification Agent in the
      pipeline configuration file called <strong class="ph b">pipeline-agent-notification.yml</strong> which is located in
        <strong class="ph b">/opt/stack/service/ceilometer-agent-notification/etc/</strong>. <p class="p">For polling white-listing,
        you must specify pipeline configuration for Ceilometer Central Agent in the pipeline
        configuration file called <strong class="ph b">pipeline-agent-central.yml</strong> which is located in<strong class="ph b">
          /opt/stack/service/ceilometer-agent-central/etc</strong>. </p>
The pipeline configuration file
      on all the controller nodes must be changed in order to change the white-listing or polling
      strategy. You should run <strong class="ph b">ceilometer-reconfigure.yml</strong> to make those changes stick. <p class="p">As
        there are references to the version of the components installed in the respective
        environment, those need to be verified appropriately. </p>
The pipeline configuration file is
      comprised of two major elements: Sources and Sinks. Sources represent the data that is
      harvested either from notifications posted by services or collected through polling. Sinks
      represent how the data is modified before it is published to the internal queue for collection
      and storage. <p class="p">In the Sources section there is a list of meters that represent the data that
        is going to be collected. For a full list please refer to the Ceilometer documentation
        available at <a class="xref" href="http://docs.openstack.org/admin-guide-cloud/telemetry-measurements.html" target="_blank">http://docs.openstack.org/admin-guide-cloud/telemetry-measurements.html</a>
      </p>
The following is an example of the default pipeline configuration for Central Agent
        <strong class="ph b">pipeline-agent-central.yml
      </strong><pre class="pre codeblock">---
sources:
    - name: swift_source
      interval: 3600
      meters:
          - "storage.objects"
          - "storage.objects.size"
          - "storage.objects.containers"
      resources:
      discovery:
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
         - notifier://</pre>

      The following is an example of the default pipeline configuration for Agent Notification:
      <pre class="pre codeblock">---
sources:
    - name: meter_source
      interval: 30
      meters:
          - "instance"
          - "ip.floating"
          - "network"
          - "network.create"
          - "network.update"
      sinks:
          - meter_sink
    - name: image_source
      interval: 30
      meters:
          - "image"
          - "image.size"
          - "image.upload"
          - "image.delete"
      sinks:
          - meter_sink
    - name: volume_source
      interval: 30
      meters:
          - "volume"
          - "volume.size"
          - "snapshot"
          - "snapshot.size"
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
         - notifier://</pre>

      The interval attribute dictates the frequency of the data polling whenever the meter can be
      polled. In general the meters that are available as notification and polling (indicated as
      both in <a class="xref" href="http://docs.openstack.org/admin-guide-cloud/telemetry-measurements.html" target="_blank">http://docs.openstack.org/admin-guide-cloud/telemetry-measurements.html</a>) are going
      to be polled at the specified interval. This combination of meters and polling interval is the
      most performant compromise that we found using the SQL cluster back-end. In our setting, since
      we limited the polling meters to Swift only, the interval is set to 3600 (1 hour expressed in
      seconds). In our setting, since we want to rely on notifications rather than polling, we have
      limited the meters to be polled to be only Swift. </div>

   
    <div class="section" id="notifications__list"><h2 class="title sectiontitle">Editing the List of Meters</h2> The list of meters can be easily
      reduced or increased by editing the pipeline configuration of the respective components (which
      are notification or central/polling agent) and subsequently restarting the respectrive agent.
      If pollsters are modified then the Central Agent requires a restart and if notifications are
      added, then the Notification Agent requires a restart. Also, Collector needs to be restarted.
      Here it is an example of compute only <strong class="ph b">pipeline.yml </strong>with the daily polling interval:
      <pre class="pre codeblock">---
sources:
    - name: meter_source
      interval: 86400
      meters:
          - "instance"
          - "memory"
          - "vcpus"
          - "compute.instance.create.end"
          - "compute.instance.delete.end"
          - "compute.instance.update"
          - "compute.instance.exists"
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
          - notifier://</pre>

      If changes are made to this configuration to enable meters at container level, every time the
      polling interval is due for polling at least 5 messages per existing object/container in Swift
      are collected. The following table illustrate the amount of data will be produce hourly in
      different scenarios: Swift Containers Swift Objects per container Number of Samples/hr Samples
      Stored in a 24H Period 
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" class="table" frame="border" border="1" rules="all">
          <tbody class="tbody">
            <tr class="row">
              <td class="entry" valign="top">Swift Containers</td>

              <td class="entry" valign="top">Swift Objects per container</td>

              <td class="entry" valign="top">Samples per Hour</td>

              <td class="entry" valign="top">Samples stored per 24 hours</td>

            </tr>

            <tr class="row">
              <td class="entry" valign="top">10</td>

              <td class="entry" valign="top">10</td>

              <td class="entry" valign="top">500</td>

              <td class="entry" valign="top">12000</td>

            </tr>

            <tr class="row">
              <td class="entry" valign="top">10</td>

              <td class="entry" valign="top">100</td>

              <td class="entry" valign="top">5000</td>

              <td class="entry" valign="top">120000</td>

            </tr>

            <tr class="row">
              <td class="entry" valign="top">100</td>

              <td class="entry" valign="top">100</td>

              <td class="entry" valign="top">50000</td>

              <td class="entry" valign="top">1200000</td>

            </tr>

            <tr class="row">
              <td class="entry" valign="top">100</td>

              <td class="entry" valign="top">1000</td>

              <td class="entry" valign="top">500000</td>

              <td class="entry" valign="top">12000000</td>

            </tr>

          </tbody>

        </table>
</div>
 This means that even a very small Swift storage with 10 containers and 100 files will
      store 120,000 samples in 24 hours, generating a grand total of 3.6 million samples! <p class="p">Note
        that the file size of each file does not have any impact on the number of samples collected.
        As shown above, the smallest number of samples results from polling when there are a small
        number of files and a small number of containers. When  there a lot of small files and
        containers,  the number of samples is the highest. </p>
</div>

    
    <div class="section"><h2 class="title sectiontitle">Updating the Polling Strategy and Swift Considerations</h2>
      <p class="p">Polling can be very taxing on the system due to the sheer volume of data that the system
        may have to process. It also has a severe impact on queries since the database will now have
        a very large amount of data to scan to respond to the query. This consumes a great amount of
        cpu and memory. This can result in long wait times for query responses, and in extreme cases
        can result in timeouts.</p>
 There are 3 polling meters in Swift: <ul class="ul">
        <li class="li">storage.objects </li>

        <li class="li">storage.objects.size </li>

        <li class="li">storage.objects.containers</li>

      </ul>
 Here is an example of <strong class="ph b">pipeline.yml </strong>in which Swift polling is set to occur hourly.
        <pre class="pre codeblock">---
      sources:
      - name: swift_source
      interval: 3600
      meters:
      - "storage.objects"
      - "storage.objects.size"
      - "storage.objects.containers"
      resources:
      discovery:
      sinks:
      - meter_sink
      sinks:
      - name: meter_sink
      transformers:
      publishers:
      - notifier://</pre>
<p class="p">With
        this configuration above, we did not enable polling of container based meters and we only
        collect 3 messages for any given tenant, one for each meter listed in the configuration
        files. Since we have 3 messages only per tenant, it doesn't create a heavy load on the MySQL
        database as it would have if container-based meters were enabled. Hence, other APIs doesn't
        get hit because of this data collection configuration. </p>
</div>

   
   
   
  </div>

<div class="navfooter"><!----></div><div class="footer">WebHelp output generated by<a class="oxyFooter" href="http://www.oxygenxml.com" target="_blank"><span class="oXygenLogo"><img src="../../oxygen-webhelp/resources/img/LogoOxygen100x22.png" alt="Oxygen"></img></span><span class="xmlauthor">XML Author</span></a></div>
</body>
</html>