
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xml:lang="en-us" lang="en-us">
<head><meta name="description" content="This page describes troubleshooting scenarios for Ceph. Issue 1: If no Ceph monitor nodes are defined for the cloud, then Ceph cloud deployment fails with the following error while executing the ..."></meta><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><meta name="DC.Type" content="topic"></meta><meta name="DC.Title" content="HPE Helion OpenStack 2.0: Ceph Service: Troubleshooting"></meta><meta name="prodname" content="HPE Helion"></meta><meta name="version" content="4.1.0"></meta><meta name="copyright" content="HPE Helion 2015" type="primary"></meta><meta name="DC.Rights.Owner" content="HPE Helion 2015" type="primary"></meta><meta name="DC.Format" content="XHTML"></meta><meta name="DC.Identifier" content="topic_mnk_sq1_5t"></meta><link rel="stylesheet" type="text/css" href="../../../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>HPE Helion OpenStack 2.0: Ceph Service: Troubleshooting</title><!--  Generated with Oxygen version 17.0, build number 2015072912.  --><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><link rel="stylesheet" type="text/css" href="../../../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../../../oxygen-webhelp/resources/skins/skin.css"><!----></link><script type="text/javascript"><!--
          
          var prefix = "../../../index.html";
          
          --></script><script type="text/javascript" src="../../../oxygen-webhelp/resources/js/jquery-1.8.2.min.js"><!----></script><script type="text/javascript" src="../../../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../../../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../../../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../../../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script></head>
<body onload="highlightSearchTerm()" class="frmBody" id="topic_mnk_sq1_5t">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td width="75%"></td><td><div class="navheader"></div></td></tr></tbody></table>

  <h1 class="title topictitle1">HPE Helion OpenStack<sup>®</sup> 2.0: Ceph Service: Troubleshooting</h1>

  <div class="body">
    <p class="p">This page describes troubleshooting scenarios for Ceph.</p>

    <p class="p"><strong class="ph b">Issue 1:</strong></p>

    <p class="p">If no Ceph monitor nodes are defined for the cloud, then Ceph cloud deployment fails with the
      following error while executing the <samp class="ph codeph">site.yml</samp> playbook:</p>

    <pre class="pre codeblock">TASK: [_CEP-CMN | install | Install ceph] ************************************* 
        changed: [stratushelion-cp1-ceph0001-mgmt]
        changed: [stratushelion-cp1-ceph0002-mgmt]
        changed: [stratushelion-cp1-ceph0005-mgmt]
        changed: [stratushelion-cp1-ceph0003-mgmt]
        changed: [stratushelion-cp1-ceph0004-mgmt]

        TASK: [_CEP-CMN | configure | Copy "{{ deployer_ceph_dir }}/ceph.client.admin.keyring" to /etc/ceph directory] *** 
        fatal: [stratushelion-cp1-ceph0001-mgmt] =&gt; input file not found at /etc/ceph/ceph.client.admin.keyring or /etc/ceph/ceph.client.admin.keyring
        fatal: [stratushelion-cp1-ceph0002-mgmt] =&gt; input file not found at /etc/ceph/ceph.client.admin.keyring or /etc/ceph/ceph.client.admin.keyring
        fatal: [stratushelion-cp1-ceph0003-mgmt] =&gt; input file not found at /etc/ceph/ceph.client.admin.keyring or /etc/ceph/ceph.client.admin.keyring
        fatal: [stratushelion-cp1-ceph0004-mgmt] =&gt; input file not found at /etc/ceph/ceph.client.admin.keyring or /etc/ceph/ceph.client.admin.keyring
        fatal: [stratushelion-cp1-ceph0005-mgmt] =&gt; input file not found at /etc/ceph/ceph.client.admin.keyring or /etc/ceph/ceph.client.admin.keyring

        FATAL: all hosts have already failed -- aborting</pre>

    <p class="p"><strong class="ph b">Resolution:</strong></p>

    <div class="p">Compare the cloud control plane defined in the
        <samp class="ph codeph">~/helion/my_cloud/definition/data/control_plane.yml</samp> file with the example
      one. Ensure that either: <ol class="ol" id="topic_mnk_sq1_5t__ol_ir1_2s4_5t">
        <li class="li">The service component includes <samp class="ph codeph">- ceph-monitor</samp> for <samp class="ph codeph">server-role:
            CONTROLLER-ROLE</samp>
          <p class="p">OR</p>
</li>

        <li class="li">The resource nodes for Ceph Monitor are added to the
            <samp class="ph codeph">~/helion/my_cloud/definition/data/control_plane.yml</samp> file, which have
          the service-components (<samp class="ph codeph">- ntp-client</samp> and <samp class="ph codeph">- ceph-monitor</samp>)
          defined.</li>

      </ol>
</div>

    <p class="p"><strong class="ph b">Issue 2: </strong></p>

    <p class="p">If the disk presented as OSD data and/or a journal disk has some pre-existing partitions,
      then the Ceph cloud deployment fails with the following error while executing
        <samp class="ph codeph">site.yml</samp> playbook:</p>

    <div class="p">
      <pre class="pre codeblock">TASK: [CEP-OSD | configure | Configure the osds of {{ inventory_hostname }}] *** 
        failed: [stratushelion-cp1-ceph0001-mgmt] =&gt; (item={'key': '/dev/sdd', 'value': '/dev/sdc'}) =&gt; {"failed": true, "item": {"key": "/dev/sdd", "value": "/dev/sdc"}}
        msg: Exception: 'ceph-disk -v prepare --cluster-uuid 2645bbf6-16d0-4c42-8835-8ba9f5c95a1d --cluster ceph --osd-uuid 41cfdfe8-1c97-4c8c-9561-5ced0e88ebfd --fs-type xfs --zap-disk /dev/sdd /dev/sdc' failed with error DEBUG:ceph-disk:Zapping partition table on /dev/sdd
        INFO:ceph-disk:Running command: /sbin/sgdisk --zap-all -- /dev/sdd
        Caution: invalid backup GPT header, but valid main header; regenerating
        backup header from main header.

        INFO:ceph-disk:Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/sdd
        DEBUG:ceph-disk:Calling partprobe on zapped device /dev/sdd
        INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
        INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
        INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
        INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
        INFO:ceph-disk:Running command: /sbin/parted --machine -- /dev/sdc print
        WARNING:ceph-disk:OSD will not be hot-swappable if journal is not the same device as the osd data
        DEBUG:ceph-disk:Creating journal partition num 4 size 5120 on /dev/sdc
        INFO:ceph-disk:Running command: /sbin/sgdisk --new=4:0:+5120M --change-name=4:ceph journal --partition-guid=4:26d3ce3e-5843-4655-ba3c-26d9383b6fc0 --typecode=4:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdc
        Could not create partition 4 from 0 to 10485759
        Unable to set partition 4's name to 'ceph journal'!
        Could not change partition 4's type code to 45b0969e-9b03-4f30-b4c6-b4b80ceff106!
        Error encountered; not saving changes.
        ceph-disk: Error: Command '['/sbin/sgdisk', '--new=4:0:+5120M', '--change-name=4:ceph journal', '--partition-guid=4:26d3ce3e-5843-4655-ba3c-26d9383b6fc0', '--typecode=4:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sdc']' returned non-zero exit status 4
</pre>

    </div>

    <p class="p"><strong class="ph b">Resolution:</strong></p>

    <div class="p">You must manually delete any partitions on the (failed) disk presented as OSD data and/or
      journal disk and re-execute the <samp class="ph codeph">site.yml</samp> playbook to resume the cloud
      deployment.<pre class="pre codeblock">cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml </pre>
</div>

    <p class="p"><strong class="ph b">Issue 3:</strong></p>

    <p class="p">If on a entry-scale-kvm-ceph cloud, the controller nodes is not always a part of the
      actionable nodes then executing the <samp class="ph codeph">site.yml</samp> playbook for a particular node
      (using --limit &lt;node&gt;) fails with the following error:</p>

    <pre class="pre codeblock">TASK: [_CEP-CMN | check_network | Set cluster_network_enabled to True if available] *** 
        skipping: [localhost]

        TASK: [ceph-deployer | configure | Generate "/etc/ceph/{{ ceph_cluster }}.conf" file] *** 
        fatal: [localhost] =&gt; {'msg': "AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'host'", 'failed': True}
        fatal: [localhost] =&gt; {'msg': "AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'host'", 'failed': True}

        FATAL: all hosts have already failed -- aborting  
        </pre>

    <p class="p"><strong class="ph b">Resolution</strong>:</p>

    <div class="p">Edit the limit file or limit string specified while executing the <samp class="ph codeph">site.yml</samp>
      playbook and include the hostnames of all the <strong class="ph b">Ceph monitor nodes</strong> in the cloud. You can
      execute the <samp class="ph codeph">site.yml</samp> option in either of the following ways:<ol class="ol" id="topic_mnk_sq1_5t__ol_jxh_bfq_5t">
        <li class="li"><samp class="ph codeph">ansible-playbook -i hosts/verb_hosts site.yml –-limit new-compute</samp><div class="p">You
            must add monitor nodes in the above
            command:<pre class="pre codeblock">ansible-playbook -i hosts/verb_hosts site.yml –-limit new-compute,monitor-1,monitor-2,monitor-3</pre>
</div>
</li>

        <li class="li"><samp class="ph codeph">ansible-playbook -i hosts/verb_hosts site.yml –limit
            @new_nodes.txt</samp><div class="p">You can add the monitor hosts to the text file (named
              <samp class="ph codeph">new_nodes.txt</samp>), which specifies the new compute host (along with
            monitor hosts) as follows:<ul class="ul" id="topic_mnk_sq1_5t__ul_ak5_fgq_5t">
              <li class="li">new-compute</li>

              <li class="li">monitor-1</li>

              <li class="li">monitor-2</li>

              <li class="li">monitor-3</li>

            </ul>
</div>
</li>

      </ol>
</div>

    <p class="p"><strong class="ph b">Issue 4:</strong></p>

    <div class="p">If the time required for placement group creation is slower than usual (&gt;50 secs for PGs to
      get created), the <samp class="ph codeph">ceph-client-prepare.yml</samp> playbook fails with the following
      error while creating placement
      groups:<pre class="pre codeblock">Symptom:
    Following error traces seen during site.yml playbook execution:
        TASK: [ceph-client-prepare | prepare-cluster-user | Set pool {{ item.1.name }} pgp_num to {{ item.1.attrs.pg }}] ***
        changed: [localhost] =&gt; (item=({'user': {'type': 'openstack', 'name': 'cinder', 'secret_id': '457eb676-33da-42ec-9a8c-9293d545c337'}}, {'usage': {'purpose': 'cinder-volume'}, 'name': 'volumes', 'attrs': {'type': 'replicated', 'creation_policy': 'eager', 'replica_size': 3, 'pg': 100, 'permission': 'rwx'}}))
        changed: [localhost] =&gt; (item=({'user': {'type': 'openstack', 'name': 'cinder', 'secret_id': '457eb676-33da-42ec-9a8c-9293d545c337'}}, {'usage': {'purpose': 'nova'}, 'name': 'vms', 'attrs': {'creation_policy': 'eager', 'type': 'replicated', 'pg': 100, 'permission': 'rwx'}}))
        skipping: [localhost] =&gt; (item=({'user': {'type': 'openstack', 'name': 'cinder', 'secret_id': '457eb676-33da-42ec-9a8c-9293d545c337'}}, {'usage': {'purpose': 'glance-datastore'}, 'name': 'images', 'attrs': {'type': 'erasure', 'creation_policy': 'lazy', 'replica_size': 2, 'pg': 128, 'permission': 'rwx'}}))
        failed: [localhost] =&gt; (item=({'user': {'type': 'openstack', 'name': 'glance'}}, {'usage': {'purpose': 'glance-datastore'}, 'name': 'images', 'attrs': {'creation_policy': 'eager', 'pg': 128, 'permission': 'rwx'}})) =&gt; {"changed": true, "cmd": "ceph --cluster bvceph3 osd pool set images pgp_num 128", "delta": "0:00:00.456878", "end": "2015-10-19 12:08:37.379630", "item": [{"user": {"name": "glance", "type": "openstack"}}, {"attrs": {"creation_policy": "eager", "permission": "rwx", "pg": 128}, "name": "images", "usage": {"purpose": "glance-datastore"}}], "rc": 16, "start": "2015-10-19 12:08:36.922752", "warnings": []}
        stderr: Error EBUSY: currently creating pgs, wait
        failed: [localhost] =&gt; (item=({'user': {'type': 'openstack', 'name': 'cinder-backup'}}, {'usage': {'purpose': 'cinder-backup'}, 'name': 'backups', 'attrs': {'type': 'replicated', 'creation_policy': 'eager', 'replica_size': 3, 'pg': 128, 'permission': 'rwx'}})) =&gt; {"changed": true, "cmd": "ceph --cluster bvceph3 osd pool set backups pgp_num 128", "delta": "0:00:00.242184", "end": "2015-10-19 12:08:37.824764", "item": [{"user": {"name": "cinder-backup", "type": "openstack"}}, {"attrs": {"creation_policy": "eager", "permission": "rwx", "pg": 128, "replica_size": 3, "type": "replicated"}, "name": "backups", "usage": {"purpose": "cinder-backup"}}], "rc": 16, "start": "2015-10-19 12:08:37.582580", "warnings": []}
        stderr: Error EBUSY: currently creating pgs, wait

        FATAL: all hosts have already failed -- aborting</pre>
</div>

    <p class="p"><strong class="ph b">Resolution</strong>:</p>

    <p class="p">Pause for a minute and check the status of <samp class="ph codeph">ceph --cluster &lt;ceph-cluster&gt; -s |grep
        creating</samp>. If no results are returned by this command then re-execute the
        <samp class="ph codeph">ceph-client-prepare.yml</samp> playbook.</p>

  </div>

<div class="navfooter"><!----></div><div class="footer">WebHelp output generated by<a class="oxyFooter" href="http://www.oxygenxml.com" target="_blank"><span class="oXygenLogo"><img src="../../../oxygen-webhelp/resources/img/LogoOxygen100x22.png" alt="Oxygen"></img></span><span class="xmlauthor">XML Author</span></a></div>
</body>
</html>