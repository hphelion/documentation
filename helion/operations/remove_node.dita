<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_fld_zzy_lt">
  <title>HPE Helion <tm tmtype="reg">OpenStack</tm> 2.0: Removing a Compute Node</title>
  <body>
    <!--Needs Work-->
    <section id="removeNodes"> The process for removing a compute node requires that you run these
      steps. Note that to run any playbooks whatsoever for cloud maintenance, you will always run
      from the lifecycle manager.<ol>
        <li>Disable provisioning to prevent new instances from being created on this node. </li>
        <li>If availability zones are configured, remove the Compute host from its availability
          zone.</li>
        <li>If possible, use live migration to move any instances on the node to other nodes. </li>
        <li>Shut down the node or stop the nova-compute service. </li>
        <li>Delete the compute node from nova </li>
        <li>Amend servers.yml in the cloud configuration to remove entry for this node, commit the
          change and re run the configuration processor </li>
        <li>Remove the node from cobbler</li>
        <li>Remove the node from monitoring</li>
      </ol>
    </section>

    <section>
      <p><b>Disable Provisioning on the Compute Host</b></p>
      <ol>
        <li>Get a list of the Nova services running which will provide us with the details we need
          to disable the provisionong on the Compute host you are wanting to remove: <codeblock>nova service-list</codeblock>
          <p>Here is an example below. I've highlighted the Compute node we are going to remove in
            the examples:</p>
          <codeblock>$ nova service-list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 10 | nova-scheduler   | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:34.000000 | -               |
| 13 | nova-conductor   | helion-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 16 | nova-conductor   | helion-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 25 | nova-consoleauth | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -               |
| 28 | nova-scheduler   | helion-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -               |
| 31 | nova-scheduler   | helion-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:42.000000 | -               |
| 34 | nova-compute     | helion-cp1-comp0001-mgmt | AZ1      | enabled | up    | 2015-11-22T22:50:35.000000 | -               |
<b>| 37 | nova-compute     | helion-cp1-comp0002-mgmt | AZ2      | enabled | up    | 2015-11-22T22:50:44.000000 | -               |</b>
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+</codeblock></li>
        <li>Disable the Nova service on the Compute node you are wanting to remove which will ensure
          it is taken out of the scheduling rotation: <codeblock>nova service-disable --reason "&lt;enter reason here>" &lt;node hostname> nova-compute</codeblock>
          <p>Here is an example if I wanted to remove the <codeph>helion-cp1-comp0002-mgmt</codeph>
            in the output above:</p>
          <codeblock>$ nova service-disable --reason "hardware reallocation" helion-cp1-comp0002-mgmt nova-compute
+--------------------------+--------------+----------+-----------------------+
| Host                     | Binary       | Status   | Disabled Reason       |
+--------------------------+--------------+----------+-----------------------+
| helion-cp1-comp0002-mgmt | nova-compute | disabled | hardware reallocation |
+--------------------------+--------------+----------+-----------------------+</codeblock></li>
      </ol>
      <p><b>Remove the Compute Host from it's Availability Zone</b></p>
      <p>If you configured the Compute host to be part of an availability zone, these steps will
        show you how to remove it.</p>
      <ol>
        <li>Get a list of the Nova services running which will provide us with the details we need
          to remove a Compute node: <codeblock>nova service-list</codeblock>
          <p>Here is an example below. I've highlighted the Compute node we are going to remove in
            the examples:</p>
          <codeblock>$ nova service-list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 10 | nova-scheduler   | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:34.000000 | -                     |
| 13 | nova-conductor   | helion-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 16 | nova-conductor   | helion-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 25 | nova-consoleauth | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -                     |
| 28 | nova-scheduler   | helion-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -                     |
| 31 | nova-scheduler   | helion-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:42.000000 | -                     |
| 34 | nova-compute     | helion-cp1-comp0001-mgmt | AZ1      | enabled | up    | 2015-11-22T22:50:35.000000 | -                     |
<b>| 37 | nova-compute     | helion-cp1-comp0002-mgmt | AZ2      | enabled | up    | 2015-11-22T22:50:44.000000 | hardware reallocation |</b>
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+</codeblock></li>
        <li>You can remove the Compute host from the availability zone it was a part of with this
          command: <codeblock>nova aggregate-remove-host &lt;availability zone> &lt;nova hostname></codeblock>
          <p>So for the same example as the previous step, the
              <codeph>helion-cp1-comp0002-mgmt</codeph> host was in the <codeph>AZ2</codeph>
            availability zone so I would use this command to remove it:</p>
          <codeblock>$ nova aggregate-remove-host AZ2 helion-cp1-comp0002-mgmt
Host helion-cp1-comp0002-mgmt has been successfully removed from aggregate 4
+----+------+-------------------+-------+-------------------------+
| Id | Name | Availability Zone | Hosts | Metadata                |
+----+------+-------------------+-------+-------------------------+
| 4  | AZ2  | AZ2               |       | 'availability_zone=AZ2' |
+----+------+-------------------+-------+-------------------------+</codeblock></li>
        <li>You can confirm the last two steps completed successfully by running another
            <codeph>nova service-list</codeph>. <p>Here is an example which confirms that the node
            has been disabled and that it has been removed from the availability zone. I have
            highlighted these:</p>
          <codeblock>$ nova service-list
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | helion-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 10 | nova-scheduler   | helion-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:34.000000 | -                     |
| 13 | nova-conductor   | helion-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 16 | nova-conductor   | helion-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 25 | nova-consoleauth | helion-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 28 | nova-scheduler   | helion-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 31 | nova-scheduler   | helion-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:32.000000 | -                     |
| 34 | nova-compute     | helion-cp1-comp0001-mgmt | AZ1      | enabled  | up    | 2015-11-22T23:04:25.000000 | -                     |
<b>| 37 | nova-compute     | helion-cp1-comp0002-mgmt | nova     | disabled | up    | 2015-11-22T23:04:34.000000 | hardware reallocation |</b>
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+</codeblock></li>
      </ol>
      <p><b>Use Live Migration to Move Any Instances on This Host to Other Hosts</b></p>
      <ol>
        <li>You will need to verify if the Compute node is currently hosting any instances on it.
          You can do this with the command below: <codeblock>nova list --host=&lt;nova hostname> --all_tenants=1</codeblock>
          <p>Here is an example below which shows that we have a single running instance on this
            node currently:</p>
          <codeblock>$ nova list --host=helion-cp1-comp0002-mgmt --all_tenants=1
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+
| ID                                   | Name   | Tenant ID                        | Status | Task State | Power State | Networks        |
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+
| 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9 | paul4d | 5e9998f1b1824ea9a3b06ad142f09ca5 | ACTIVE | -          | Running     | paul=10.10.10.7 |
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+</codeblock></li>
        <li>You will likely want to migrate this instance off of this node before removing it. You
          can do this with the live migration functionality within Nova. The command will look like
          this: <codeblock>nova live-migration --block-migrate &lt;nova instance ID></codeblock>
          <p>Here is an example using the instance in the previous step:</p>
          <codeblock>$ nova live-migration --block-migrate 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9</codeblock>
          <p>You can check the status of the migration using the same command from the previous
            step:</p>
          <codeblock>$ nova list --host=helion-cp1-comp0002-mgmt --all_tenants=1
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+
| ID                                   | Name   | Tenant ID                        | Status    | Task State | Power State | Networks        |
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+
| 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9 | paul4d | 5e9998f1b1824ea9a3b06ad142f09ca5 | MIGRATING | migrating  | Running     | paul=10.10.10.7 |
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+</codeblock>
        </li>
        <li> Run nova list again
          <codeblock>$ nova list --host=helion-cp1-comp0002-mgmt --all_tenants=1</codeblock> to see
          that the running instance has been
          migrated:<codeblock>+----+------+-----------+--------+------------+-------------+----------+
| ID | Name | Tenant ID | Status | Task State | Power State | Networks |
+----+------+-----------+--------+------------+-------------+----------+
+----+------+-----------+--------+------------+-------------+----------+</codeblock>
        </li>
      </ol>
      <p><b>Shutdown or Stop the Nova Service on the Compute Host</b></p>
      <p>To do this step you have a few options. You can SSH to the Compute host and run the
        following commands:</p>
      <codeblock>sudo systemctl stop nova-compute
    sudo shutdown now</codeblock>
      <p>OR</p>
      <p>From the lifecycle manager you can use the <codeph>bm-power-down.yml</codeph> playbook to
        shut the node down:</p>
      <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-power-down.yml -e nodelist=&#60;node name></codeblock>
      <note>The <codeph>&#60;node name></codeph> value will be the value corresponding to this node
        in Cobbler. You can run <codeph>sudo cobbler system list</codeph> to retrieve these
        names.</note>
      <p><b>Delete The Compute Host from Nova</b></p>
      <p>Retrieve the list of Nova services:</p>
      <codeblock>nova service-list</codeblock>
      <p>Here is an example highlighting the Compute host we're going to remove:</p>
      <codeblock>$ nova service-list
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | helion-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 10 | nova-scheduler   | helion-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:34.000000 | -                     |
| 13 | nova-conductor   | helion-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 16 | nova-conductor   | helion-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 25 | nova-consoleauth | helion-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 28 | nova-scheduler   | helion-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 31 | nova-scheduler   | helion-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:32.000000 | -                     |
| 34 | nova-compute     | helion-cp1-comp0001-mgmt | AZ1      | enabled  | up    | 2015-11-22T23:04:25.000000 | -                     |
<b>| 37 | nova-compute     | helion-cp1-comp0002-mgmt | nova     | disabled | up    | 2015-11-22T23:04:34.000000 | hardware reallocation |</b>
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+</codeblock>
      <p>Delete the host from Nova using the command below:</p>
      <codeblock>nova service-delete &lt;service ID></codeblock>
      <p>Following our example above, you would use:</p>
      <codeblock>nova service-delete 37</codeblock>
      <p>Use the command below to confirm that the Compute host has been completely removed from Nova:</p>
      <codeblock>nova hypervisor-list</codeblock>
      
      <p><b>Remove the Compute Host from the <codeph>servers.yml</codeph> File and Run the Configuration Processor</b></p>
      <p>Complete these steps from the lifecycle manager to remove the Compute node:</p>
      <ol>
        <li>Log in to the lifecycle manager</li>
        <li>Edit your <codeph>servers.yml</codeph> file in the location below to remove references to the Compute node(s) you want to remove:
        <codeblock>~/helion/my_cloud/definition/data/servers.yml</codeblock></li>
        <li>You may also need to edit your <codeph>control_plane.yml</codeph> file to update the values for <codeph>member-count</codeph>, <codeph>min-count</codeph>, and
          <codeph>max-count</codeph> if you used those to ensure they reflect the proper number of nodes you are using.
        <p>See <xref
          href="../input_model.dita#input_model/co_controlplane">Input Model - Control
          Plane</xref> for more details.</p></li>
        <li>Commit the changes to git:
          <codeblock>git commit -a -m "Remove node &lt;name>"</codeblock></li>
        <li>Run the configuration processor:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
          <p>You may want to use the <codeph>remove_deleted_servers</codeph> and <codeph>free_unused_addresses</codeph> switches to free up the resources when running the configuration processor.  See <xref href="../input_model.dita#input_model/persisteddata">Persisted Data</xref> for more details.</p>
          <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</codeblock></li>
        <li>Run the ready deployment playbook:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
        </li>
      </ol>
      
      <p><b>Remove the Compute Host from Cobbler</b></p>
      <p>Complete these steps to remove the node from Cobbler:</p>
      <ol>
        <li>Confirm the system name in Cobbler with this command:
        <codeblock>sudo cobbler system list</codeblock></li>
        <li>Remove the system from Cobbler using this command:
          <codeblock>sudo cobbler system remove --name=&lt;node></codeblock></li>
        <li>Run the <codeph>cobbler-deploy.yml</codeph> playbook to complete the process:
        <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
      </ol>
      
      <p><b>Removing the Compute Host from Monitoring</b></p>
         <p>Once you have removed the Compute nodes, the alarms against them will trigger so there are
        additional steps to take to resolve this issue.</p>
      <p>You will want to SSH to each of the Monasca API servers and edit the
          <codeph>/etc/monasca/agent/conf.d/host_alive.yaml</codeph> file to remove references to
        the Compute node you removed. This will require <codeph>sudo</codeph> access. The entries
        will look similar to the one below:</p>
      <codeblock>
- alive_test: ping
  built_by: HostAlive
  host_name: helion-cp1-comp0001-mgmt
  name: helion-cp1-comp0001-mgmt ping</codeblock>
      <p>Once you have removed the references on each of your Monasca API servers you then need to
        restart the Monasca Agent on each of those servers with this command:</p>
      <codeblock>sudo service monasca-agent restart</codeblock>
      <p>With the Compute node references removed and the Monasca Agent restarted, you can then
        delete the corresponding alarm to finish this process. To do so we recommend using the
        Monasca CLI which should be installed on each of your Monasca API servers by default:</p>
      <codeblock>monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=&#60;compute node deleted></codeblock>
      <p>For example, if your Compute node looked like the example above then you would use this
        command to get the alarm ID:</p>
      <codeblock>monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=helion-cp1-comp0001-mgmt</codeblock>
      <p>You can then delete the alarm with this command:</p>
      <codeblock>monasca alarm-delete &#60;alarm ID></codeblock>
    </section>
  </body>
</topic>
