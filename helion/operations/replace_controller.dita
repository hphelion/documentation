<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_ijj_45j_nt">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Replacing a Controller Node</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section>
      <p>For <keyword keyref="kw-hos-phrase"/> versions, you must have three controller nodes.
        Therefore, adding or removing nodes is not an option. However, if you need to repair or
        replace a controller node, you may do so by following the steps outlined here. Note that to
        run any playbooks whatsoever for cloud maintenance, you will always run from the lifecycle
        manager.</p>
    </section>
    <section id="notes"><title>Notes</title>
      <p>Do not add entries for a new server; instead, just update the entries for the broken one.
        Also note that if you are replacing a control plane node that also happens to be running the
        lifecycle manager, that process is different. Youâ€™ll need to restore your lifecycle manager
        data first.</p>
      <p>Be aware that all management commands are run on the node where the lifecycle manager is
        running.</p>
    </section>
    <section id="steps"><title>Procedure for Replacing a Controller Node</title>
      <p><b>Additional Instructions for a Shared Lifecycle Manager/Controller Node</b></p>
      <p>If the controller node you need to replace was also being used as your lifecycle manager
        then use these steps below. If this is not a shared controller then skip to the next
        section.</p>
      <ol>
        <li>Ensuring that you use the same version of HPE Helion OpenStack that you previously had
          loaded on your lifecycle manager, you will need to download and install the lifecycle
          management software using the instructions from the installation guide: <ol>
            <li><xref href="../installation/install_entryscale_kvm.dita#install_kvm/setup_deployer"
                >Set up the Lifecycle-manager</xref></li>
          </ol></li>
        <li>Then you will want to restore your data using the <xref
            href="../bura/cloud_control_plane_recovery.dita#topic_tb4_lqy_qt/deployer">Deployer
            Disaster Recovery</xref> instructions.</li>
        <li>Once that is complete you can continue on to the <xref
            href="replace_controller.dita#topic_ijj_45j_nt/image">Image the new node</xref> steps,
          starting on step #5.</li>
      </ol>
      <p><b>Update your model with replacement server information</b></p>
      <ol>
        <li> Update your cloud model (<codeph>servers.yml</codeph>) with the new
            <codeph>mac-addr</codeph>, <codeph>ilo-ip</codeph>, <codeph>ilo-password</codeph>, or
            <codeph>ilo-user</codeph> fields where these have changed. Do not change the
            <codeph>id</codeph>, <codeph>ip-addr</codeph>, <codeph>role</codeph>, or
            <codeph>server-group</codeph> settings. (Please follow the procedure for updating your
          cloud model in the git repo)</li>
        <li> Get the <b>servers.yml</b> file stored in git:
          <codeblock>cd ~/helion/my_cloud/definition/data
git checkout site</codeblock> then change,
          as necessary, the <codeph>mac-addr</codeph>, <codeph>ilo-ip</codeph>,
            <codeph>ilo-password</codeph>, and <codeph>ilo-user</codeph> fields of this existing
          controller node. Save and commit the change
          <codeblock>git commit -a -m "repaired node X"</codeblock></li>
        <li>Run the configuration processor as follows:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
          Then run ready-deployment:
          <codeblock>ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
        </li>
      </ol>
    </section>
    <section id="image">
      <p><b>Image the new node</b></p>
      <ol>
        <li>Log onto the node that is running the lifecycle manager. Remove the broken control plane
          node from cobbler by running sudo cobbler system remove --name &lt;server-id>. Here's an
          example:<codeblock> sudo cobbler system remove --name controller2</codeblock>
        </li>
        <li>Remove the ssh key of the old controller node from the known hosts file. Here's an
          example:
          <codeblock>ssh-keygen -f "/home/stack/.ssh/known_hosts" -R 10.123.12.3</codeblock></li>
        <li>Run the cobbler deploy playbook to add the replacement node
          <codeblock>ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock>
        </li>
        <li> Re-image the new nodes by running <b>bm-reimage.yml</b>: ansible-playbook -i
          hosts/localhost bm-reimage.yml -e nodelist=&lt;node-name>. Note: Note: you must ensure
          that the old controller node is powered off before completing this step. This is because
          the new controller node will re-use the original IP address.
          <codeblock>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=controller2</codeblock></li>
        <li>Configure the necessary keys used for the database etc:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml</codeblock></li>
        <li>Log into the 2 original controller nodes as "dbadmin" and run the following command:
          <codeblock>ssh-keygen -f "/home/dbadmin/.ssh/known_hosts" -R {{ip_of_node_to_replace}}</codeblock></li>
        <li>Run osconfig on the replacement controller node. For example:
          <codeblock>ansible-playbook -i hosts/verb_hosts osconfig-run.yml  -e rebuild=True --limit=hlm-cp1-c1-m2-mgmt</codeblock>
        </li>
        <li> If the controller being replaced is the first server running the swift-proxy service
          (see <xref href="../objectstorage/first_proxy_server.dita">Identifying the First Proxy
            Server</xref>) you need to restore the Swift Ring Builder files to the
          /etc/swiftlm/builder_dir directory. See <xref
            href="../objectstorage/recovering_builder_file.dita#topic_gbz_13t_mt">Recovering Builder
            Files</xref> for details. <p/></li>
        <li>Run swift-deploy across all controllers using:
          <codeblock>ansible-playbook -i hosts/verb_hosts swift-deploy.yml</codeblock></li>
        <li> Run hlm-deploy on the replacement controller
          <codeblock>ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e rebuild=True --limit=hlm-cp1-c1-m2-mgmt</codeblock>
        </li>
      </ol>
      <!--
      <ul>
      <li>When the node is ready to be restored, the process for re-introducing it will depend on the
        actions performed to repair it. If it was simply rebooted to introduce new software, etc.
        then running the <b>hlm-start.yml</b> playbook will suffice. If needed, use
        <b>bm-power-up.yml </b>playbook to restart the node. Specify just the node(s) you want
        to start in the 'nodelist' parameter aguments, i.e.
        nodelist=&lt;node1>[,&lt;node2>][,&lt;node3>]
        <codeblock>cd ~/helion/hos/ansible
~/helion/hos/ansible$ ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=cpn-0004</codeblock>
      </li><li>Execute the <b>hlm-start.yml </b>playbook. Specifying the node(s) you want to start in the
        'limit' parameter arguments. This parameter accepts wildcard arguments and also
        '@&lt;filename>' to process all hosts listed in the file.
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml - -limit hlm004-ccp-comp0004-mgmt</codeblock>
      </li><li>If the node has undergone repairs that impact disk contents, then running the
        <b>re-image.yml</b> and <b>hlm-deploy.yml </b>playbooks will be required.
        Run:<codeblock>cd ~/helion/hos/ansible
~/helion/hos/ansible$ ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=cpn-0004</codeblock>
      </li><li>Then execute the <b>hlm-start.yml</b> playbook.
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml - -limit hlm004-ccp-comp0004-mgmt</codeblock>
      </li>
      </ul>
      -->
    </section>
  </body>
</topic>
