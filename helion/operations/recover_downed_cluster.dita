<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="recover_downed_cluster">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Recovering a Downed Cluster After Power
    Outage</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <note type="attention">Hyperlinks intermittently do not work in the Google Chrome browser. <xref
        href="http://docs.hpcloud.com/helion/operations/recover_downed_cluster.html"
        scope="external" format="html">Click here</xref> for a frameless version of this page where
      the links should work.</note>

    <section id="about">
      <p>You may have a need to recover your system if you have lost power or the control nodes have
        gone down.</p>
      <p>The steps involved are:</p>
      <ol>
        <li><xref href="recover_downed_cluster.dita#add_node/prereqs">Prerequisites</xref></li>
        <li><xref href="recover_downed_cluster.dita#add_node/mysql">Recovering the MySQL
            Database</xref></li>
        <li><xref href="recover_downed_cluster.dita#add_node/vertica">Recovering the Vertica
            Database</xref></li>
        <li><xref href="recover_downed_cluster.dita#add_node/hlm">Restarting the Lifecycle
            Manager</xref></li>
      </ol>
    </section>

    <section id="prereqs"><title>Prerequisites</title>
      <p>You must be able to power on all of the control nodes before continuing.</p>
    </section>

    <section id="mysql"><title>Recovering the MySQL Database</title>
      <p>You will need to power on all of the control planes before bootstrapping the MySQL cluster.
        To bootstrap a downed MySQL cluster, you will need to discover the last node to go down and
        run bootstrap on that node. Since the cluster didn't go down cleanly, you will need to SSH
        into each control node and run the following commands to find the node with the highest
        sequence number. When you find the node with the highest sequence number, you will need to
        run the bootstrap on that node. </p>
      <ol>
        <li>SSH into each controller node and use the commands below to get the log sequence number: <codeblock>sudo /usr/bin/mysqld_safe --wsrep-recover
sudo grep --text 'log sequence number' /var/log/mysql/error.log | tail -1</codeblock>
          <p>Here is an example showing the output from a three controller node cluster:</p>
          <note type="important">In this example, two nodes have the same sequence number which
            means you can run the bootstrap on either node but you can only run the bootstrap
            command once and only on one node.</note>
          <codeblock>stack@helion-cp-c1-m1-mgmt:~$ sudo /usr/bin/mysqld_safe --wsrep-recover
stack@helion-cp-c1-m1-mgmt:~$ sudo grep --text 'log sequence number' /var/log/mysql/error.log | tail -1 
151119 14:37:12 InnoDB: Shutdown completed; log sequence number 165002105 

stack@helion-cp-c1-m2-mgmt:~$ sudo /usr/bin/mysqld_safe --wsrep-recover 
stack@helion-cp-c1-m2-mgmt:~$ sudo grep --text 'log sequence number' /var/log/mysql/error.log | tail -1 
151119 14:53:22 Percona XtraDB (http://www.percona.com) 5.5.41-37.0 started; log sequence number 165252407

stack@helion-cp-c1-m3-mgmt:~$ sudo /usr/bin/mysqld_safe --wsrep-recover
stack@helion-cp-c1-m3-mgmt:~$ sudo grep --text 'log sequence number' /var/log/mysql/error.log | tail -1 
151119 14:53:22 Percona XtraDB (http://www.percona.com) 5.5.41-37.0 started; log sequence number 165252407 
151119 14:53:23 InnoDB: Shutdown completed; log sequence number 165252407 </codeblock>
        </li>
        <li>SSH into the node with the highest sequence and bootstrap the cluster.
          <codeblock>sudo /etc/init.d/mysql bootstrap-pxc
sudo service mysql restart</codeblock>
        </li>

        <li>Log onto the other two controller nodes and run mysql start.
          <codeblock>sudo service mysql start</codeblock>
        </li>
      </ol>
    </section>
    <section id="vertica"><title>Recovering the Vertica Database</title>
      <p>The first step is to check the health of the Vertica databases as the Vertica nodes try to
        rejoin the cluster.</p>
      <p><b>Health Check</b></p>
      <p>These are the steps to check the health status of the database and should be run after all
        of your control plane nodes are up and running.</p>
      <ol>
        <li>Log in to any of the control plane nodes and change to the root user by using
            <codeph>sudo su</codeph>.</li>
        <li>Check the status of the database using this command: <codeblock>su dbadmin -c '/opt/vertica/bin/admintools -t view_cluster -d mon'</codeblock>
          <p>You should see an output that looks like this below if it's up:
            <codeblock> DB  | Host | State
-----+------+-------
mon  | ALL  | UP</codeblock></p></li>
        <li>If the output of all of your Vertica databases show they are up then you are good to go
          onto the next section. If any of them show they are down then you need to continue onto
          the next steps to recover the databases.</li>
      </ol>
      <p><b>Vertica Recovery Steps</b></p>
      <p>If the output of the health check shows any node in an initializing state, you cannot
        continue on with the Vertica recovery as Vertica is still attempting to start up on it's
        own. You can either wait until the status goes into either a <codeph>Down</codeph> or
          <codeph>Up</codeph> state or you can force the node into a down state to continue on with
        the recovery steps.</p>
      <p>To shut an initializing node down, follow these steps:</p>
      <ol>
        <li>Log in to any of the control plane nodes and change to the root user by using
            <codeph>sudo su</codeph>.</li>
        <li>Shut down the node using the command below. Note that the IP address you use here is
          going to be the IP address of the node that shows the database in an initializing state:
          <codeblock>su dbadmin -c '/opt/vertica/bin/admintools -t stop_node -s &lt;ip address of node to shutdown>'</codeblock></li>
      </ol>
      <p>Once all of the nodes you need to recover are no longer in an initializing state then here
        are the steps to recover the down nodes:</p>
      <ol>
        <li>Locate the Vertica admin password from the lifecycle manager node:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
grep dbadmin_user_password group_vars/*</codeblock></li>
        <li>Log in to any of the control plane nodes and change to the root user by using
            <codeph>sudo su</codeph>.</li>
        <li>Restart the Vertica database using the command below. Specify your Vertica admin
          password in the command:
          <codeblock>su dbadmin -c '/opt/vertica/bin/admintools -t restart_db -d mon -e last -p &lt;vertica_admin_password>'</codeblock></li>
        <li>Check the Vertica database status:
          <codeblock>su dbadmin -c '/opt/vertica/bin/admintools -t view_cluster -d mon'</codeblock></li>
        <li>If the output shows that your nodes are now <codeph>Up</codeph> then you are good to go.
          If they do not show in an <codeph>Up status then continue to the next step.</codeph></li>
        <li>Force a restart of each node. Replace the IP address in the command below to match the
          IP address of your control plane nodes.
          <codeblock>su dbadmin -c '/opt/vertica/bin/admintools -t restart_node -s &lt;vertica_cluster_ip> -d mon -p &lt;vertica_admin_password>'</codeblock></li>
      </ol>
    </section>
    <section id="hlm"><title>Restarting the Lifecycle Manager</title>
      <ol>
        <li>Log into the lifecycle manager and execute the hlm-start playbook for the 3 controller
          nodes. <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit=&lt;host_var name&gt;</codeblock>
          <p>Here is an example using the above command using the example in the previous step.</p>
          <codeblock>ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit=helion-cp-c1-m1-mgmt,helion-cp-c1-m2-mgmt,helion-cp-c1-m3-mgmt</codeblock>
        </li>
        <li>Now run the hlm-start playbook for all nodes. When this playbook completes, all services
          should be started.
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml</codeblock>
        </li>
      </ol>
    </section>
  </body>
</topic>
