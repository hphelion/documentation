<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic id="alarmdefinitions">
  <title>HPE Helion <tm tmtype="reg">OpenStack</tm> 2.0: Service Alarm Definitions</title>
  <shortdesc>This topic contains a list of service-specific alarms and the recommended
    troubleshooting steps.</shortdesc>
  <body><!--Needs Work; should the command/code content in the table be surrounded by codeph tags; we are inconsistent? for Ceilometer/HTTP Status, what needs to be restarted (it?); is there a mititgation for Keystone/Host alive Check?-->
    <p>When alarms are triggered it may be helpful for you to review the service logs. For details
      on how to use the logging user interface built into the Operations Console, see <xref
        href="centralized_logging.dita#logging/interface"/>.</p>
    <section id="services"><title>Services</title>
      <p>Alarms for the following services are described below:</p>
      <ul>
        <li><xref href="#alarmdefinitions/metering">Ceilometer</xref></li>
        <li><xref href="#alarmdefinitions/blockstorage">Cinder</xref></li>
        <li><xref href="#alarmdefinitions/cue">Cue</xref></li>
        <li><xref href="#alarmdefinitions/glance">Glance</xref></li>
        <li><xref href="#alarmdefinitions/haproxy">HAProxy</xref></li>
        <li><xref href="#alarmdefinitions/heat">Heat</xref></li>
        <li><xref href="#alarmdefinitions/horizon">Horizon</xref></li>
        <li><xref href="#alarmdefinitions/hlinux">Linux for HPE Helion</xref></li>
        <li><xref href="#alarmdefinitions/identity">Keystone</xref></li>
        <li><xref href="#alarmdefinitions/logging">Logging</xref></li>
        <li><xref href="#alarmdefinitions/monasca">Monasca</xref></li>
        <li><xref href="#alarmdefinitions/mysql">MySQL</xref></li>
        <li><xref href="#alarmdefinitions/networking">Neutron</xref></li>
        <li><xref href="#alarmdefinitions/nova">Nova</xref></li>
        <li><xref href="#alarmdefinitions/opsconsole">Operations Console</xref></li>
        <li><xref href="#alarmdefinitions/rabbitmq">RabbitMQ</xref></li>
        <li><xref href="#alarmdefinitions/objectstorage">Swift</xref></li>
      </ul>
    </section>
    <section id="metering"><title>Ceilometer</title>
      <p><table frame="all" rowsep="1" colsep="1" id="ceilometer_table">
          <tgroup cols="4">
            <colspec colname="c1" colnum="1" colwidth="1*"/>
            <colspec colname="c2" colnum="2" colwidth="2*"/>
            <colspec colname="c3" colnum="3" colwidth="1*"/>
            <colspec colname="c4" colnum="4" colwidth="2*"/>
            <thead>
              <row>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=ceilometer-agent-notification</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=ceilometer-collector</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=ceilometer-agent-central</entry>
                <entry>Process crashed.</entry>
                <entry>Restart process on the affected node. Review the associated  logs.</entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>
                  <p>service=telemetry and used with hostname of one of the controller </p>
                </entry>
                <entry>Ceilometer API.</entry>
                <entry>Restart Apache on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>component=ceilometer-api </entry>
                <entry>Ceilometer API on administrator VIP.</entry>
                <entry><p>If this occurs with an http_status in error on all nodes, then restart Apache on all
                    controllers.</p>
                  <p>If this occurs with a specific host with http_status in non-error for
                    telemetry, then it should be a haproxy issue and it needs to be restarted.</p>
                  <p>For further troubleshooting, look into the Ceilometer access log in the
                    Ceilometer log directory for the ceilometer_modwsgi file and ceilometer-api
                    logs.</p></entry>
              </row>
            </tbody>
          </tgroup>
        </table></p>
    </section>
    <section id="blockstorage"><title>Cinder</title>
      <p><table frame="all" rowsep="1" colsep="1" id="cinder_table">
          <tgroup cols="4">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="2*"/>
            <colspec colname="c3" colnum="3" colwidth="1.0*"/>
            <colspec colname="c4" colnum="4" colwidth="2*"/>
            <thead>
              <row>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=cinder-api</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=cinder-backup</entry>
                <entry>Process crashed.</entry>
                <entry>Alert may be incorrect if the service has migrated. Validate that the service
                  is intended to be running on this node before restarting the service. Review the
                  associated logs.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=cinder-scheduler</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=cinder-volume</entry>
                <entry>Process crashed.</entry>
                <entry>Alert may be incorrect if the service has migrated. Validate that the service
                  is intended to be running on this node before restarting the service. Review the
                  associated logs.</entry>
              </row>
              <row>
                <entry>Cinder backup running &lt;HOSTNAME&gt; check</entry>
                <entry>Cinder backup singleton check.</entry>
                <entry>
                  <p>Backup process is either:</p>
                  <p><ul>
                      <li>running on a node it should not be on, or</li>
                      <li>not running on a node it should be on</li>
                    </ul></p>
                </entry>
                <entry>Run the cinder-migrate-volume playbook to migrate the volume and backup to
                  the correct node.</entry>
              </row>
              <row>
                <entry>Cinder volume running &lt;HOSTNAME&gt; check</entry>
                <entry>Cinder volume singleton check.</entry>
                <entry>
                  <p>Volume process is either:</p>
                  <p><ul>
                      <li>running on a node it should not be on, or</li>
                      <li>not running on a node it should be on</li>
                    </ul></p>
                </entry>
                <entry>Run the cinder-migrate-volume playbook to migrate the volume and backup to
                  correct node.</entry>
              </row>
            </tbody>
          </tgroup>
        </table></p>
    </section>
    <section id="cue"><title>Cue</title>
      <p><table frame="all" rowsep="1" colsep="1" id="cue_table">
          <tgroup cols="4">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="2*"/>
            <colspec colname="c3" colnum="3" colwidth="1.0*"/>
            <colspec colname="c4" colnum="4" colwidth="2*"/>
            <thead>
              <row>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>HTTP Status</entry>
                <entry>component=cue-api</entry>
                <entry>API is unresponsive.</entry>
                <entry>Restart Apache.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=cue-worker</entry>
                <entry>Supervisor stopped.</entry>
                <entry>Use supervisor to start all workers.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=cue-monitor</entry>
                <entry>cue-monitor process crashed.</entry>
                <entry>Restart the cue-monitor service.</entry>
              </row>
            </tbody>
          </tgroup>
        </table></p>
    </section>
    <section id="glance"><title>Glance</title>
      <p><table frame="all" rowsep="1" colsep="1" id="glance_table">
          <tgroup cols="4">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="2*"/>
            <colspec colname="c3" colnum="3" colwidth="1.0*"/>
            <colspec colname="c4" colnum="4" colwidth="2*"/>
            <thead>
              <row>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>HTTP Status</entry>
                <entry>component=glance-api</entry>
                <entry>API is unresponsive.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>component=glance-registry</entry>
                <entry>Glance registry is unresponsive.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
            </tbody>
          </tgroup>
        </table></p>
    </section>
    <section id="haproxy">
      <title>HAProxy</title>
      <table frame="all" rowsep="1" colsep="1" id="table_flj_2tl_zs">
        <tgroup cols="4" align="left">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="2.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="2.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Process Check</entry>
              <entry>process_name=haproxy</entry>
              <entry>HA Proxy is not running on this machine.</entry>
              <entry>Restart haproxy on the alarming host.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="heat">
      <title>Heat</title>
      <table frame="all" rowsep="1" colsep="1" id="table_k33_xcq_bt">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="2.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="2.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Process Check</entry>
              <entry>heat-api process check on each node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>heat-api-cfn process check on each node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>heat-api-cloudwatch process check on each node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>heat-engine process check on each node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>heat-api local http status check on each node</entry>
              <entry>Process hung or crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>heat-api-cfn local http status check on each node</entry>
              <entry>Process hung or crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>heat-api remote http status check only on one node</entry>
              <entry>Process hung or crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>heat-api-cfn remote http status check only on one node</entry>
              <entry>Process hung or crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="horizon"><title>Horizon</title>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="table_vtf_3md_3t">
          <tgroup cols="4">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
            <colspec colname="c3" colnum="3" colwidth="1.0*"/>
            <colspec colname="c4" colnum="4" colwidth="1.0*"/>
            <thead>
              <row>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>HTTP Status</entry>
                <entry>Alerts when Horizon's login page is not returned.</entry>
                <entry>Apache is not running or there is a misconfiguration.</entry>
                <entry>Check that Apache is running; investigate Horizon logs.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
    </section>
    <section id="hlinux">
      <title>Linux for HPE Helion</title>
      <table frame="all" rowsep="1" colsep="1" id="table_ik5_3km_zs">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="2.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="2.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>CPU Usage</entry>
              <entry>Alarms on high CPU usage.</entry>
              <entry>Heavy load or runaway processes.</entry>
              <entry>Log onto the system and diagnose the heavy CPU usage.</entry>
            </row>
            <row>
              <entry>Disk Inode Usage</entry>
              <entry>Nearly out of inodes for a partition.</entry>
              <entry>Many files on the disk.</entry>
              <entry>Investigate cleanup of data or migration to other partitions.</entry>
            </row>
            <row>
              <entry>Disk Usage</entry>
              <entry>High Disk usage.</entry>
              <entry>Large files on the disk.</entry>
              <entry>Investigate cleanup of data or migration to other partitions.</entry>
            </row>
            <row>
              <entry>Host Status</entry>
              <entry>Alerts when a host is unreachable.</entry>
              <entry>Host or network is down.</entry>
              <entry>If a single host, attempt to restart the system. If multiple hosts, investigate
                network issues.</entry>
            </row>
            <row>
              <entry>Memory Usage</entry>
              <entry>High memory usage.</entry>
              <entry>Overloaded system or services with memory leaks.</entry>
              <entry>Log onto the system to investigate high memory users.</entry>
            </row>
            <row>
              <entry>Network Errors</entry>
              <entry>Alarms on a high network error rate.</entry>
              <entry>Bad network or cabling.</entry>
              <entry>Take this system out of service until the network can be fixed.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="identity"><title>Keystone</title>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="table_bw1_jmd_3t">
          <tgroup cols="4">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
            <colspec colname="c3" colnum="3" colwidth="1.0*"/>
            <colspec colname="c4" colnum="4" colwidth="1.0*"/>
            <thead>
              <row>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>HTTP Status</entry>
                <entry>component=keystone-api,api_endpoint=public</entry>
                <entry>API is unresponsive.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>component=keystone-api,api_endpoint=admin</entry>
                <entry>API is unresponsive.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>component=keystone-api,monitored_host_type=vip</entry>
                <entry>API is unresponsive on each node.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=keystone-main</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=keystone-admin</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
    </section>
    <section id="logging"><title>Logging</title>
      <table frame="all" rowsep="1" colsep="1" id="logging_table">
        <tgroup cols="5">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <colspec colname="c4" colnum="4"/>
          <colspec colname="c5" colnum="5"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Component</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Elasticsearch Unassigned Shards</entry>
              <entry>Elasticsearch</entry>
              <entry>Elasticsearch unassigned shards count is greater than 0.</entry>
              <entry>Environment could be misconfigured.</entry>
              <entry>
                <p>To find the unassigned shards, run the following command on the lifecycle-manager node
                  from the <codeph>~/scratch/ansible/next/hos/ansible</codeph> directory:</p>
                <codeblock>ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a "curl localhost:9200/_cat/shards?pretty -s" | grep UNASSIGNED</codeblock>
                <p>This should show which shards are unassigned, like this:</p>
                <codeblock>logstash-2015.10.21 4 p UNASSIGNED 11412371  3.2gb 10.241.67.11 Keith Kilham</codeblock>
                <p>The last column shows the name that Elasticsearch uses for the node that the
                  unassigned shards are on. To find the actual hostname, run:</p>
                <codeblock>ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a "curl localhost:9200/_nodes/_all/name?pretty -s"</codeblock>
                <p>Once you find the hostname, you can try the following:</p>
                <ol>
                  <li>Make sure the node is not out of disk space, and free up space if needed.</li>
                  <li>Restart the node (use caution, as this may affect other services as
                    well).</li>
                  <li>Check to make sure all versions of Elasticsearch are the same with this:
                    <codeblock>ansible -i hosts/verb_hosts LOG-SVR -m shell -a "curl localhost:9200/_nodes/_local/name?pretty -s" | grep version</codeblock></li>
                  <li>Contact customer support.</li>
                </ol>
              </entry>
            </row>
            <row>
              <entry>Elasticsearch Max Total Indices Size</entry>
              <entry>Elasticsearch</entry>
              <entry>Elasticsearch Total Indices size is greater than
                elasticsearch_max_total_indices_size_in_bytes defined by user.</entry>
              <entry>elasticsearch_max_total_indices_size_in_bytes may be set too low, or you may
                have insufficient resources.</entry>
              <entry>If the total size of all the indices exceeds the size of the
                elasticsearch_max_total_indices_size_in_bytes as you have defined it, you can adjust
                the value if you want to give more space and you have the resources. If not, you may
                need to adjust the Curator to remove indices sooner.</entry>
            </row>
            <row>
              <entry>Elasticsearch Number of Log Entries</entry>
              <entry>Elasticsearch</entry>
              <entry>Elasticsearch Number of Log Entries.</entry>
              <entry>The number of log entries may get too large.</entry>
              <entry>Older versions of Kibana (version 3 and earlier) may hang if the number of log
                entries is too large (e.g. above 40,000), and the page size would need to be small
                enough (about 20,000 results), because if it is larger (e.g. 200,000), it may hang
                the browser, but Kibana 4 should not have this issue.</entry>
            </row>
            <row>
              <entry>Elasticsearch Field Data Evictions</entry>
              <entry>Elasticsearch</entry>
              <entry>Elasticsearch Field Data Evictions count is greater than 0.</entry>
              <entry>Field Data Evictions may be found even though it is nowhere near the limit
                set.</entry>
              <entry>The elasticsearch_indices_fielddata_cache_size is tuned out-of-the box, but if
                it is insufficient, you may need to increase this configuration parameter and run a
                reconfigure.</entry>
            </row>
            
            
            
            <row>
              <entry>Elasticsearch Low Watermark alarm definition</entry>
              <entry>Elasticsearch</entry>
              <entry>Elasticsearch Disk LOW Watermark. Backup indices. If high watermark is reached,
                indices will be deleted. Adjust curator_low_watermark_percent,
                curator_high_watermark_percent, and elasticsearch_max_total_indices_size_in_bytes if
                needed.</entry>
              <entry>Running out of disk space for /var/lib/elasticsearch.</entry>
              <entry>Free up space by removing indices (backing them up first if desired).
                Alternatively, adjust curator_low_watermark_percent, curator_high_watermark_percent,
                and/or elasticsearch_max_total_indices_size_in_bytes if needed.</entry>
            </row>
            <row>
              <entry>Elasticsearch High Watermark alarm definition</entry>
              <entry>Elasticsearch</entry>
              <entry>Elasticsearch Disk HIGH Watermark. Attempting to delete indices to free disk
                space. Adjust curator_low_watermark_percent, curator_high_watermark_percent, and
                elasticsearch_max_total_indices_size_in_bytes if needed.</entry>
              <entry>Running out of disk space for /var/lib/elasticsearch.</entry>
              <entry>Verify that disk space was freed up by the curator. If needed, free up
                additional space by removing indices (backing them up first if desired).
                Alternatively, adjust curator_low_watermark_percent, curator_high_watermark_percent,
                and/or elasticsearch_max_total_indices_size_in_bytes if needed.</entry>
            </row>
            
            
            <row>
              <entry>Elasticsearch Bulk Pool Rejected</entry>
              <entry>Elasticsearch</entry>
              <entry>Elasticsearch Bulk Pool Rejected count is greater than 0.</entry>
              <entry>Thread pool size could be too small.</entry>
              <entry>You may need to increase the threadpool.bulk.queue_size (from 50 to 100, or
                even 3000, for example). This is found in the config/logging/elasticsearch.yml.j2
                file.</entry>
            </row>
            
            <row>
              <entry>Elasticsearch Open File Descriptors </entry>
              <entry>Elasticsearch</entry>
              <entry>Elasticsearch Open File Descriptors is greater than 90%.</entry>
              <entry>Configured to allow too many file descriptors to be opened.</entry>
              <entry>Close some old indices. This can be performed by setting the
                curator_close_indices_after_days value from the default of 0, which does not close
                indices, to a specific number of days. This is located in the
                config/logging-common/main.yml file.</entry>
            </row>
            
            <row>
              <entry>Process Check</entry>
              <entry>Elasticsearch</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>If the Elasticsearch process goes down, try restarting it with: <codeph>sudo
                  systemctl restart elasticsearch</codeph></entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Logstash</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>If the Logstash process goes down, try restarting it with: <codeph>sudo
                  systemctl restart logstash</codeph>.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Beaver</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>If the Beaver process goes down, try restarting it with: <codeph>sudo systemctl
                  restart beaver</codeph></entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Apache</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>If the Apache process goes down, try restarting it with: <codeph>sudo systemctl
                  restart apache2</codeph></entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Kibana</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>If the Kibana process goes down, try restarting it with: <codeph>sudo systemctl
                  restart kibana</codeph></entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="monasca">
      <title>Monasca</title>
      <table frame="all" rowsep="1" colsep="1" id="table_dry_245_zs">
        <tgroup cols="5">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="1.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="1.0*"/>
          <colspec colname="c5" colnum="5" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Component</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>HTTP Status</entry>
              <entry>persister</entry>
              <entry>Persister Health Check</entry>
              <entry>The process has crashed or a dependency is out.</entry>
              <entry>If the process has crashed, restart. If a dependent service is down, address
                that issue.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>api</entry>
              <entry>API Health Check</entry>
              <entry>The process has crashed or a dependency is out.</entry>
              <entry>If the process has crashed, restart. If a dependent service is down, address
                that issue.</entry>
            </row>
            <row>
              <entry>Kafka Consumer Lag</entry>
              <entry>Kafka</entry>
              <entry>Consumers are falling behind for a particular Kafka topic.</entry>
              <entry>There is a slow down in the system or heavy load.</entry>
              <entry>Look for high load in the various systems. This alert can fire for multiple
                topics or on multiple hosts. Which alarms are firing can help diagnose likely
                causes, i.e., if all are on one machine it could be the machine. If one topic across
                multiple machines it is likely the consumers of that topic, etc.</entry>
            </row>
            <row>
              <entry>Monasca Agent Collection Time</entry>
              <entry>monasca-agent</entry>
              <entry>The time the agent took to collect metrics.</entry>
              <entry>Heavy load on the box or a stuck agent plug-in.</entry>
              <entry>Address the load issue on the machine. If needed, restart the agent.</entry>
            </row>
            <row>
              <entry>Monasca Agent Emit Time</entry>
              <entry>monasca-agent</entry>
              <entry>The time the agent took to send metrics to the Monasca API.</entry>
              <entry>API or the network is slow.</entry>
              <entry>Check the network graphs and look for indication of network issues. If coming
                from most boxes, it is possibly API problems.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Kafka</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>notification</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>storm</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>zookeeper</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Zookeeper Latency</entry>
              <entry>zookeeper</entry>
              <entry>Zookeeper is experiencing high latency.</entry>
              <entry>Heavy system load.</entry>
              <entry>Check the individual system as well as activity across the entire
                service.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="mysql"><title>MySQL</title>
      <table frame="all" rowsep="1" colsep="1" id="table_ft3_bp5_zs">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="1.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>MySQL Slow Query Rate</entry>
              <entry>MySQL is reporting many slow queries.</entry>
              <entry>System load.</entry>
              <entry>This could be an indication of near capacity limits or an exposed bad query.
                First, check overall system load and then investigate MySQL details.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Alarms when MySQL process is not found.</entry>
              <entry>MySQL crashed.</entry>
              <entry>Restart MySQL on the affected node.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="networking">
      <title>Neutron</title>
      <table frame="all" rowsep="1" colsep="1" id="table_bcj_5p4_1t">
        <tgroup cols="5">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="1.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="1.0*"/>
          <colspec colname="c5" colnum="5" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Monitor Location</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-openvswitch-agent pid check</entry>
              <entry>local node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-l3-agent pid check</entry>
              <entry>local node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-dhcp-agent pid check</entry>
              <entry>local node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-metadata-agent pid check</entry>
              <entry>local node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-rootwrap pid check</entry>
              <entry>local node</entry>
              <entry>Process crashed.</entry>
              <entry>Currently neutron-rootwrap is only used to run ovsdb-client. To restart this,
                restart neutron-openvswitch-agent on the affected node. Review the associated
                logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-lbaas-agent</entry>
              <entry>local node</entry>
              <entry>Processed crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-lbaasv2-agent</entry>
              <entry>local node</entry>
              <entry>Processed crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-vpn-agent</entry>
              <entry>local node</entry>
              <entry>Processed crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-server pid check</entry>
              <entry>local node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>neutron api health check</entry>
              <entry>local node</entry>
              <entry>Process stuck if neutron-server Process Check OK</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>neutron api health check</entry>
              <entry>remote node</entry>
              <entry>Node crashed or connectivity lost if local node HTTP Status OK or
                UNKNOWN</entry>
              <entry>Reboot the node or repair the network. Review the associated logs.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="nova">
      <title>Nova</title>
      <table frame="all" rowsep="1" colsep="1" id="table_bv5_q2h_1t">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c3" colnum="2" colwidth="1.0*"/>
          <colspec colname="c4" colnum="3" colwidth="1.0*"/>
          <colspec colname="c5" colnum="4" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Process Check</entry>
              <entry>nova-api check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the nova-api log
                files.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>nova-api health check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the nova-api log files. Try to
                connect locally to the http port to see if the connection is accepted.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>nova-cert check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on the affected node. Review the nova-cert.log files.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>nova-compute check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on the affected node. Review the nova-compute.log
                files.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>nova-conductor check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the nova-conductor.log
                files.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>nova-consoleauth check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the nova-consoleauth log
                files.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>nova-novncproxy check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the nova-novncproxy log
                files.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>nova-scheduler check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the nova-scheduler.log
                files.</entry>
            </row>
            <row>
              <entry>Process Bound Check</entry>
              <entry>process_name=nova-api. Check that the number of processes found is in a
                predefined range</entry>
              <entry>Process crashed or too many processes running</entry>
              <entry>Stop all the processes and restart the nova-api process. Review the system and
                nova-api logs.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="opsconsole"><title>Operations Console</title>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="table_fdn_hpd_3t">
          <tgroup cols="4">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
            <colspec colname="c3" colnum="3" colwidth="1.0*"/>
            <colspec colname="c4" colnum="4" colwidth="1.0*"/>
            <thead>
              <row>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>HTTP Status</entry>
                <entry>service=ops-console</entry>
                <entry>The Operations Console is unresponsive</entry>
                <entry><p>Review logs in <codeph>/var/log/ops-console</codeph> and logs in
                      <codeph>/var/log/apache2</codeph>. Restart ops-console by running the
                    following commands on the lifecycle-manager:</p>
                  <codeblock>
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ops-console-stop.yml 
ansible-playbook -i hosts/verb_hosts ops-console-start.yml</codeblock></entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=leia-leia_monitor</entry>
                <entry>Process crashed or unresponsive.</entry>
                <entry><p>Review logs in <codeph>/var/log/ops-console</codeph>. Restart ops-console by running the
                    following commands on the lifecycle-manager:</p>
                  <codeblock>
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ops-console-stop.yml 
ansible-playbook -i hosts/verb_hosts ops-console-start.yml</codeblock></entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p></section>
    <section id="rabbitmq">
      <title>RabbitMQ</title>
      <table frame="all" rowsep="1" colsep="1" id="table_rsb_mp5_zs">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="1.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Process Check</entry>
              <entry>rabbitmq-server process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="objectstorage">
      <title>Swift</title>
      <table frame="all" rowsep="1" colsep="1" id="table_zn3_2pb_bt">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="1.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>swiftlm-scan monitor</entry>
              <entry>Alarms if swiftlm-scan cannot execute a monitoring task.</entry>
              <entry>The swiftlm-scan program is used to monitor/measure a number of metrics. If it
                is unable to monitor or measure something, it raises this alarm.</entry>
              <entry>Examine the <codeph>Details</codeph> field and look for a <codeph>msg</codeph>
                field. The text may explain the error problem. To view/confirm this, you can also
                log into the host specified by the <codeph>hostname</codeph> dimension, and then run
                this command: <codeph>sudo swiftlm-scan | python -mjson.tool</codeph>. The
                  <codeph>msg</codeph> field is contained in the <codeph>value_meta</codeph>
                item.</entry>
            </row>
            <row>
              <entry>Swift file ownership</entry>
              <entry>Alarms if files/directories in /srv/node/ or /etc/swift are not owned by
                Swift.</entry>
              <entry>
                <p>For files in /etc/swift, somebody may have manually edited or created a file.</p>
                <p>For directories in /srv/node/*, it may happen that root partition was reimaged or
                  reinstalled and the UID assigned to the Swift user changes. The directories and
                  files are then not owned by the UID assigned to the Swift user.</p>
              </entry>
              <entry>
                <p>For files in <codeph>/etc/swift</codeph>, use this command: <codeph>sudo chown
                    swift.swift /etc/swift/, /etc/swift/*</codeph>.</p>
                <p>For directories and files in <codeph>/srv/node/*</codeph>, compare the swift UID
                  of this system and other systems and the UID of the owner of
                    <codeph>/srv/node/*</codeph>. If possible, make the UID of the Swift user match
                  the directories/files. Otherwise, change the ownership of all files and
                  directories under the <codeph>/srv/node</codeph> path.</p>
              </entry>
            </row>
            <row>
              <entry>Drive URE errors</entry>
              <entry>Alarms if swift-drive-audit reports a unrecoverable read error on a drive used
                by Swift.</entry>
              <entry>An unrecoverable read error has occurred when Swift attempted to access a
                directory.</entry>
              <entry>
                <p>The UREs reported only apply to file system metadata (i.e., directory
                  structures). For UREs in object files, the Swift system automatically deletes the
                  file and replicates a fresh copy from one of the other replicas.</p>
                <p>UREs are a normal feature of large disk drives. It does not mean that the drive
                  has failed. However, if you get regular UREs on a specific drive, then this may
                  indicate that the drive is indeed failed and should be replaced.</p>
                <p>You can use standard XFS repair actions to correct the UREs in the file
                  system.</p>
                <p>If the XFS repair fails, you should wipe the GPT table as follows (where LETTER
                  is replaced by the actual drive name):</p>
                <codeblock>sudo dd if=/dev/zero of=/dev/sd&lt;LETTER&gt; bs=$((1024*1024)) count=1</codeblock>
                <p>Then, on the deployer, run the <codeph>_swift-configure.yml</codeph> playbook.
                  This will reformat the drive, remount it, and restart Swift services.</p>
                <p>It is safe to reformat drives containing Swift data because Swift maintains other
                  copies of the data (usually, Swift is configured to have three replicas of all
                  data).</p>
              </entry>
            </row>
            <row>
              <entry>Swift services</entry>
              <entry>Alarms if a Swift process is not running.</entry>
              <entry>A daemon specified by the <codeph>component</codeph> dimension on the host
                specified by the <codeph>hostname</codeph> dimension has stopped running.</entry>
              <entry>
                <p>Examine the <codeph>/var/log/swift/swift.log</codeph> file for possible error
                  messages related the Swift process. The process in question is listed in the alarm
                  dimensions in the <codeph>component</codeph> dimension.</p>
                <p>Restart Swift processes by running the <codeph>swift-start.yml</codeph>
                  playbook.</p>
              </entry>
            </row>
            <row>
              <entry>Swift filesystem mount status</entry>
              <entry>Alarms if a file system/drive used by Swift is not correctly mounted.</entry>
              <entry>
                <p>The device specified by the <codeph>device</codeph> dimension is not correctly
                  mounted at the mountpoint specified by the <codeph>mount</codeph> dimension.</p>
                <p>The most probable cause is that the drive has failed or that it had a temporary
                  failure during the boot process and remained unmounted.</p>
                <p>Other possible causes are a file system corruption that prevents the device from
                  being mounted.</p>
              </entry>
              <entry>
                <p>Reboot the node and see if the file system remains unmounted.</p>
                <p>If the file system is corrupt, see the process used for the "Drive URE errors"
                  alarm to wipe and reformat the drive.</p>
              </entry>
            </row>
            <row>
              <entry>Swift host socket connect</entry>
              <entry>Alarms if a socket cannot be opened to th eKeystone token validation
                API.</entry>
              <entry>The Keystone server may be down. Another possible cause is that the network
                between the host reporting the problem and the Keystone server or that the haproxy
                process is not forwarding requests to Keystone.</entry>
              <entry>The <codeph>URL</codeph> dimension contains the name of the VIP. Use cURL or a
                similar program to confirm that a connection can or cannot be made to the VIP. Check
                that haproxy is running. Check that the Keystone service is working.</entry>
            </row>
            <row>
              <entry>Swift memcache connect</entry>
              <entry>Alarms if a socket cannot be opened to the specified target memcached
                server.</entry>
              <entry>The server may be down. The memcached deamon running the server may have
                stopped.</entry>
              <entry>
                <p>If the server is down, restart it.</p>
                <p>If memcached has stopped, you can restart it by using the
                    <codeph>memcached-start.yml</codeph> playbook. if this fails, rebooting the node
                  will restart the process.</p>
                <p>If the server is running and memcached is running, there may be a network problem
                  blocking port 11211.</p>
                <p>If you see sporadic alarms on different servers, the system may be running out of
                  resources. Contact HPE Support for advice.</p>
              </entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Alarms when the specified process is not running.</entry>
              <entry>If the <codeph>service</codeph> dimension is <codeph>object-store</codeph>, see
                the description of the "Swift Services" alarm for possible causes.</entry>
              <entry>If the <codeph>service</codeph> dimension is <codeph>object-store</codeph>, see
                the description of the "Swift Services" alarm for possible mitigation tasks.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>Alarms when the specified HTTP endpoint is down or not reachable.</entry>
              <entry>If the <codeph>service</codeph> dimension is <codeph>object-store</codeph>, see
                the description of the "Swift host socket connect" alarm for possible
                causes.</entry>
              <entry>If the <codeph>service</codeph> dimension is <codeph>object-store</codeph>, see
                the description of the "Swift host socket connect" alarm for possible mitigation
                tasks.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
  </body>
</topic>
