<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic11470">
    <title>HP Helion <tm tmtype="reg">OpenStack</tm> 2.0: Ceilometer Best Practices and
        Optimization</title>
    <prolog>
        <metadata>
            <othermeta name="layout" content="default"/>
            <othermeta name="product-version" content="HP Helion Openstack"/>
            <othermeta name="product-version" content="HP Helion Openstack 1.1"/>
            <othermeta name="product-version1" content="HP Helion Openstack"/>
            <othermeta name="product-version2" content="HP Helion Openstack 1.1"/>
        </metadata>
    </prolog>
    <body>
        <p>
            <!--PUBLISHED-->
            <!--./commercial/GA1/ceilometer/1.1commerical.services-reporting-bestpractice.md-->
            <!--permalink: /helion/openstack/1.1/services/reporting/bestpractices/--></p>
        <p>Configuration changes that improve reporting API and database responsiveness by keeping
            data storage to a reasonable level.</p>

        <section id="ceilometer_nova">
            <title>Nova</title>
            <p>Nova can send notifications related to its usage and VM status simply enabling it
                from its configuration file.</p>
            <p>Nova will send following data to Ceilometer if the messaging notifications are
                enabled:</p>
        </section>
        <section id="novadata">
            <title>Nova Data Collected by Ceilometer</title>
            <table pgwide="1">
                <tgroup cols="5">
                    <colspec colname="col1" colsep="1" rowsep="1"/>
                    <colspec colname="col2" colsep="1" rowsep="1"/>
                    <colspec colname="col3" colsep="1" rowsep="1"/>
                    <colspec colname="col4" colsep="1" rowsep="1"/>
                    <colspec colname="col5" colsep="1" rowsep="1"/>
                    <tbody>
                        <row>
                            <entry>
                                <b>Name</b>
                            </entry>
                            <entry>
                                <b>Unit</b>
                            </entry>
                            <entry>
                                <b>Type</b>
                            </entry>
                            <entry>
                                <b>Resource</b>
                            </entry>
                            <entry>
                                <b>Note</b>
                            </entry>
                        </row>
                        <row>
                            <entry>instance</entry>
                            <entry>g</entry>
                            <entry>instance</entry>
                            <entry> inst ID</entry>
                            <entry>Existence of instance</entry>
                        </row>
                        <row>
                            <entry>instance: <varname>type</varname>
                            </entry>
                            <entry>g</entry>
                            <entry>instance</entry>
                            <entry> inst ID</entry>
                            <entry>Existence of instance of <varname>type</varname> (Where
                                    <varname>type</varname> is a valid OpenStack type.) </entry>
                        </row>
                        <row>
                            <entry>memory</entry>
                            <entry>g</entry>
                            <entry>MB</entry>
                            <entry> inst ID</entry>
                            <entry>Amount of allocated RAM. Measured in MB.</entry>
                        </row>
                        <row>
                            <entry>vcpus</entry>
                            <entry>g</entry>
                            <entry>vcpu</entry>
                            <entry> inst ID</entry>
                            <entry>Number of VCPUs</entry>
                        </row>
                        <row>
                            <entry>disk.root.size</entry>
                            <entry>g</entry>
                            <entry>GB</entry>
                            <entry> inst ID</entry>
                            <entry>Size of root disk. Measured in GB.</entry>
                        </row>
                        <row>
                            <entry>disk.ephemeral.size</entry>
                            <entry>g</entry>
                            <entry>GB</entry>
                            <entry> inst ID</entry>
                            <entry>Size of ephemeral disk. Measured in GB.</entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
        </section>
        <section id="novaconfigfile">
            <title>Enable Nova Notifications</title>
            <p>To enable Nova to publish these notifications is necessary to apply some changes to
                the nova.conf file.</p>
            <p> The following is an excerpt of the configuration file with the necessary changes to
                enable notifications:</p>
            <codeblock>notification_driver=messaging
notification_topics=notifications
notify_on_state_change=vm_and_task_state
instance_usage_audit=True
instance_usage_audit_period=hour</codeblock>
            <p>The <i>instance_usage_audit_period</i> interval can be set to check the instance's
                status every hour, once a day, once a week or once a month. Every time the audit
                period elapses, Nova sends a notification to Ceilometer to record whether or not the
                instance is alive and running. Metering this statistic is critical if billing
                depends on usage.</p>
        </section>
        <section id="restart-the-nova-service">
            <title>Restart the Nova Service</title>
            <note>Nova needs to be restarted in order for these changes to take effect. Please refer
                to the latest Nova documentation for restarting nova-compute service for specific
                platforms</note>
            <p>For a more in-depth look at how information is sent over <i>openstack.common.rpc</i>,
                refer to the <xref
                    href="http://docs.openstack.org/developer/ceilometer/measurements.html"
                    scope="external" format="html">OpenStack Ceilometer documentation</xref>.</p>
            <p>
                <b>Validation</b> After the Nova service is restarted, the notification daemons
                using the namespace <i>ceilometer.notification</i> should receive some traffic. Each
                plugin can listen to any topic(s), but by default it will listen to
                    <i>notifications.info</i>.</p>
        </section>
        <section id="webserverapi">
            <title><b>Improving Reporting API responsiveness</b></title>
            <p>Reporting APIs are the main access to the Metering data stored in Ceilometer. These
                APIs are access by Horizon to provide basic usage data and information.</p>
            <p>HP Helion OpenStack uses Apache2 Web Server to serve the API access. It is possible
                to tune performance to optimize the front-end and back-end (database). Our
                experience indicates that an excessive increase of concurrent access to the
                front-end tends to put strain in the database. We are striving to improve
                performance for the back-end but for now we have documented the best set-up
                providing a good compromise.</p>
            <p>These are the steps to configure Apache2 and Ceilometer API:</p></section>
        <section><title>Configure Apache2 for the Ceilometer API</title>
            <p>The first step to execute is to add the ceilometer.conf file in the
                    <b>/etc/apache2/sites-available</b> folder. </p>
            <p>The ceilometer.conf file should have the following data:</p>
            <codeblock><b>#####ceilometer.conf#####</b>
Listen &lt;ipaddress>:8777 
&lt;VirtualHost *:8777>
    WSGIDaemonProcess ceilometer user=ceilometer group=ceilometer processes=4 threads=5 home=/opt/stack/venvs/openstack python-path=/opt/stack/venvs/openstack/lib/python2.7/site-packages
    WSGIScriptAlias / /opt/stack/venvs/openstack/lib/python2.7/site-packages/ceilometer/api/app.wsgi
    SetEnv APACHE_RUN_USER ceilometer
    SetEnv APACHE_RUN_GROUP ceilometer
    WSGIProcessGroup ceilometer
    ErrorLog /var/log/apache2/ceilometer_error.log
    LogLevel info
    CustomLog /var/log/apache2/ceilometer_access.log combined
    &lt;Directory />
        Require all granted
    &lt;/Directory>
&lt;/VirtualHost></codeblock>
            <p>The Ceilometer API are running as WSGI processes. Each process can have a certain
                amount of threads taking care of the filters and applications comprising the
                processing pipeline. </p>
            <p>To increase the responsiveness you may increase the number of threads and processes
                in ceilometer.conf.</p>
            <note>WSGIDaemon Recommended Settings The best, and hence, recommended configuration is
                to have four processes running in parallel: <codeblock>processes=4</codeblock> Five
                threads for each process is also recommended: <codeblock>threads=5</codeblock>
            </note>
        </section>
        <section id="add_soft_link"><title>Add softlink for config file</title>
            <p>Next, create/add a softlink for the ceilometer.conf in the
                    <b>/etc/apache2/sites-enabled</b> folder.</p>
            <codeblock>ln -s /etc/apache2/sites-available/ceilometer.conf /etc/apache2/sites-enabled</codeblock>
        </section>
        <section id="reload_apache"><title>Reload Apache2</title>
            <p>For the changes to take effect, the apache2 service needs to be reloaded. This
                ensures that all the configuration changes are considered and the service has
                applied them. The system administrator can change the configuration of processes and
                threads and experiment if alternative settings if necessary. The command to reload
                the apache2 service is the following:</p>
            <codeblock>sudo service apache2 reload</codeblock>
        </section>
        <section id="verification"><title>Verification</title>
            <p>Once the Apache2 service has been reloaded it is possible to make sure that the
                Ceilometer APIs are running and able to receive incoming traffic. </p>
            <p>The Ceilometer APIs are listening on port 8777. The verification can be performed
                using the following command:</p>
            <codeblock>ps -ef | grep apache</codeblock> This should generate output showing Apache2
            with Ceilometer Running Instances, similar to the following if everything is in order: <codeblock>ceilome+ 31430 31427 10 16:29 ? 00:02:40 /usr/sbin/apache2 -k start
ceilome+ 31431 31427 10 16:29 ? 00:02:41 /usr/sbin/apache2 -k start
ceilome+ 31432 31427 10 16:29 ? 00:02:42 /usr/sbin/apache2 -k start
ceilome+ 31433 31427 10 16:29 ? 00:02:43 /usr/sbin/apache2 -k start</codeblock>
            <note> The list of entries in the above output should match the number of processes set
                in the configuration file, in the recommended case, 4.</note>
            <p>You may also verify that Apache2 is accepting incoming traffic on port 8777:</p>
            <codeblock>netstat -tulpn | grep 8777</codeblock>
            <p>Which should produce the following output:</p>
            <codeblock>tcp6 0 0 :::8777 :::* LISTEN 8959/apache2</codeblock>
            <note>If Ceilometer fails to deploy, check the proxy setting, and unset https_proxy
                http_proxy HTTP_PROXY HTTPS_PROXY</note>
        </section>
        <section id="notification_strategy"><title>Notification Whitelisting and Polling
                Strategies</title>
            <p>In HP Helion 2.0 we have adopted a strategy to reduce the amount of data that is sent
                to the storage. The main reason is that we are currently using a SQL based cluster
                that is not highly indicated for big data. Ceilometer can be controlled for the data
                that collects using the <b>pipeline.yaml</b> file. This configuration file is
                located in the<b> /etc/ceilometer</b> folder in any of the controller nodes.</p>
            <note>The pipeline.yaml file needs to be changed in all the controller nodes to change
                the white-listing and or polling strategy.</note>
            <p> The pipeline file is comprised of two major elements:</p>
            <ul>
                <li>Sources</li>
                <li>Sinks</li>
            </ul>
            <p> The Sources represents the data that is harvested either from notifications posted
                by services or collected through polling and the Sinks represents how the data is
                modified before is published to the internal queue for collection and storage. In
                the Sources section there is a list of meters. These meters are the data that is
                going to be collected. For a full list please refer to the Ceilometer documentation
                available at: <xref
                    href="http://docs.openstack.org/admin-guide-cloud/telemetry-measurements.html"
                    format="html" scope="external"
                    >http://docs.openstack.org/admin-guide-cloud/telemetry-measurements.html</xref>
                Here it is an example of the default <b>pipeline.yaml</b> file:</p>
            <codeblock>---
sources:
    - name: meter_source
      interval: 604800
      meters:
          - "instance"
          - "image"
          - "image.size"
          - "image.upload"
          - "image.delete"
          - "volume"
          - "volume.size"
          - "snapshot"
          - "snapshot.size"
          - "ip.floating"
          - "network.*"
          - "compute.instance.create.end"
          - "compute.instance.delete.end"
          - "compute.instance.update"
          - "compute.instance.exists"
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
          - notifier://</codeblock>
            <p>The interval attribute decides the frequency of the data polling whenever the meter
                can be polled. In general the meters that are available as notification and polling
                (indicated as both in <xref
                    href="http://docs.openstack.org/openstack-ops/content/scaling.htmlhttp://docs.openstack.org/admin-guide-cloud/telemetry-measurements.html"
                    format="html" scope="external"
                    >http://docs.openstack.org/admin-guide-cloud/telemetry-measurements.html</xref>)
                are going to be polled at the specified interval. In our setting, since we want to
                rely on notifications rather than polling, the interval is set to 604800 that
                corresponds to 1 week expressed in seconds. The combination of meters and polling
                interval is the most performing compromise that we found using the SQL cluster
                back-end.</p>
            <note>Swift account data will be collected using the polling mechanism in an hourly
                interval </note>
        </section>
        <section id="changing_meter_list"><title>Changing the List of Meters </title>
            <p>The list of meters can be easily reduced or increased editing the pipeline.yaml and
                subsequently restarting the central-agent (please see: Helion Ceilometer Runbook -
                Ceilometer Central Agent) and the Collector (please see: Helion Ceilometer Runbook -
                Ceilometer Collector). Here it is an example of compute only pipeline.yaml with the
                daily poll interval:</p>
            <codeblock>---
sources:
    - name: meter_source
      interval: 86400
      meters:
          - "instance"
          - "memory"
          - "vcpus"
          - "compute.instance.create.end"
          - "compute.instance.delete.end"
          - "compute.instance.update"
          - "compute.instance.exists"
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
          - notifier://</codeblock>
            <note> This change will cause all non-default meters to stop receiving
                notifications</note>
        </section>
        <section id="update_polling_strategy"><title>Updating the Polling Strategy and Swift
                Considerations</title>
            <p>Polling can be very taxing on the system due to the sheer amount of data that the
                system may have to ingest. It also has severe impact on queries since the database
                will now have very large amount of data to scan to respond to the query, usually
                consuming large amount of cpu and memory to satisfy the requests. Clients can also
                experience long waits for queries to come back and, in extreme cases, even timeout.
                There are 5 polling meters in Swift:</p>
            <ul>
                <li> storage.objects</li>
                <li>storage.objects.size</li>
                <li>storage.objects.containers</li>
                <li>storage.containers.objects</li>
                <li>storage.containers.objects.size </li>
            </ul>
            <p>Here it is an example of the pipeline.yaml with Swift polling on an hourly
                interval.</p>
            <codeblock>---
sources:
    - name: meter_source
      interval: 604800
      meters:
          - "instance"
          - "image"
          - "image.size"
          - "image.upload"
          - "image.delete"
          - "volume"
          - "volume.size"
          - "snapshot"
          - "snapshot.size"
          - "ip.floating"
          - "network.*"
          - "compute.instance.create.end"
          - "compute.instance.delete.end"
          - "compute.instance.update"
          - "compute.instance.exists"
      sinks:
          - meter_sink
    - name: swift_source
      interval: 3600
      meters:
          - "storage.objects"
          - "storage.objects.size"
          - "storage.objects.containers"
          - "storage.containers.objects"
          - "storage.containers.objects.size"
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
          - notifier://</codeblock>
            <p>So every time the polling interval is due for polling at least 5 messages per
                existing object/container in Swift are collected. The following table illustrate the
                amount of data will be produce hourly in different scenarios:</p>
            <table>
                <tgroup cols="4">
                    <tbody>
                        <row>
                            <entry>Swift Containers</entry>
                            <entry>Swift Objects per container</entry>
                            <entry>Samples per Hour</entry>
                            <entry>Samples stored per 24 hours</entry>
                        </row>
                        <row>
                            <entry>10</entry>
                            <entry>10</entry>
                            <entry>500</entry>
                            <entry>12000</entry>
                        </row>
                        <row>
                            <entry>10</entry>
                            <entry>100</entry>
                            <entry>5000</entry>
                            <entry>120000</entry>
                        </row>
                        <row>
                            <entry>100</entry>
                            <entry>100</entry>
                            <entry>50000</entry>
                            <entry>1200000</entry>
                        </row>
                        <row>
                            <entry>100</entry>
                            <entry>1000</entry>
                            <entry>500000</entry>
                            <entry>12000000</entry>
                        </row>
                    </tbody>
                </tgroup>


            </table>
            <p>This means that even a very small Swift storage with 10 containers and 100 files will
                store 120K samples in 24H bringing to a grand total of 3.6 million samples.</p>
            <note>The file size of each file does not have any impact on the number of samples
                collected. In fact the smaller is the number of container or files the better it is.
                So the scenario where there a lot of small files and containers is the worst.</note>
        </section>
        <section id="pipelines"><title>Separate pipeline.yaml for Ceilometer Central Agent and
                Ceilometer Notification Agent</title>
            <ul>
                <li>Ceilometer Central Agent and Ceilometer Notification agent use different
                    pipeline.yaml to configure meters that are fetched via polling and using
                    notifications. This was done to prevent accidently polling for meters which can
                    be retrieved by polling agent as well as the notification agent. (e.g glance
                    image and image.size are meters which are both of pollster and notification
                    type)</li>
                <li>Ceilometer Central Agent's (polling agent) pipeline.yaml is located at
                        <b>/opt/stack/service/ceilometer-common/etc/pipeline-agent-central.yaml
                        </b><p>Here is an example of the pipeline-agent-central</p>
                    <codeblock>---
sources:
    - name: meter_source
      interval: 600
      meters:
          - "!*"
      resources:
      discovery:
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
         - notifier://</codeblock>
                </li>
                <li> Ceilometer Notification Agent (notification agent) <b>pipeline.yaml </b>is
                    located at <b>/opt/stack/service/ceilometer-common/etc/pipeline.yaml </b><p>
                        Here is an example of the default pipeline.yaml file:</p>
                    <codeblock>---
sources:
    - name: meter_source
      interval: 30
      meters:
          - "instance"
          - "image"
          - "image.size"
          - "image.upload"
          - "image.delete"
          - "volume"
          - "volume.size"
          - "snapshot"
          - "snapshot.size"
          - "ip.floating"
          - "network"
          - "network.create"
          - "network.update"
      resources:
      discovery:
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
         - notifier://</codeblock>
                </li>
            </ul>
        </section>
        <section id="ceilometer_api"><title>Ceilometer API</title>
            <b>Separate pipeline.yaml for Ceilometer Post Sample API</b>
            <p>Post Sample API is disabled by default in HOS CM 2.0 and its configured with a
                pipeline configuration different than the agents. The api pipeline has no meters
                enabled, so it needs to be configured with meters when the Post sample API is
                enabled. Exercise caution when adding meters to the API pipeline. Ensure that only
                those meters are added to this pipeline which are already present in the
                notification agent and the central agent pipeline. Here is a sample of the API
                pipeline:</p>
            <codeblock>---
sources:
    - name: meter_source
      interval: 600
      meters:
          - "!*"
      resources:
      discovery:
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
         - notifier://</codeblock>
        </section>
        <section id="ceilomter_sample_API"><title>Ceilometer Sample API</title>
            <p>Ceilometer query-sample API uses anonymous/alias table to cache the JOIN query
                results. When the filter parameters are not sufficiently provided, the result set
                can grow large, to the point of exceeding the temp table space available on the
                mysql database instance. If this happens, an exception is thrown, halting the query
                execution. If the resource and sample tables have close to 80~100K records, the
                suggestion is to use filter conditions or in case of wanting to get all the data,
                the temp table size should be increased to avoid query interruption.</p>
            <codeblock>ceilometer sample-list --meter image.serve -q 'resource_id=a1ec2585'</codeblock>
            <codeblock>ceilometer query-samples --filter '{"and": [{"=": {"counter_name":"cpu_util"}}, {"=": {"project":"7d4af57f557547b4a28b40b054d6ffb2"}}]}'</codeblock>
        </section>
        <section id="ceilometer_stats"><title>Ceilometer Statistics API</title>
            <p>Ceilometer Statistics is an open ended query and can put a significant load on the
                database leading to unexpected results and or failures. Ceilometer Statistics API
                does a query on sample table and obtains the minimum and maximum timestamp for the
                meter that is being queried. Based on the minimum and maximum timestamp it returns a
                few statistics If a period parameter is not supplied it simply returns few
                statistics like min, max, avg and sum If a period parameter is supplied then it
                divides the range into equal periods and finds the min, max, avg and sum for each of
                the periods It is recommended that you should always provide a query parameter with
                timestamp>={$start-timestamp} and timestamp&lt;{$end-timestamp} to cover a time
                range of at the most 1 day (24 hours) irrespective of whether you provide a period
                or or not. Example: With period parameter</p>
            <codeblock>ceilometer statistics -q "timestamp&gt;=2014-12-11T00:00:10;timestamp&lt;2014-12-11T23:00:00" -m "instance" -p 3600</codeblock>
            <p>Without period parameter:</p>
            <codeblock>ceilometer statistics -q "timestamp>=2014-12-11T00:00:10;timestamp&lt;2014-12-11T23:00:00" -m "instance"</codeblock>
            The recommended values for query (-q) parameter and period (-p) parameter are </section>
        <section id="query_params"><b>query parameter (-q):</b> Always provide a timestamp range which restricts the
            query to one day (24 hours). Providing a timestamp range more than a day or not at all
            providing the time stamp range as a query parameter is not recommended e.g. -q
            "timestamp>=2014-12-11T00:00:10;timestamp&lt;2014-12-11T23:00:00" If query parameter and
            timestamp is not provided, it will end up querying all the records in the database which
            is not recommended </section>
        <section><b>period parameter(-p):</b> Provide a fairly large number for period, the
            recommended value 3600 or more (1 hour or more). Providing a period of less than 3600 is
            not recommended. e.g. -p 3600 Period parameter (value in seconds) is used to divide the
            overall time range into intervals. A small period value will translate into huge number
            of queries against the database which is not recommended </section>
        <section id="alarm_post-meters_APIs"><title>Alarm API and Post Meters API disabled</title>
            <ul>
                <li>Ceilometer Alarms are disabled</li>
                <li>Post Meters API is also disabled </li>
                <li>Custom rule hp_disabled_rule:not_implemented is added to each of those APIs in
                    ceilometer's <b>policy.json
                    </b>.<codeblock>{
"context_is_admin": "role:admin",
"context_is_project": "project_id:%(target.project_id)s",
"context_is_owner": "user_id:%(target.user_id)s",
"segregation": "rule:context_is_admin",
 
"telemetry:create_samples": "hp_disabled_rule:not_implemented",
 
"telemetry:get_alarm": "hp_disabled_rule:not_implemented",
"telemetry:change_alarm": "hp_disabled_rule:not_implemented",
"telemetry:delete_alarm": "hp_disabled_rule:not_implemented",
"telemetry:alarm_history": "hp_disabled_rule:not_implemented",
"telemetry:change_alarm_state": "hp_disabled_rule:not_implemented",
"telemetry:get_alarm_state": "hp_disabled_rule:not_implemented",
"telemetry:create_alarm": "hp_disabled_rule:not_implemented",
"telemetry:get_alarms": "hp_disabled_rule:not_implemented",
 
"default": ""
}</codeblock>
                </li>
                <li>Accessing any of the Alarm APIs or Post Meter API will result in HTTP response
                    501 Not Implemented</li>
                <li>The following Alarm APIs are disabled
                    <codeblock>POST /v2/alarms
GET /v2/alarms
GET /v2/alarms/(alarm_id)
PUT /v2/alarms/(alarm_id)
DELETE /v2/alarms/(alarm_id)
GET /v2/alarms/(alarm_id)/history
PUT /v2/alarms/(alarm_id)/state
GET /v2/alarms/(alarm_id)/state
POST /v2/query/alarms
POST /v2/query/alarms/history</codeblock>
                </li>
                <li>Post Meters API is disabled <codeblock>POST /v2/meters/(meter_name)</codeblock>
                </li>
                <li>To manually enable any of the APIs remove the corresponding rule and restart
                    Apache</li>
            </ul>
        </section>
        <section id="failover_support"><title>Failover Support (HA)</title>
            <p>In the Helion environment, Ceilometer supports native Active-Active HA for
                notification agent and central agent. Agent HA support includes
                workload-balancing/distribution and failover. Tooz is the coordination engine that
                is used to coordinate workload among mulltiple active agent instances and maintain
                the knowledge of active instance to handle failover and group membership using
                hearbeats(pings). Zookeeper is the coordination backend used but the internals of
                that is encapsulated using Tooz which exposes APIs to manage group membership and
                retrieve workload specific to each agent. The following section in conf is used to
                configure HA:</p>
            <codeblock>[coordination]
backend_url = &lt;IP address of Zookeeper host: port> (port is usually 2181 as a zookeeper default)
heartbeat = 1.0
check_watchers = 10.0</codeblock>
            <p>For the notification agent to be configured in HA mode, additional configuration is
                needed:</p>
            <codeblock>[notification]
workload_partitioning = true</codeblock>
            <p>The notification agent HA distributes workload among multiple queues that are created
                based on the number of unique source:sink combinations in the notification agent
                pipeline configuration file. If there are additional services to be metered using
                notifications, then the recommendation is to use a separate source for those events,
                especially if the expected load of data from that source is considered high. This
                should lead to better workload balancing among multiple active notification
                agents.</p>
            <p>Ceilometer-expirer is also Active-Active HA, but the key thing is to ensure that a
                single expirer process runs when multiple processes are scheduled to run at same
                time (using cron-based scheduling) on multiple controller nodes. Tooz is used to
                pick an expirer process that acquires a lock when there are multiple contenders and
                the winning process runs. There is no failover support, as expirer is not a daemon
                anyway and is scheduled to run at pre-determined intervals. The following
                configuration is needed to enable expirer HA:</p>
            <codeblock>[database]
expirer_ha = true
expirer_ha_backend_url = &lt;IP address of coordination backend url></codeblock>
        </section>
    </body>
</topic>
