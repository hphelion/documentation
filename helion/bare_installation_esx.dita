<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="install_esx">
  <title>Deploying ESX Cloud </title>
  <body>
    <p>This page describes the procedure to deploy ESX cloud using input model.</p>
    <section>
      <note type="important">
        <p>Before you start your ESX cloud deployment ensure that you read the following
          instructions carefully. </p>
      </note>
    </section>
    <section conref="bare_installation_kvm.dita#install_kvm/important-notes"/>
    <section conref="bare_installation_kvm.dita#install_kvm/Prereqs"/>
    <section id="Prereqs">
      <title>Prerequisite</title>
      <p>ESX/vCenter integration is not fully automatic, vCenter administrators are advised of the
        following responsibilities to ensure secure operation:</p>
      <p>
        <ul id="ul_shv_s4b_2t">
          <li>The VMware administrator is responsible for administration of the vCenter servers and
            the ESX nodes using the VMware administration tools. These responsibilities include: <ul
              id="ul_py3_j5l_ft">
              <li>Installing and configuring vCenter Server</li>
              <li>Installing and configuring ESX server and ESX cluster</li>
              <li>Installing and configuring shared datastores</li>
              <li>Establishing network connectivity between the ESX network and the HP Helion
                management network</li>
            </ul></li>
          <li>The VMware administration staff is responsible for the review of vCenter logs. These
            logs are not automatically included in Helion centralized logging.</li>
          <li>Logging levels for vCenter should be set appropriately to prevent logging of the
            password for the Helion message queue.</li>
          <li>The vCenter cluster and ESX Compute nodes must be appropriately backed up.</li>
          <li>Backup procedures for vCenter should ensure that the file containing the Helion
            configuration as part of Nova and Cinder volume services is backed up and the backups
            are protected appropriately.</li>
          <li>Since the file containing the Helion message queue password could appear in the swap
            area of a vCenter server, appropriate controls should be applied to the vCenter cluster
            to prevent discovery of the password via snooping of the swap area or memory dumps</li>
        </ul>
      </p>
    </section>
    <section id="deployCloud"><b>Deploy ESX Cloud</b><p>At a high level, here are the steps to
        configure and deploy ESX cloud:</p><p><image href="../media/esx/esx_deploy.jpg"
          id="image_kjt_zlm_ft"/></p></section>
    <section>
      <title>Procedure to Deploy ESX cloud</title>
    </section>
    <p>The following topics in this section explain how to deploy ESX cloud.</p>
    <section conref="bare_installation_kvm.dita#install_kvm/DeployerInstall"/>
    <section conref="bare_installation_kvm.dita#install_kvm/HLM_Node_Personalization"/>
    <section id="Configure">
      <title>Prepare and Deploy Cloud Controllers</title>
      <ol id="ol_c1w_pfl_ft">
        <li>See a sample set of configuration files in the
            <codeph>~/helion/examples/one-region-poc-with-esx</codeph> directory. The accompanying
          README.md file explains the contents of each of the configuration files. </li>
        <li>Copy the example configuration files into the required setup directory and edit them as
          required:
          <codeblock>cp -r ~/helion/examples/one-region-poc-with-esx/* ~/helion/my_cloud/definition/</codeblock></li>
        <li>The configuration files for editing are available at
            <codeph>~/helion/my_cloud/definition/</codeph>. Refer to the <b><xref
              href="input_model.dita">Helion OpenStack 2.0 Input Model</xref></b> document for
          assistance with the configuration files. <note type="important">If you chose to use your
            first controller node as your deployer, ensure that your
              <codeph>baremetalConfig.yml</codeph> file contains the <codeph>is_deployer:
              true</codeph> notation in your controller options. If you are using a dedicated
            deployer node you can omit this. Here is an example snippet of a
              <codeph>baremetalConfig.yml</codeph> file where a user is using their first controller
            node as their deployer:
            <codeblock>
node_name: "ccn-0001"
role: "ROLE-CONTROLLER"
pxe_mac_addr: "b2:72:8d:ac:7c:6f"
pxe_interface: "eth2"
pxe_ip_addr: "192.168.10.3"
ilo_ip: "192.168.9.3"
ilo_user: "admin"
ilo_password: "password"
<b>is_deployer: true</b></codeblock></note></li>
        <li>Modify the <codeph>~/helion/hos/ansible/hlm-deploy.yml</codeph> to uncomment the line
          containing <codeph>eon-deploy.yml</codeph>.</li>
        <li>Modify the <codeph>neutron.conf.j2</codeph> at
              <codeph>~/helion/my_cloud/config/<codeph>neutron/neutron.conf.j2</codeph></codeph>with
          the following values:<codeblock>router_distributed = False</codeblock><p> And add
              <codeph>mechanism_drivers = ovsvapp</codeph> at the end of the
              <codeph>neutron.conf.j2</codeph>.</p></li>
        <li>Modify <codeph>ml2_conf.ini.j2</codeph> at
              <codeph>~/helion/my_cloud/config/<codeph>neutron/ml2_conf.ini.j2</codeph></codeph>with
          the following
          values:<codeblock>[ml2]
mechanism_drivers = ovsvapp, openvswitch, l2population
[agent]
enable_distributed_routing = False</codeblock></li>
        <li>Commit your cloud deploy configuration to the<xref
            href="using_git.dita#topic_u3v_1yz_ct"> local git repo</xref>, as follows:
          <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config"</codeblock></li>
      </ol>
    </section>
    <section conref="bare_installation_kvm.dita#install_kvm/CobblerDeploy"/>
    <section conref="bare_installation_kvm.dita#install_kvm/NodeProvision"/>
    <section conref="bare_installation_kvm.dita#install_kvm/run-config-processor"/>
    <section conref="bare_installation_kvm.dita#install_kvm/deploy"/>
    <section id="prepAndDeploy"><b>Prepare and Deploy ESX Computes and OVSvAPPs</b>
      <p>The following sections describe the procedure to install and configure ESX compute and
        OVSvAPPs on vCenter.</p>
      <ul id="ul_lns_fjl_ft">
        <li><xref href="#install_esx/deploy-template" format="dita">Deploy Helion Linux Shell VM
            Template</xref></li>
        <li><xref href="#install_esx/prepare-esx-cloud-deployment" format="dita">Preparation for ESX
            Cloud Deployment</xref></li>
      </ul>
    </section>
    <section id="deploy-template"><b>Deploy Helion Linux Shell VM Template</b><p>The first step in
        deploying the ESX compute proxy and OVSvAPPs is to create a VM template that will make it
        easier to deploy the ESX compute proxy for each Cluster and OVSvAPPs on each ESX server.
        </p><p>Perform the following steps to deploy a template:<ol id="ol_qdt_ljs_ft">
          <li>Import the <codeph>hlm-shell-vm.ova</codeph> in the vCenter using the vSphere client. </li>
          <li>In the vSphere Client, click <b>File</b> and then click <b>Deploy OVF
            Template</b></li>
          <li>Follow the instructions in the wizard to specify the data center, cluster, and node to
            install. Refer to the VMWare vSphere documentation as needed.</li>
        </ol></p></section>
    <section id="prepare-esx-cloud-deployment"><b>Preparation for ESX Cloud Deployment</b><p>This
        section describes the procedures to prepare and deploy the ESX computes and OVSvAPPs for
        deployment. <ol id="ol_xpc_zqs_ft">
          <li>Login to the deployer node.</li>
          <li>Source <codeph>service.osrc</codeph>.</li>
          <li><xref href="#install_esx/register-vcenter" format="dita">Register a vCenter
              Server</xref></li>
          <li><xref href="#install_esx/register-network" format="dita">Register ESX Cloud Network
              Configuration</xref></li>
          <li><xref href="#install_esx/import-cluster" format="dita">Import Clusters</xref></li>
          <li><xref href="#install_esx/activate-cluster" format="dita">Activate Clusters</xref></li>
          <li><xref href="#install_esx/modify-volume-config" format="dita">Modify the Volume
              Configuration File</xref></li>
          <li><xref href="#install_esx/commit-your-cloud" format="dita">Commit your Cloud
              Definition</xref></li>
          <li><xref href="#install_esx/deploy-compute-proxy-ovsvapps" format="dita">Deploy ESX
              Compute Proxy and OVSvApps</xref></li>
        </ol></p></section>
    <p><b>Manage vCenters and Clusters</b></p>
    <p>The following section describes the detailed procedure on managing the vCenters and
      clusters.</p>
    <p id="register-vcenter"><b>Register a vCenter Server</b></p>
    <p>vCenter provides centralized management of virtual host and virtual machines from a single
        console.<ol id="ol_xdj_5js_ft">
        <li>Add a vCenter using EON python
            client.<codeblock><codeph># eon vcenter-add --name &lt;vCenter Name> --ip-address &lt;vCenter IP address> --username &lt;vCenter Username> --password &lt;vCenter Password> --port &lt;vCenter Port></codeph></codeblock><p>where:
              <ul id="ul_bp3_yjs_ft">
              <li>vCenter Name - the identical name of the vCenter server.</li>
              <li>vCenter IP address - the IP address of the vCenter server.</li>
              <li>vCenter Username - the admin privilege username for the vCenter.</li>
              <li>vCenter Password - the password for the above username.</li>
              <li>vCenter Port - the vCenter server port. By default it is 443. <note
                  type="important">Please do not change the port unless you are sure about the
                  vCenter Port.</note></li>
            </ul></p><p><b>Sample
            Output:</b><codeblock><codeph># eon vcenter-add --name vc01 --ip-address 10.1.200.41 --username administrator@vsphere.local --password password --port 443</codeph>
+------------+--------------------------------------+
| Property   | Value                                |
+------------+--------------------------------------+
| created_at | 2015-08-20T12:08:09.000000           |
| deleted    | False                                |
| deleted_at | None                                 |
| id         | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| ip_address | 10.1.200.41                          |
| name       | vc01                                 |
| password   | &lt;SANITIZED>                          |
| port       | 443                                  |
| type       | vcenter                              |
| updated_at | 2015-08-20T12:08:09.000000           |
| username   | administrator@vsphere.local          |
+------------+--------------------------------------+</codeblock></p></li>
      </ol></p>
    <section><b>Show vCenter</b><ol id="ol_wh5_rsb_lt">
        <li>Show vCenter using EON python
              client.<codeblock># eon vcenter-show &lt;vCenter ID></codeblock><p><b>Sample
              Output:</b><codeblock><codeph># eon vcenter-show BC9DED4E-1639-481D-B190-2B54A2BF5674 </codeph>
+------------+--------------------------------------+
| Property   | Value                                |
+------------+--------------------------------------+
| created_at | 2015-08-20T12:08:09.000000           |
| datacenters| DC1                                  |
| deleted    | False                                |
| deleted_at | None                                 |
| id         | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| ip_address | 10.1.200.41                          |
| name       | vc01                                 |
| password   | &lt;SANITIZED>                          |
| port       | 443                                  |
| type       | vcenter                              |
| updated_at | 2015-08-20T12:08:09.000000           |
| username   | administrator@vsphere.local          |
+------------+--------------------------------------+</codeblock></p></li>
      </ol></section>
    <p id="register-network"><b>Register ESX Cloud Network Configuration</b></p>
    <p>This involve getting a sample network information template. Fill the details of the template
      and use that template to register cloud network configuration for the vCenter.</p>
    <p>
      <ol>
        <li>Execute the following command to get the network information
            template:<codeblock><codeph># eon get-network-info-template --filename &lt;<b>NETWORK_CONF_FILENAME</b>></codeph></codeblock><p>For
            example:<codeblock># eon get-network-info-template --filename net_conf.json</codeblock></p><p>Sample
            file of <codeph>net_conf.json</codeph> is shown
            below:<codeblock>{
    "network": {
        # Deployer Network details
        # This network should be reachable from the Deployer node
        "deployer_network": {
            #Deployer Portgroup Name.
            "deployer_pg_name": "hlm-Deployer-PG",

            #VLAN id for Deployer Portgroup
            "deployer_vlan": "33",

            #Enable DHCP for Deployer N/W
            "enable_deployer_dhcp": "no",

            #CIDR and gateway for deployer network only when static
            "deployer_cidr": "10.20.18.0/23",
            "deployer_gateway_ip": "10.20.18.1",

            #Deployer Node's PXE IP Address
            "deployer_node_ip": "10.20.16.2"
        },

        #Management Network details
        "management_network": {
            #Mgmt DVS name.
            "mgmt_dvs_name": "hlm-Mgmt",

            #Physical NIC name for Mgmt DVS
            "mgmt_nic_name": "vmnic3",

            #Mgmt Portgroup Name.
            "mgmt_pg_name": "hlm-Mgmt-PG",

            #Interface order: Example eth1
            "mgmt_interface_order": "eth1",

            "active_nics": "",

            "load_balancing": "1",

            "network_failover_detection": "1",

            "notify_switches": "yes"
        },

        "data_network": {
            #Tenant network type
            "tenant_network_type": "vlan",

            "data_dvs_name": "hlm-Data",

            "data_nic_name": "vmnic2, vmnic1",

            #Data Portgroup Name.
            "data_pg_name": "hlm-Data-PG",

            #Interface order: Example eth2
            "data_interface_order": "eth2",

            #If more than one mgmt_nic_name are specified then NIC teaming will be enabled by default
            #Active uplink NICs for NIC teaming(If this field is empty, first data_nic_name will be "Active" and the rest will be in "Standby")
            "active_nics": "vmnic2",

            #Load Balancing. Please choose the corresponding number
            #    1 -> Route based on the originating virtual port
            #    2 -> Route based on IP hash
            #    3 -> Route based on source MAC hash
            #    4 -> Route based on physical NIC load
            #    5 -> Use explicit failover order
            "load_balancing": "1",

            #Network Failover Detection. Please choose the corresponding number
            #    1 -> Link Status
            #    2 -> Beacon Probing
            "network_failover_detection": "1",

            #Notify Switches(yes/no)
            "notify_switches": "yes"
        },

        "hpvcn_trunk_network": {
            #Trunk DVS name
            "trunk_dvs_name": "hlm-Trunk",

            #Trunk Portgroup Name.
            "trunk_pg_name": "hlm-Trunk-PG",

            #Interface order: Example eth3
            "trunk_interface_order": "eth3"
        },

        #VLAN Range for Data &amp; Trunk port group. Please provide the range separated by a hyphen(vlan-vlan).
        #Multiple vlan or vlan ranges has to be a comma separated value(*OPTIONAL)
        "vlan_range": "1-4094"
    },

    "template": {
        #Provide the template/appliance name that will be used for cloning Computeproxy and OVSvApp VMs
        "template_name": "hlm-template"
    },

    "vmconfig": {
        #Number of CPUs for OVSvApp/Computeproxy VM
        "cpu": "4",

        #Amount of RAM in MB
        "memory_in_mb": "4096",

        #SSH public key content for OVSvAPP/Computeproxy password less login.
        "ssh_key": "&lt;deployer-ssh-pub-key-contents>"
    },
        # Do you want to skip inactive or maintenance mode hosts ?
    "skip_inactive_hosts": "yes",
        # Provide new host mo ids when you are adding a new host in an activated cluster.
    "new_host_mo_ids": ""
}</codeblock></p></li>
        <li>Modify the template (json file) as per your
            environment.<codeblock>vi &lt;<b>NETWORK_CONF_FILENAME</b>></codeblock><p>For
            example:<codeblock>vi net_conf.json</codeblock></p></li>
        <li>Use the template to register Cloud Network Configuration. This sets the network
          information for a vCenter which is used to deploy and configure compute proxy and OVSvAPP
          VMs during the cluster activation.
            <codeblock><codeph># eon set-network-info --vcenter-id &lt;vCenter ID> --datacenter-name &lt;datacenter name> --config-json &lt;NETWORK_CONF_FILENAME></codeph></codeblock><p>For
            example:
            <codeblock># eon set-network-info --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674 --datacenter DC1 --config-json net_conf.json</codeblock></p><p>
            <note>The vcenter ID is generated when you execute the above (<b>step 2</b>)
              command.</note>
          </p></li>
        <li>Execute the following command to view the list of clusters for the given
            vCenter.<codeblock><codeph># eon cluster-list --vcenter-id &lt;vCenter ID></codeph></codeblock><b>Sample
            Output</b><codeblock># eon cluster-list --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674
+------------+----------+------------+---------------+
| MOID       | Name     | Datacenter | Import Status |
+------------+----------+------------+---------------+
| domain-c21 | Cluster1 | DC1        | not_imported  |
+------------+----------+------------+---------------+</codeblock></li>
      </ol>
    </p>
    <p id="import-cluster"><b>Import Cluster</b>
    </p>
    <p>You can use one or more ESX clusters for ESX Cloud Deployment. When a Import Cluster is
      invoked, required ESX Compute Proxy and OVSvApp nodes are deployed.</p>
    <p>
      <ol>
        <li> Import the cluster for the EON database under the given vCenter. <codeblock><codeph># eon cluster-import --vcenter-id &lt;vCenter ID> --cluster-name &lt;Cluster Name> --cluster-moid &lt;Cluster Moid></codeph></codeblock>where:<p>
            <ul id="ul_r4g_rjs_ft">
              <li>vCenter ID - ID of the vcenter containing the cluster.</li>
              <li>Cluster Name - the name of the cluster that needs to be imported.</li>
              <li>cluster Moid - Moid of the cluster that needs to be imported.</li>
            </ul>
          </p><p><b>Sample
            Output</b><codeblock># eon cluster-import --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674 --cluster-name Cluster1 --cluster-moid domain-c21
+--------------+-----------+
| Property     | Value     |
+--------------+-----------+
| cpu_free     | 83071.73  |
| cpu_total    | 83072     |
| cpu_used     | 0.27      |
| datacenter   | DC1       |
| disk_free    | 1022.79   |
| disk_total   | 1023.75   |
| errors       | []        |
| memory_free  | 496.82    |
| memory_total | 511.76    |
| memory_used  | 14.94     |
| name         | Cluster1  |
| state        | importing |
| switches     | []        |
+--------------+-----------+</codeblock></p><p>One
            vCenter can have multiple clusters. But it allows you to import only one cluster at a
            time.</p></li>
        <li> Execute the following command to view the list of clusters for the given
            vCenter.<codeblock><codeph># eon cluster-list --vcenter-id &lt;vCenter ID></codeph></codeblock><b>Sample
            Output</b><codeblock># eon cluster-list --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674
+------------+----------+------------+---------------+
| MOID       | Name     | Datacenter | Import Status |
+------------+----------+------------+---------------+
| domain-c22 | Cluster2 | DC1        | imported      |
+------------+----------+------------+---------------+</codeblock></li>
      </ol>
    </p>
    <p id="activate-cluster"><b>Activate Clusters</b><note>You can activate the cluster only after
        the import status of the cluster is changed to <b>imported</b>.<p>When you execute the
          active cluster command, the <codeph>server.yml</codeph> of the input model is updated with
          IP Addresses of compute proxy and OVSvApp.
          <!--VMs correspesponding to the clusters are being activated.--></p></note></p>
    <p>
      <ol>
        <li>Activate the cluster for the selected
              vCenter.<codeblock><codeph># eon cluster-activate --vcenter-id &lt;vCenter ID> --cluster-moid &lt;Cluster Moid> </codeph></codeblock><p><b>Sample
              Output</b></p><p>
            <codeblock># eon cluster-activate --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674 --cluster-moid domain-c22 
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Property      | Value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| node_info     | {u'computeproxy': {u'pxe-mac-addr': u'00:50:56:b6:ce:1b', u'pxe-ip-addr': u'172.170.2.4', u'name': u'COMPUTEPROXY_Cluster1', u'cluster-moid': u'domain-c21'}, u'network_driver': {u'cluster_dvs_mapping': u'DC1/host/Cluster1:hlm-Trunk', u'Cluster1': [{u'host-moid': u'host-29', u'pxe-ip-addr': u'172.170.2.3', u'esx_hostname': u'10.1.200.33', u'ovsvapp_node': u'ovsvapp-10-1-200-33', u'pxe-mac-addr': u'00:50:56:b6:5e:9a'}, {u'host-moid': u'host-25', u'pxe-ip-addr': u'172.170.2.2', u'esx_hostname': u'10.1.200.66', u'ovsvapp_node': u'ovsvapp-10-1-200-66', u'pxe-mac-addr': u'00:50:56:b6:56:e6'}]}} |
| resource_moid | domain-c21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| resource_name | Cluster1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| state         | activated                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</codeblock>
          </p></li>
      </ol>
    </p>
    <p id="modify-volume-config"><b>Modify the Volume Configuration File</b></p>
    <p>Once the cluster is activated you must configure the volume.</p>
    <p>Perform the following steps to modify the volume configuration files:</p>
    <p>
      <ol>
        <li>Change the directory. The <codeph>cinder.conf.j2</codeph> is present in following
          directories
            :<codeblock>cd /home/stack/helion/hos/ansible/roles/_CND-CMN/templates</codeblock><p>OR<codeblock>cd /home/stack/helion/my_cloud/config/cinder</codeblock></p><p>It
            is recommended to modify the <codeph>cinder.conf.j2</codeph> present in
              <codeph>/home/stack/helion/my_cloud/config/cinder</codeph></p></li>
        <li>Modify the <codeph>cinder.conf.j2</codeph> as follows:
          <codeblock># Start of section for VMDK block storage
#
# If you have configured VMDK Block storage for cinder you must
# uncomment this section, and replace all strings in angle brackets
# with the correct values for vCenter you have configured. You
# must also add the section name to the list of values in the
# 'enabled_backends' variable above. You must provide unique section
# each time you configure a new backend.
#
#[&lt;unique-section-name>]
#vmware_api_retry_count = 10
#vmware_tmp_dir = /tmp
#vmware_image_transfer_timeout_secs = 7200
#vmware_task_poll_interval = 0.5
#vmware_max_objects_retrieval = 100
#vmware_volume_folder = cinder-volumes
#volume_driver = cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver
#vmware_host_ip = &lt;ip_address_of_vcenter>
#vmware_host_username = &lt;vcenter_username>
#vmware_host_password = &lt;password>
#
#volume_backend_name = &lt;vmdk-backend-name>
#
# End of section for VMDK block storage</codeblock></li>
      </ol>
    </p>
    <p id="commit-your-cloud"><b>Commit your Cloud Definition</b></p>
    <p>
      <ol>
        <li> Add the cloud deployment definition to the git
          :<codeblock>cd /home/stack/helion/hos/ansible;
git add -A;
git commit -m 'Adding ESX Configurations';</codeblock></li>
        <li>Prepare your environment for deployment:
          <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml;
ansible-playbook -i hosts/localhost ready-deployment.yml;
cd /home/stack/scratch/ansible/next/hos/ansible;</codeblock></li>
      </ol>
    </p>
    <p id="deploy-compute-proxy-ovsvapps"><b>Deploy ESX Compute Proxy and OVSvApps</b></p>
    <p>Execute the following command to deploy a esx compute and
      OVSvApps:<codeblock>ansible-playbook -i hosts/verb_hosts guard-deployment.yml
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit '*<b>esx-ovsvapp:*esx-compute</b>'Â 
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml --limit NOV-ESX:NEU-OVSVAPP
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></p>
    <note>The variable <b>esx-ovsvapp</b> and <b>esx-compute</b> must be taken from the <b>name</b>
      key in the <codeph>resource-nodes</codeph> section in the
        <codeph>/data/control_plane.yml</codeph> file
        (<codeph>/home/stack/helion/my_cloud/definition/data/control_plane.yml</codeph>). </note>
  </body>
</topic>
