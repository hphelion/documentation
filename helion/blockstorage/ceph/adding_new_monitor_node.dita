<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_zbc_pg4_rt">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Adding a New Ceph Monitor Node</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <note type="attention">Hyperlinks intermittently do not work in the Google Chrome browser. <xref
        href="http://docs.hpcloud.com/helion/blockstorage/ceph/adding_new_monitor_node.html"
        scope="external" format="html">Click here</xref> for a frameless version of this page where
      the links should work.</note>
    <section id="about">
      <p>In <keyword keyref="kw-hos-phrase"/>, a Ceph monitor node is installed on the controller nodes by
        default. A new monitor node cannot be added to an existing Ceph cluster if Ceph is deployed
        with the default configuration. However, if monitor nodes are added on a dedicated resource
        node, then you can add a new monitor node.</p>
      <p>If a monitor resource is not already defined, you will need to first define one and then
        add the new monitor node. For instructions, see <xref
          href="defining_monitor_resource.dita#topic_owr_kf4_rt"/>.</p>
      <p>The steps involved are:</p>
      <ol>
        <li><xref href="adding_new_osd_node.dita#add_osdnode/prereqs">Prerequisites</xref></li>
        <li><xref href="adding_new_osd_node.dita#add_osdnode/steps">Adding a Ceph Monitor
            Node</xref></li>
        <li><xref href="adding_new_osd_node.dita#add_osdnode/verify">Verifying the New Ceph Monitor
            Node</xref></li>
        <li><xref href="adding_new_osd_node.dita#add_osdnode/monitoring">Adding a New Ceph Monitor
            Node to Monitoring</xref></li>
      </ol>
    </section>

    <section id="prereqs"><title>Prerequisites</title>
      <p>You should have one or more baremetal servers that meet the minimum hardware requirements
        for a Ceph node which are documented in the <xref href="../../hardware.dita">Hardware
          Support Matrix</xref>.</p>
    </section>

    <section id="steps"><title>Adding a Ceph Monitor Node</title>
      <p>These steps will show you how to add the new Ceph monitor node to your
          <codeph>servers.yml</codeph> file and then run the playbooks that update your cloud
        configuration. You will run these playbooks from the lifecycle manager.</p>
      <ol>
        <li>Log in to your lifecycle manager.</li>
        <li>Checkout the <codeph>site</codeph> branch of your local git so you can begin to make the
          necessary edits:
          <codeblock>cd ~/helion/my_cloud/definition/data
git checkout site</codeblock></li>
        <li>In the same directory, edit your <codeph>servers.yml</codeph> file to include the Ceph
          monitor nodes: <codeblock>- id: mon4
ip-addr: 192.168.10.11
role: MON-ROLE
server-group: RACK3
nic-mapping: MY-2PORT-SERVER
mac-addr: 8b:f6:9e:ca:3b:7a
ilo-ip: 192.168.9.11
ilo-password: password
ilo-user: admin</codeblock>
          <p>You can find detailed descriptions of these fields <xref
              href="../../input_model.dita#input_model/co_servers">here</xref>.</p>
          <note type="important">You will need to verify that the <codeph>ip-addr</codeph> value you
            choose for this node does not conflict with any other IP address in your cloud
            environment. You can confirm this by checking the
              <codeph>~/helion/my_cloud/info/address_info.yml</codeph> file on your lifecycle
            manager.</note>
        </li>
        <li>In your <codeph>control_plane.yml</codeph> file you will need to check the values for
            <codeph>member-count</codeph>, <codeph>min-count</codeph>, and
            <codeph>max-count</codeph>, if you specified them, to ensure that they match up with
          your new total node count. So for example, if you had previously specified
            <codeph>member-count: 3</codeph> and are adding a fourth Ceph monitor node, you will
          need to change that value to <codeph>member-count: 4</codeph>. <p>See <xref
              href="../../input_model.dita#input_model/co_controlplane">Input Model - Control
              Plane</xref> for more details.</p></li>
        <li>Commit the changes to git:
          <codeblock>git commit -a -m "Add node &lt;name>"</codeblock></li>
        <li>Run the configuration processor:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
        <li>Run the ready deployment playbook:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
        </li>
        <li>Add the new node into Cobbler:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
        <li>Then you can image the node: <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&#60;node name></codeblock>
          <note>If you don't know the <codeph>&#60;node name></codeph> already, you can get it by
            using <codeph>sudo cobbler system list</codeph></note>
          <p>Before proceeding, you may want to take a look at <b>info/server_info.yml</b> to see if
            the assignment of the node you have added is what you expect. It may not be, as nodes
            will not be numbered consecutively if any have previously been removed. This is to
            prevent loss of data; the config processor retains data about removed nodes and keeps
            their ID numbers from being reallocated. See the Persisted Server Allocations section in
            <xref href="../../input_model.dita#input_model/persistedserverallocations"><keyword keyref="kw-hos-phrase"/>
              Input Model</xref> for information on how this works.</p></li>
        <li>Run the following series of playbooks complete the deployment of your additional Ceph
          monitor node(s) to your Ceph cluster:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts guard-deployment.yml
ansible-playbook -i hosts/verb_hosts osconfig-run.yml
ansible-playbook -i hosts/verb_hosts site.yml --limit &#60;host_var name></codeblock>
          <note>If you don't know the <codeph>&#60;host_var name></codeph> already, you can get it
            by looking in the <codeph>~/scratch/ansible/next/hos/ansible/host_vars</codeph>
            directory.</note></li>
      </ol>
    </section>

    <section id="verify"><title>Verifying the New Ceph Monitor Node</title>
      <p>From the lifecycle manager, run the following command:</p>
      <codeblock>ceph --cluster ceph -f json quorum_status</codeblock>
      <p>If the Ceph cluster name is different than the default (i.e. <b>ceph</b>), you need to
        modify the above command accordingly.</p>
      <p>The newly added Ceph monitor's hostname appears in the <codeph>quorum_names</codeph>
        section of the output of above command.</p>
    </section>

    <section id="monitoring"><title>Adding a New Ceph Monitor Node to Monitoring</title>
      <p>If you want to add a new Ceph monitor node to the monitoring service checks, there is an
        additional playbook that must be run to ensure this happens:</p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags active_ping_checks</codeblock>
    </section>
  </body>
</topic>
