<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="add_osdnode">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Adding an Object Storage Daemon (OSD) Node </title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <note type="attention">Hyperlinks intermittently do not work in the Google Chrome browser. <xref
        href="http://docs.hpcloud.com/helion/blockstorage/ceph/adding_new_osd_node.html"
        scope="external" format="html">Click here</xref> for a frameless version of this page where
      the links should work.</note>
    <section id="about">
      <p>You may have a need to add an object storage daemon (OSD) node for additional Ceph capacity
        or another purpose and these steps will help you achieve this.</p>
      <p>The steps involved are:</p>
      <ol>
        <li><xref href="adding_new_osd_node.dita#add_osdnode/prereqs">Prerequisites</xref></li>
        <li><xref href="adding_new_osd_node.dita#add_osdnode/steps">Adding an Object Storage Daemon
            (OSD) Node</xref></li>
        <li><xref href="adding_new_osd_node.dita#add_osdnode/verify">Verifying the New Object
            Storage Daemon (OSD) Node</xref></li>
        <li><xref href="adding_new_osd_node.dita#add_osdnode/monitoring">Adding a New Object Storage
            Daemon (OSD) Node to Monitoring</xref></li>
      </ol>
    </section>

    <section id="prereqs"><title>Prerequisites</title>
      <p>You should have one or more baremetal servers that meet the minimum hardware requirements
        for an object storage daemon (OSD) node which are documented in the <xref
          href="../../hardware.dita">Hardware Support Matrix</xref>.</p>
    </section>

    <section id="steps"><title>Adding an Object Storage Daemon (OSD) Node</title>
      <p>These steps will show you how to add the new OSD node to your <codeph>servers.yml</codeph>
        file and then run the playbooks that update your cloud configuration. You will run these
        playbooks from the lifecycle manager.</p>
      <ol>
        <li>Log in to your lifecycle manager.</li>
        <li>Checkout the <codeph>site</codeph> branch of your local git so you can begin to make the
          necessary edits:
          <codeblock>cd ~/helion/my_cloud/definition/data
git checkout site</codeblock></li>
        <li>In the same directory, edit your <codeph>servers.yml</codeph> file to include the
          details about your new OSD node(s). <p>For example, if you already had a cluster of three
            OSD nodes and needed to add a fourth one you would add your details to the bottom of the
            file in this format:</p>
          <codeblock># Ceph OSD Nodes
- id: osd4
  ip-addr: 192.168.10.9
  role: OSD-ROLE
  server-group: RACK1
  nic-mapping: MY-2PORT-SERVER
  mac-addr: 8b:f6:9e:ca:3b:78
  ilo-ip: 192.168.9.9
  ilo-password: password
  ilo-user: admin</codeblock>
          <p>You can find detailed descriptions of these fields <xref
              href="../../input_model.dita#input_model/co_servers">here</xref>.</p>
          <note type="important">You will need to verify that the <codeph>ip-addr</codeph> value you
            choose for this node does not conflict with any other IP address in your cloud
            environment. You can confirm this by checking the
              <codeph>~/helion/my_cloud/info/address_info.yml</codeph> file on your lifecycle
            manager.</note>
        </li>
        <li>In your <codeph>control_plane.yml</codeph> file you will need to check the values for
            <codeph>member-count</codeph>, <codeph>min-count</codeph>, and
            <codeph>max-count</codeph>, if you specified them, to ensure that they match up with
          your new total node count. So for example, if you had previously specified
            <codeph>member-count: 3</codeph> and are adding a fourth OSD node, you will need to
          change that value to <codeph>member-count: 4</codeph>. <p>See <xref
              href="../../input_model.dita#input_model/co_controlplane">Input Model - Control
              Plane</xref> for more details.</p></li>
        <li>Commit the changes to git:
          <codeblock>git commit -a -m "Add node &lt;name>"</codeblock></li>
        <li>Run the configuration processor:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
        <li>Run the ready deployment playbook:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
        </li>
        <li>Add the new node into Cobbler:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
        <li>Then you can image the node: <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&#60;node name></codeblock>
          <note>If you don't know the <codeph>&#60;node name></codeph> already, you can get it by
            using <codeph>sudo cobbler system list</codeph></note>
          <p>Before proceeding, you may want to take a look at <b>info/server_info.yml</b> to see if
            the assignment of the node you have added is what you expect. It may not be, as nodes
            will not be numbered consecutively if any have previously been removed. This is to
            prevent loss of data; the config processor retains data about removed nodes and keeps
            their ID numbers from being reallocated. See the Persisted Server Allocations section in
            <xref href="../../input_model.dita#input_model/persistedserverallocations"><keyword keyref="kw-hos-phrase"/>
              Input Model</xref> for information on how this works.</p></li>
        <li>Run the following series of playbooks complete the deployment of your additional OSD
          node(s) to your Ceph cluster:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts guard-deployment.yml
ansible-playbook -i hosts/verb_hosts osconfig-run.yml
ansible-playbook -i hosts/verb_hosts ceph-deploy.yml</codeblock></li>
      </ol>
    </section>

    <section id="verify"><title>Verifying the New OSD Node</title>
      <p>From the lifecycle manager, run the following command:</p>
      <codeblock>ceph --cluster ceph osd tree</codeblock>
      <p>If the Ceph cluster name is different than the default (i.e. <b>ceph</b>), you need to
        modify the above command accordingly.</p>
      <p>When you run the above command, if the OSD node has been successfully added to the existing
        Ceph cluster, the hostname of the newly added OSD should appear in the output.</p>
    </section>

    <section id="monitoring"><title>Adding a New OSD Node to Monitoring</title>
      <p>If you want to add a new OSD node to the monitoring service checks, there is an additional
        playbook that must be run to ensure this happens:</p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags active_ping_checks</codeblock>
    </section>

  </body>
</topic>
