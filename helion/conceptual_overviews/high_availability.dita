<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="HP2.0HA">
  <title>HPE Helion <tm tmtype="reg">OpenStack</tm> 2.0: High Availability (HA)</title>
  <body><!--Needs Edit-->
    <p>This page covers the following topics:</p>
    <ul>
      <li><xref href="#HP2.0HA/concepts_overview">High Availability Concepts Overview</xref>
        <ul>
          <li><xref href="#HP2.0HA/cloud_infrastructure">Highly Available Cloud Infrastructure</xref></li>
          <li><xref href="#HP2.0HA/tenant_workloads">Highly Available Cloud-Aware Tenant Workloads</xref></li>
        </ul>
      </li>
      <li><xref href="#HP2.0HA/highly_available_cloud_infrastructure">Highly Available Cloud Infrastructure</xref></li>
      <li><xref href="#HP2.0HA/high_availablity_controllers">High Availablity of Controllers</xref>
        <ul>
          <li><xref href="#HP2.0HA/api_request">API Request Message Flow</xref></li>
          <li><xref href="#HP2.0HA/node_failure">Handling Node Failure</xref></li>
          <li><xref href="#HP2.0HA/network_partitions">Handling Network Partitions</xref></li>
          <li><xref href="#HP2.0HA/galera_cluster">MySQL Galera Cluster</xref></li>
          <li><xref href="#HP2.0HA/singleton_services">Singleton Services</xref>
            <ul>
              <li><xref href="#HP2.0HA/cinder_volume">Cinder-Volume</xref></li>
              <li><xref href="#HP2.0HA/sherpa">Sherpa</xref></li>
              <li><xref href="#HP2.0HA/nova_consoleauth">Nova consoleauth</xref></li>
            </ul>
          </li>
          <li><xref href="#HP2.0HA/failed_controller_nodes">Rebuilding or Replacing failed Controller Nodes</xref></li>
        </ul>
      </li>
      <li><xref href="#HP2.0HA/availability_zones">Availability Zones</xref></li>
      <li><xref href="#HP2.0HA/compute_kvm">Compute with KVM</xref></li>
      <li><xref href="#HP2.0HA/nova_availability_zones">Nova Availability Zones</xref></li>
      <li><xref href="#HP2.0HA/compute_esx">Compute with ESX Hypervisor</xref></li>
      <li><xref href="#HP2.0HA/block_storage_vsa">Block Storage with StoreVirtual VSA</xref></li>
      <li><xref href="#HP2.0HA/cinder_availability_zones">Cinder Availability Zones</xref></li>
      <li><xref href="#HP2.0HA/object_storage_swift">Object Storage with Swift</xref></li>
      <li><xref href="#HP2.0HA/highly_available_app_workloads">Highly Available Cloud Applications and Workloads</xref></li>
      <li><xref href="#HP2.0HA/what_not_ha">What is not Highly Available?</xref>
        <ul>
          <li><xref href="#HP2.0HA/deployer">Deployer</xref></li>
          <li><xref href="#HP2.0HA/control_plane">Control Plane</xref></li>
        </ul>
      </li>
      <li><xref href="#HP2.0HA/more_information">More Information</xref></li>
    </ul>


    <section id="concepts_overview">
      <title>High Availability Concepts Overview</title>
      <p>Highly Available Cloud ensures that at least a minimum of cloud resources are always
        available on request, which results in uninterrupted operations for users.</p>
      <p>In order to achieve this high availability of infrastructure and workloads, we define the
        scope of HA to be limited to protecting these only against single points of failure (SPOF).
        Single points of failure include:</p>
      <ul>
        <li><b>Hardware SPOFs</b>: Hardware failures can take the form of server failures, memory going bad, power failures, 
          hypervisors crashing, hard disks dying, NIC cards breaking, switch ports failing, network cables loosening, 
          and so forth.</li>
        <li><b>Software SPOFs</b>: Server processes can crash due to software defects, out-of-memory conditions, operating system kernel panic, and so forth.</li>
      </ul>
      <p>By design, HPE Helion OpenStack strives to create a system architecture resilient to SPOFs,
        and does not attempt to automatically protect the system against multiple cascading levels
        of failures; such cascading failures will result in an unpredictable state.  Hence, the
        cloud operator is encouraged to recover and restore any failed component, as soon as the
        first level of failure occurs.</p>
 
      <sectiondiv id="cloud_infrastructure">
        <p><b>Highly Available Cloud Infrastructure</b></p>
        <p>Cloud users are able to provision and manage the compute, storage, and network
          infrastructure resources at any given point in time and the Horizon Dashboard and the
          OpenStack APIs must be reachable and be able to fulfill user requests.</p>
        <p><image href="../../media/ha/ha-resilient-cloud-infrastructure.png" id="CloudInfrastructure"/></p>
        <p>Once the Compute, Storage, and Network resources are deployed, users expect these resources to be 
          reliable in the following ways:</p>
        <ul>
          <li>If the Nova-Compute KVM Hypervisors/servers hosting the project compute virtual
            machine (VM) dies and the compute VM is lost along with its local ephemeral storage, the
            re-launching of the dead compute VM succeeds because it launches on another Nova-Compute
            KVM Hypervisor/server.  If ephemeral storage loss is undesirable, the compute VM can be
            booted from the Cinder volume.</li>
          <li>Data stored in Block Storage service volumes can be made highly-available by clustering <xref href="#HP2.0HA/block_storage_vsa">(Details below in VSA section below)</xref></li>
          <li>Data stored by the Object service is always available <xref href="#HP2.0HA/object_storage_swift">(Details in Swift section below)</xref></li>
          <li>Network resources such as routers, subnets, and floating IP addresses provisioned by the 
            Networking Operation service are made highly-available via Helion Control Plane redundancy and DVR.</li>
        </ul>
        <p>The infrastructure that provides these features is called a <b>Highly Available Cloud Infrastructure</b>.</p>
      </sectiondiv>
      
      <sectiondiv id="tenant_workloads">
        <p><b>Highly Available Cloud-Aware Tenant Workloads</b></p>
        <p>HPE Helion OpenStack Compute hypervisors do not support transparent high availability for user 
          applications; as such, the project application provider is responsible for deploying their applications 
          in a redundant and highly available manner, using multiple VMs spread appropriately across availability 
          zones, routed through the load balancers and made highly available through clustering.</p>
        <p>These are known as <b>Highly Available Cloud-Aware Tenant Workloads</b>.</p>
      </sectiondiv>
      
    </section>
    
    <section id="highly_available_cloud_infrastructure">
      <title>Highly Available Cloud Infrastructure</title>
      <p>The highly available cloud infrastructure consists of the following:</p>
      <ul>
        <li>High Availability of Controllers</li>
        <li>Availability Zones</li>
        <li>Compute with KVM</li>
        <li>Nova Availability Zones</li>
        <li>Compute with ESX</li>
        <li>Clock Storage with StoreVirtual VSA</li>
        <li>Object Storage with Swift</li>
      </ul>
    </section>
    
    <section id="high_availablity_controllers">
      <title>High Availability of Controllers</title>
      <p>The HPE Helion OpenStack installer deploys highly available configurations of 
        OpenStack cloud services, resilient against single points of failure.</p>
      <p>The high availability of the controller components comes in two main forms.</p>
      <ul>
        <li>Many services are stateless and multiple instances are run across the control 
          plane in active-active mode. The API services (nova-api, cinder-api, etc.) are 
          accessed through the HA proxy load balancer whereas the internal services 
          (nova-scheduler, cinder-scheduler, etc.), are accessed through the message 
          broker. These services use the database cluster to persist any data.
          <note>The HA proxy load balancer is also run in active-active mode and 
            keepalived (used for Virtual IP (VIP) Management) is run in active-active 
            mode, with only one keepalived instance holding the VIP at any one point 
            in time.</note></li>
        <li>The high availability of the message queue service and the database service 
          is achieved by running these in a clustered mode across the three nodes of the 
          control plane: RabbitMQ cluster with Mirrored Queues and Percona MySQL Galera cluster.</li>
      </ul>
      <p><image href="../../media/ha/ControlPlaneHA2.0_1.png" id="ControlPlane1"/></p>
      <p>The above diagram illustrates the HA architecture with the focus on VIP management 
        and load balancing. It only shows a subset of active-active API instances and does 
        not show examples of other services such as nova-scheduler, cinder-scheduler, etc.</p>
      <p>In the above diagram, requests from an OpenStack client to the API services are 
        sent to VIP and port combination; for example, 192.0.2.26:8774 for a Nova request. 
        The load balancer listens for requests on that VIP and port. When it receives a request, 
        it selects one of the controller nodes configured for handling Nova requests, in this 
        particular case, and then forwards the request to the IP of the selected controller 
        node on the same port.</p>
      <p>The nova-api service list, which is listening for requests on the IP of its host 
        machine, then receives the request and deals with it accordingly. The database service 
        is also accessed through the load balancer . RabbitMQ, on the other hand, is not 
        currently accessed through VIP/HA proxy as the clients are configured with the set 
        of nodes in the RabbitMQ cluster and failover between cluster nodes is automatically 
        handled by the clients.</p>
      <p>The sections below cover the following topics in detail:</p>
      <ul>
        <li><xref href="#HP2.0HA/api_request">API Request Message Flow</xref></li>
        <li><xref href="#HP2.0HA/node_failure">Handling Node Failure</xref></li>
        <li><xref href="#HP2.0HA/network_partitions">Handling Network Partitions</xref></li>
        <li><xref href="#HP2.0HA/galera_cluster">MySQL Galera Cluster</xref></li>
      </ul>    
      
      <sectiondiv>
        <p id="api_request"><b>API Request Message Flow</b></p>
        <p>The diagram below shows the flow for an API request in an HA deployment. All API 
          requests (internal and external) are sent through the VIP.</p>
        <p><image href="../../media/ha/ControlPlaneHA2.0_2.png" id="ControlPlane2"/></p>
        <p>The flow of a sample API request is explained below:</p>
        <ul>
          <li>keepalived has currently configured the VIP on the Controller0 node; 
            client sends Nova request to VIP:8774</li>
          <li>HA proxy (listening on VIP:8774) receives the request and selects Controller0 
            from the list of available nodes (Controller0, Controller1, Controller2). 
            The request is forwarded to the Controller0IP:8774</li>
          <li>nova-api on Controller0 receives the request and determines that a 
            database change is required. It connects to the database using VIP:</li>
          <li>HA proxy (listening on VIP:3306) receives the database connection request 
            and selects Controller1 from the list of available nodes (Controller0, 
            Controller1, Controller2). The connection request is forwarded to 
            Controller1IP:3306</li>
        </ul>
      </sectiondiv>
      
      <sectiondiv>
        <p id="node_failure"><b>Handling Node Failure</b></p>
        <p>With the above HA set up, loss of a controller node is handled as follows:</p>
        <p>Assume that the Controller0, which is currently in control of the VIP, is lost, 
          as shown in the diagram below:</p>
        <p><image href="../../media/ha/ControlPlaneHA2.0_3.png" id="ControlPlane3"/></p>
        <p>When this occurs, keepalived immediately moves the VIP on to the Controller1 
          and can now receive API requests, which is load-balanced by HA proxy, as stated 
          earlier.</p>
        <note>Although MySQL and RabbitMQ clusters have lost a node, they still continue 
          to be operational:</note>
        <p><image href="../../media/ha/ControlPlaneHA2.0_4.png" id="ControlPlane4"/></p>
        <p>Finally, when Controller0 comes back online, keepalived and HA proxy will 
          resume in standby/slave mode and be ready to take over, should there be a 
          failure of Controller1. The Controller0 rejoins the MySQL and RabbitMQ clusters.</p>
      </sectiondiv>
      
      <sectiondiv>
        <p id="network_partitions"><b>Handling Network Partitions</b></p>
        <p>It is important for the HA setup to tolerate network failures, specifically those 
          that result in a partition of the cluster, whereby one of the three nodes in the 
          control plane cannot communicate with the remaining two nodes of the cluster. 
          The description of network partition handling is separated into the main HA 
          components of the controller.</p>
      </sectiondiv>
      
      <sectiondiv>
        <p id="galera_cluster"><b>MySQL Galera Cluster</b></p>
        <p>The handling of network partitions is illustrated in the diagram below. 
          Galera has a quorum mechanism so when there is a partition in the cluster, 
          the primary or quorate partition can continue to operate as normal, whereas 
          the non-primary/minority partition cannot commit any requests. In the example 
          below, Controller0 is partitioned from the rest of the control plane. As a 
          result, requests can only be satisfied on Controller1 or Controller2. 
          Controller0 will continue to attempt to rejoin the cluster:</p>
        <p><image href="../../media/ha/ControlPlaneHA2.0_5.png" id="ControlPlane5"/></p>
        <p>When HA proxy detects the errors against the mysql instance on Controller0, 
          it removes that node from its pool for future database requests.</p>
      </sectiondiv>
      <sectiondiv id="singleton_services">
        <p><b>Singletone Services</b></p>
        
        <sectiondiv>
          <p id="cinder_volume"><b>Cinder-Volume</b></p>
          <p>Due to the single threading required in both cinder-volume and the drivers, 
            the Cinder volume service is run as a singleton in the control plane. 
            <image href="../../media/ha/ha-cinder-volume.png" id="CinderVolume"/></p>
          <p>Cinder-volume is deployed on all three controller nodes, but kept active on 
            only one node at a time. By default, cinder-volume is kept active on the controller. 
            If the controller fails, you must enable and start the cinder-volume service on 
            one of the other controller nodes, until it is restored. Once the controller is 
            restored, you must shut down the Cinder volume service from all other nodes and 
            start it on the controller to ensure it runs as a singleton.</p>
          <p>Since cinder.conf is kept synchronized across all the 3 nodes, Cinder volume 
            can be run on any of the nodes at any given time. Ensure that it is run on only 
            one node at a time.</p>
          <p>Details of how to activate Cinder Volume after controller failure is documented in
              <xref href="../blockstorage/singleton_service_cinder.dita#topic_l1p_vpy_jt">Managing
              Cinder Volume and Backup Services</xref>.</p>
        </sectiondiv>
        <sectiondiv>
          <p id="sherpa"><b>Sherpa</b></p>
          <p>In Helion OpenStack 2.0, Sherpa is used by the Helion Development Platform and is not
            used for Helion OpenStack Upgrade.</p>
          <p>You must take periodic backups of the Sherpa service since it does 
            maintain some state information on local disk storage on the controller. 
            If the controller fails, Sherpa becomes unavailable until you rebuild or 
            restore the controller. After restoring the controller, you should restore 
            the Sherpa state from its latest backup.</p>
          <note>The Sherpa backup needs to be done manually. It's not backed up as 
            part of Control Plane Backup (i.e. Freezer)</note>
        </sectiondiv>
        <sectiondiv>
          <p id="nova_consoleauth"><b>Nova consoleauth</b></p>
          <p>If the controller fails, the Nova consoleauth service will become 
            unavailable and users will not be able to connect to their VM consoles 
            via VNC. The service will be restored once you restore the controller.</p>
        </sectiondiv> 
      </sectiondiv>
      <sectiondiv>
        <p id="failed_controller_nodes"><b>Rebuilding or Replacing failed Controller Nodes</b></p>
        <p>As described above, the three node controller cluster provides a robust, highly 
          available control plane of OpenStack services. Either Controller1 or Controller2 
          servers can be shut down for a short duration for maintenance activities without 
          impacting cloud service availability. (Controller0 cannot be shut down without 
          affecting cloud service availability.)</p>
        <note>The HA design is only robust against single points of failure and may not 
          protect you against multiple levels of failure. As soon as first-level failure 
          occurs, you must try to fix the symptom/root cause and recover from the failure, 
          as soon as possible.</note>
        <p>In the unlikely event that one of the controller servers suffers an irreparable hardware
          failure, you can decommission and delete it from the cluster. You can then deploy the
          failed controller on a new server and connect it back into the original three node
          controller cluster. Learn more about <xref
            href="../operations/replace_controller.dita#topic_ijj_45j_nt">Replacing/Rebuilding
            Controller Nodes</xref>.</p>
      </sectiondiv>
    </section>
    
    
    <section id="availability_zones">
      <title>Availability Zones</title>
      <p><image href="../../media/ha/DeploymentZonesHA2.0_1.png" id="DeploymentZones"/></p>
      <p>While planning your OpenStack deployment, you should decide on how to zone 
        various types of nodes - such as compute, block storage, and object storage. 
        For example, you may decide to place all servers in the same rack in the same 
        zone. For larger deployments, you may plan more elaborate redundancy schemes 
        for redundant power, network ISP connection, and even physical firewalling 
        between zones (<i>this aspect is outside the scope of this document</i>).</p>
        <p>HPE Helion OpenStack offers APIs, CLIs and Horizon UIs for the administrator 
          to define and user to consume, availability zones for Nova, Cinder and Swift 
          services. This section outlines the process to deploy specific types of nodes 
          to specific physical servers, and makes a statement of available support for 
          these types of availability zones in the current release.</p>
        <note>By default, HPE Helion OpenStack is deployed in a single availability zone upon
        installation. Multiple availability zones can be configured by an administrator
        post-install, if required. Refer to the <xref
          href="http://docs.openstack.org/openstack-ops/content/scaling.html" format="html"
          scope="external">Chapter 5: Scaling</xref> (in the OpenStack Operations Guide for more
        information).</note>
    </section>
    
    
    <section id="compute_kvm">
      <title>Compute with KVM</title>
      <p>You can deploy your KVM Nova-Compute nodes either during initial installation, 
        or by adding compute nodes post initial installation.</p>
      <p>While adding compute nodes post initial installation, you can specify the 
        target physical servers for deploying the compute nodes.</p>
      <p>Learn more about <xref href="../operations/add_node.dita">Adding Compute
          Nodes after Initial Installation</xref>.</p>
    </section>
    
    <section id="nova_availablility_zones">
      <title>Nova Availability Zones</title>
      <p>Nova host aggregates and  Nova availability zones can be used to segregate 
        Nova compute nodes across different failure zones.</p>
    </section>
    
    <section id="compute_esx">
      <title>Compute with ESX Hypervisor</title>
      <p>Compute nodes deployed on ESX Hypervisor can be made highly available using 
        the HA feature of VMware ESX Clusters. For more information on VMware HA, 
        please refer to your VMware ESX documentation.</p>
    </section>
    
    <section id="block_storage_vsa">
      <title>Block Storage with StoreVirtual VSA</title>
      <p>Highly available Cinder block storage volumes are provided by the network 
        RAID 10 implementation in the HPE StoreVirtual VSA software. You can deploy 
        the VSA nodes in three node cluster and specify Network RAID 10 protection 
        for Cinder volumes.</p>
      <p>The underlying SAN/iQ operating system of the StoreVirtual VSA ensures that 
        the two-way replication maintains two mirrored copies of data for each volume.</p>
      <p>This Network RAID 10 capability ensures that failure of any single server 
        does not cause data loss, and maintains data access to the clients.</p>
      <p>Furthermore, each of the VSA nodes of the cluster can be strategically deployed in
        different zones of your data center for maximum redundancy and resiliency. For more
        information on how to deploy VSA nodes on desired target servers, refer to the <xref
          href="../installation/configure_vsa.dita#config_vsa">Configuring for VSA Block Storage
          Backend</xref> document.</p>
      <p><image href="../../media/ha/ha-block-storage.png" id="BlockStorage"/></p>
    </section>
    
    <section id="cinder_availability_zones">
      <title>Cinder Availability Zones</title>
      <p>Cinder availability zones are not supported for general consumption 
        in the current release.</p>
    </section>
    
    <section id="object_storage_swift">
      <title>Object Storage with Swift</title>
      <p>High availability in Swift is achieved at two levels.</p>
      <p><b>Control Plane</b></p>
      <p>The Swift API is served by multiple Swift proxy nodes. Client requests 
        are directed to all Swift proxy nodes by  the HA Proxy load balancer in 
        round-robin fashion. The HA Proxy load balancer regularly checks the node 
        is responding, so that if it fails, traffic is directed to the remaining 
        nodes. The Swift service will continue to operate and respond to client 
        requests as long as at least one Swift proxy server is running.</p>
      <p>If a Swift proxy node fails in the middle of a transaction, the transaction fails. However
        it is standard practice for Swift clients to retry operations. This is transparent to
        applications that use the python-swift client library.</p>
      <p>The entry-scale example cloud models contain three Swift proxy nodes. 
        However, it is possible to add additional clusters with additional Swift 
        proxy nodes to handle a larger workload or to provide additional resiliency.</p>
      <p><b>Data</b></p>
      <p>Multiple replicas of all data is stored. This happens for account, 
        container and object data. The example cloud models recommend a replica 
        count of three. However, you may change this to a higher value if needed.</p>
      <p>When Swift stores different replicas of the same item on disk, it ensures 
        that as far as possible, each replica is stored in a different zone, 
        server or disk. This means that if a single server of disk drives fails, 
        there should be two copies of the item on other servers or disk drives.</p>
      <p>In this release, only a single zone is supported.</p>
      <p>If a disk drive is failed, Swift will continue to store three replicas. 
        The replicas that would normally be stored on the failed drive are 
        “handed off” to another drive on the system. When the failed drive is 
        replaced, the data on that drive is reconstructed by the replication 
        process. The replication process re-creates the “missing” replicas by 
        copying them to the drive using one of the other remaining replicas.  
        While this is happening, Swift can continue to store and retrieve data.</p>
      <!--  Orignal Image Included with the 1.1, 1.1.1 and initial 2.0 versions.
      <p><image href="../../media/ha/ha-swift.png" id="HA_Swift"/></p>
      -->
    </section>
    
    <section id="highly_available_app_workloads">
      <title>Highly Available Cloud Applications and Workloads</title>
      
      
      
      
      <p>Projects writing applications to be deployed in the cloud must be aware 
        of the cloud architecture and potential points of failure and architect 
        their applications accordingly for high availability.</p>
      <p>Some guidelines for consideration:</p>
      <ol>
        <li>Assume intermittent failures and plan for retries
          <ul>
            <li><b>OpenStack Service APIs</b>: invocations can fail - you should 
              carefully evaluate the response of each invocation, and retry in 
              case of failures.</li>
            <li><b>Compute</b>: VMs can die - monitor and restart them</li>
            <li><b>Network</b>: Network calls can fail - retry should be successful</li>
            <li><b>Storage</b>: Storage connection can hiccup - retry should be successful</li>
          </ul>
        </li>
        <li>Build redundancy into your application tiers
          <ul>
            <li>
              <ul>
                <li>Replicate VMs containing stateless services such as Web application 
                  tier or Web service API tier and put them behind load balancers 
                  (you must implement your own HA Proxy type load balancer in your 
                  application VMs until HPE Helion OpenStack delivers the LBaaS service).
                </li>
                <li>Boot the replicated VMs into different Nova availability zones.</li>
                <li>If your VM stores state information on its local disk (Ephemeral Storage), 
                  and you cannot afford to lose it, then boot the VM off a Cinder volume.</li>
                <li>Take periodic snapshots of the VM which will back it up to Swift 
                  through Glance.</li>
                <li>Your data on ephemeral may get corrupted (but not your backup 
                  data in Swift and not your data on Cinder volumes).</li>
                <li>Take regular snapshots of Cinder volumes and also back up 
                  Cinder volumes or your data exports into Swift.</li>
              </ul>
            </li>
          </ul>
        </li>
        <li>Instead of rolling your own highly available stateful services, use 
          readily available HPE Helion OpenStack platform services such as:
          <ul>
            <li>Database as a Service (DBaaS)</li>
            <li>DNS as a Service(DNSaaS)</li>
            <li>Messaging as a Service (MSGaaS)</li>
          </ul>
        </li>
      </ol>
     </section>
    
    <section id="what_not_ha">
      <title>What is not Highly Available?</title>
      
      <sectiondiv>
        <p id="deployer"><b>Deployer</b></p>
        <p>The deployer in Helion Openstack is not highly-available. 
          The deployer state/data are all maintained in filesystem and are backed 
          up by the Freezer backup. In case a deployer node failure, the deployer 
          state/data can be recovered from the backup.</p>
      </sectiondiv>
      
      <sectiondiv>
        <p id="control_plane"><b>Control Plane</b></p>
        <p>High availability is not supported for Stateful L3 Services 
          (Source-NAT), and Network Services (LBaaS, VPNaaS, FWaaS)</p>
      </sectiondiv>
    </section>
    
    <section id="more_information">
      <title>More Information</title>
      <ul>
        <li><xref format="html" scope="external" href="http://docs.openstack.org/high-availability-guide/content/ch-intro.html">OpenStack High-availability Guide</xref></li>
        <li><xref format="html" scope="external" href="http://12factor.net/">12-Factor Apps</xref></li>
      </ul>
    </section>
  </body>
</topic>
