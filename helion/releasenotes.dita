<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="release_notes">
  <title>HPE Helion <tm tmtype="reg">OpenStack</tm> 2.0: Release Notes</title>
  <body>
    <!--Needs Edit-->
    <p>This document provides an overview of the features contained within HPE Helion <tm
        tmtype="reg">OpenStack</tm> 2.0, including known issues and workarounds for this
      release:</p>

    <section id="Features">
      <p><b>New GUI Installer</b></p>
      <p>HPE Helion OpenStack 2.0 introduces a GUI installer for configuring and installing the
        operating system, the cloud, and OpenStack services. It provides an easy way to populate the
        configuration files needed to define the <xref href="input_model.dita">Input Model</xref>,
        and uses them to configure and install your cloud. For help using the installer, see the
        page <xref href="gui_installer.dita"/>.</p>
      <p><b>Transport Layer Security (TLS) Support</b>
      </p><p>TLS is now supported for public endpoints. See <xref
          href="security/tls.dita#topic_yym_nps_4t">Enabling TLS for Public Endpoints</xref> for
        more information.</p>
      <p><b>Encryption of sensitive data</b></p>
      <p>In this release, connection details and passwords are encrypted and or protected. See <xref
          href="security/encrypted_storage.dita">Encryption of Passwords and Sensitive Data
        </xref>for more information.</p>
      <p><b>AppArmor enabled</b></p>
      <p>AppArmor in HPE Helion OpenStack 2.0 is installed and enabled on the KVM compute nodes by
        default. It runs in enforce mode. It enforces mandatory access control policies for the
        libvirt process. See <xref href="security/using_apparmor.dita#topic_rmq_j1v_4t">AppArmor in
          HPE Helion OpenStack 2.0</xref> for more information.</p>
      <p><b>The lifecycle manager only supports American English language</b></p>
      <p>The lifecycle-manager installation process prompts you to select a language. The only
        supported language at this time is American English.</p>
      <p><b>Local Git repository for config tracking</b></p>
      <p>In HPE Helion OpenStack 2.0, a local git repository is used to track configuration changes
        as well as acting as a repository for the Configuration Processor to gather configuration
        settings from. The operations work as explained below: <ul>
          <li>Operations under <codeph>~/helion</codeph> are under the aegis of git. You’ll need to
            commit any config into git (on the "site" branch) before the Configuration Processor can
            run it. The script that runs the config processor now guards against uncommitted
            changes.</li>
          <li>Deployment operations are run from a scratch directory that's assembled from various
            parts (the ansible output of the Configuration Processor and the playbooks). The
            directory is created using <codeph>ready-deployment.yml</codeph>.</li>
          <li>All deployment operations should be run from
              <codeph>~/scratch/ansible/next/hos/ansible</codeph>.</li>
        </ul>
      </p>
      <p>The Configuration Processor uses the config files stored in the repo to apply configuration
        settings. An explanation can be found <xref href="installation/using_git.dita">in this
          topic</xref>.</p>
      <p><b>New UEFI support</b></p>
      <p>HPE Helion OpenStack 2.0 includes support for UEFI (which replaces the BIOS on newer
        servers), while continuing to support Legacy BIOS. Select the required mode on each node
        through its BIOS settings before beginning the install process. The installer will detect
        and use the mode you have selected for each node.</p>
      <p><b>Updated HPE Helion OpenStack Services</b></p>
      <p>We have included the core set of OpenStack services from the <xref
          href="https://wiki.openstack.org/wiki/ReleaseNotes/Kilo" scope="external" format="html"
          >Kilo release</xref> with the exception of limitations notated in this document.</p>
      <p><b>More Block Storage Backend Options Available</b></p>
      <p>You can now choose between Ceph, VSA, and 3PAR for Block Storage backends.</p>
      <p>See: <xref href="blockstorage/blockstorage_overview.dita">Block Storage Overview</xref> for
        more details</p>
      <p><b>Neutron FWaaS, LBaaS and VPNaaS Extensions</b></p>
      <p>We have implemented networking extensions firewall, load balancing, and virtual private
        networks. Here is a description of each of these: <ul>
          <li>
            <p>LBaaS (Load-Balancing-as-a-Service) is a Neutron extension that introduces a load
              balancing feature set. HPE Helion OpenStack 2.0 installs by default LBaaS version 2
              which uses an implementation of haproxy but third-party load balancers can be deployed
              as well. If a third-party driver doesn’t support LBaaS V2, then installing LBaaS V1
              instead of V2 is possible.</p>
            <ul>
              <li>For more information, see: <xref href="networking/lbaas_admin.dita">Load Balancer
                  (LBaaS) Configuration</xref></li>
            </ul>
          </li>
          <li>
            <p>FWaaS (Firewall-as-a-Service) is a Neutron extension that introduces a firewall
              feature set. The included reference implementation is using iptables to filter traffic
              but third-party firewalls can be deployed with this extension into HPE Helion
              OpenStack as well.</p>
            <ul>
              <li>For more information, see: <xref href="networking/fwaas.dita">Firewall (FWaaS)
                  Configuration</xref></li>
            </ul>
          </li>
          <li>
            <p>VPNaaS (Virtual-Private-Network-as-a-Service) is a Neutron extension that introduces
              a VPN feature set. It allows for point-to-point traffic between two routers in
              (potentially) different datacenters with configurable encryption hence supporting
              secure multi Availability Zone architectures for customers.</p>
            <ul>
              <li>For more information, see: <xref href="networking/vpnaas.dita">VPNaaS
                  Configuration</xref></li>
            </ul>
          </li>
          <li>
            <p>Multiple networks is a feature in HPE Helion OpenStack 2.0 that enables bridging of
              multiple external physical networks to a Neutron network. Using this functionality a
              tenant can dictate the path taken by the data in the physical network outside the
              cloud. It can also be used for multi-homing to two physical networks.</p>
            <ul>
              <li>For more information, see: <xref href="networking/multinetwork.dita">Multiple
                  External Network Configuration</xref></li>
            </ul>
          </li>
        </ul>
      </p>
      <p><b>New Network Configuration Options</b></p><ul>
        <li>Network interface (NIC) bonding with multiple modes will allow you to set up a highly
          available and performant network infrastructure.</li>
        <li>Multi-network support will allow you to bridge multiple external physical networks to a
          Neutron network. This will enable you to dictate the path taken by the data in the
          physical network outside of your cloud deployment.</li>
        <li>Network separation (both physical and VLAN) will allow you to segregate traffic by type.
          For example, traffic can be separated into management, tenant, external, and service
          networks.</li>
      </ul>
      <p><b>NIC Mapping Feature</b></p> The new NIC mappings feature in HPE Helion OpenStack allows
      the cloud administrator to map network interface names to PCI bus addresses. This is important
      because for well-understood reasons, Linux can sometimes name the network interfaces in a
      pseudo-random order. So in a cluster of identical machines, the <b>eth3</b> device can appear
      as <b>eth4</b> on one or more machines in the cluster. Once this erroneous name is determined
      by the operating system, it will use persistence rules to ensure that the NIC device is called
        <b>eth4</b> thereafter. This makes cluster configuration using Helion quite difficult if not
      impossible, if most machines understand the Management Network to be <b>eth3</b>, but some
      subset of machines understand that network to be on <b>eth4</b> ( and whatever was on eth4 is
      similarly swapped to <b>eth3</b>). <p>To resolve this, and to ensure that all NIC devices have
        consistent naming across the entire cloud, HPE Helion OpenStack 2.0 includes NIC Mapping
        facilities. This is a mechanism in the Input Model to define a specific NIC interface for a
        given PCI address for a given class of machine. Generally, when all of the network
        interfaces are defined in the <xref href="input_model.dita#input_model/co_nicmappings"
          type="section">Input Model</xref>, it is advisable to use the traditional naming, ie
          <b>ethN</b>. However, if one or more interfaces are not defined in the Input Model
        (perhaps because the Ansible network is not defined, or because there is a customer-specific
        network which is outside the scope of Helion), then it is recommended that a different
        naming convention is used, to prevent the definitions from colliding with existing
        configuration data. For example, the NIC mapping definitions could use <b>hos0</b> through
          <b>hosN</b> rather than <b>eth0</b> through <b>ethN</b>, bearing in mind that the new
        naming must be consistent across the entire Input Model.</p>
      <p><b>Monasca-based Monitoring</b></p><p>Monasca is a multi-tenant, scalable, fault-tolerant,
        monitoring service. It uses a REST API for high-speed metrics processing and querying, and
        has a streaming alarm and notification engine. In the HPE Helion OpenStack 2.0 release, the
        OpenStack monitoring standard, Monasca, is supported as the HPE Helion OpenStack monitoring
        solution with the following exceptions (which are not implemented):</p>
      <ul>
        <li>Transform Engine</li>
        <li>Events Engine </li>
        <li>Anomaly and Prediction Engine</li>
      </ul>
      <p>Monitoring is integrated for the following services: Logging, Neutron, Swift, Ceilometer,
        Heat, Rabbit, MySQL, HA Proxy, Glance, Cinder, Monitoring of Monasca, Nova, HPE Linux for HP
        Helion OpenStack.</p><p>Learn more about Monasca here: <xref
          href="operations/monitoring_service.dita">Monasca Overview</xref></p>
      <p><b>Monitoring Configuration Options Available for Metrics Database</b></p><p>In this
        release you will have the option to specify which database platform you want to use for the
        metrics database. The default option with the installation is Vertica. The opensource
        alternative is InfluxDB.</p> Learn more about these options here: <xref
        href="administration/configure_monitoring.dita">Configuring the Monitoring Service</xref>
      <p><b>Centralized Logging</b></p> Logging for the services below is enabled in the default
      configuration. You will have the ability to disable per-service logging as needed. <p>Logging
        is integrated for the following services: Nova, Glance, Swift, Neutron, Ceilometer, Monasca,
        Horizon, Keystone, Cinder, Heat, OpsConsole, DNSaaS, LBaaS, FWaaS, and Trove.</p><p>Learn
        more about Centralized Logging here: <xref href="operations/centralized_logging.dita"
          >Centralized Logging Overview</xref></p>
      <p><b>Core Features in HPE Helion OpenStack 2.0</b></p>
      <!-- 
        
        Core and Non-core descriptions provided by Carolyn Adler (cadler@hpe.com) in Legal.
        DO NOT CHANGE THE FOLLOWING TWO PARAGRAPHS
        
        -->
      <p>Core features in the <tm tmtype="reg">OpenStack</tm> Foundation’s Kilo release (as
        identified below) are enabled in the HPE Helion OpenStack 2.0 standard settings and are
        included in Helion standard product support. </p>
      <p>Non-core services (listed in the Features tables below) are provided as-is. Some non-core
        services are included in this release to provide early access to features that HPE may
        support fully in future releases, but HPE cannot guarantee that support will be provided or
        issues resolved in a future release. Other non-core features are provided in this release
        for a limited time and may be deprecated in future releases. HPE will attempt to provide
        guidance and telephone support for these features at its discretion. If you need additional
        support on any of these features for a specific use case, please contact the Helion
        professional services team. </p>
      <table frame="none">
        <tgroup cols="2">
          <thead>
            <row>
              <entry>Service</entry>
              <entry>Status</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Ceilometer</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Cinder</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Glance</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Heat</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Horizon</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Keystone</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Monasca</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Neutron</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Nova</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Swift</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Tempest</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Neutron LBaaS v1</entry>
              <entry>Non-core</entry>
            </row>
            <row>
              <entry>Neutron LBaaS v2</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Neutron FWaaS</entry>
              <entry>Non-core</entry>
            </row>
            <row>
              <entry>Neutron VPNaaS</entry>
              <entry>Non-core</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <!-- End Section "CoreFeatures"-->
    </section>


    <section id="KnownIssues">
      <title>Limitations in this Release</title>
      <!-- INSTALLATION -->
      <p><b>Installing Swift</b></p>
      <p>Swift installs with the following limitations:</p>
      <ul>
        <li>Only one Swift zone is supported.</li>
      </ul>
      <!-- SERVICES -->
      <!-- MONITORING -->
      <p><b>Monitoring (Monasca) Limitations</b></p> The monitoring service has the following
      limitations: <ul id="ul_fn1_d45_tt">
        <li>Vertica is the default metrics database option in the install. If you want to use
          InfluxDB then read <xref
            href="administration/configure_monitoring.dita#configure_monitoring/config"
            type="section">Configuring Monasca</xref> for more details.</li>
        <li>InfluxDB is not configured behind a VIP. It must directly access one node in the
          cluster.</li>
        <li>Backup/restore of Monasca configuration and metrics data is not available.</li>
      </ul>
      <p><b>Issue: Monasca Command-line Client (CLI) Redirect Fails</b></p>
      <p>If you are using the Monasca CLI and you receive the error below, we have included a
        workaround:</p>
      <p>Error:
        <codeblock>$ monasca metric-list|grep hostname
'ascii' codec can't encode characters in position 1058218-1058219: ordinal not in range(128)</codeblock></p>
      <p>Workaround:
        <codeblock>PYTHONIOENCODING=utf-8 monasca metric-list | grep hostname</codeblock></p>
      <p><b>Monasca Forwarder Log Reporting False Errors</b></p>
      <p>There is an issue where the <codeph>/var/log/monasca/agent/forwarder.log</codeph> log file
        will contain many lines containing text like this:</p>
      <codeblock>2015-09-23 17:33:34 UTC | ERROR | forwarder |
monascaclient.exc(exc.py:60) | exception: Authentication failed.</codeblock>
      <p>This is due to an issue where the monasca-agent is attempting to authenticate when it's
        token has expired. These can be ignored.</p>
      <p><b>Issue: The 'Cinderlm diagnostics monitor' alarm alerts with an 'UNDETERMINED'
        state</b></p>
      <p>This alert is not always reported but may occur after the system has been running for some
        time.</p>
      <p>Check if any other Cinder related alarms are alerting. If no other other Cinder related
        alarms are alerting then this alert for the Cinderlm diagnostics monitor alarm can be
        ignored. If other Cinder related alarms are alerting then follow the mitigation steps
        described in the Service Alarm Definitions section.</p>
      <p><b>Issue: The 'Swiftlm-scan' alarm sticks</b></p>
      <p>The <codeph>swiftlm-scan</codeph> monitor alarm may be triggered. However, once triggered,
        the alarm does not automatically return to the normal state even if the condition that
        caused it has been resolved. To confirm this, run the following command:</p>
      <codeblock>monasca measurement-list swiftlm.swift.swift_services -3 --merge_metrics --dimensions hostname=&#60;hostname></codeblock>
      <p>
        <note>The <codeph>hostname</codeph> value is the host reporting the 'swiftlm-scan' monitor
          alarm.</note>
      </p>
      <p>If the command output reports that the metrics are being reported then the system is fine
        and the alarm can be ignored.</p>
    <!--
      <p><b>The Threshold Engine is not deployed properly if you use a single Control Plane node</b></p>
      <p>If you are using a cloud model that has a single control plane (controller) node, as specified by having the <codeph>member-count</codeph> value in your <codeph>control_plane.yml</codeph> set to <codeph>1</codeph> for your control-planes, then the Storm supervisor processes will not get properly installed. This causes the threshold engine to not run.</p>
      <p>The workaround is as follows:</p>
      <ol>
        <li>Ed</li>
      </ol>
      -->
      <!-- OPS CONSOLE -->
      <p><b>Operations Console (Ops Console)</b></p> The Operations Console has the following
      limitations (listed below by menu option in the console): <ul id="ul_wv1_d45_tt">
        <li>Dashboard <ul id="ul_s2b_d45_tt">
            <li>There are two notification sections for the Storage services. The one in the upper
              left is the Compute service mislabeled as Storage.</li>
            <li>The New Alarms section uses the UTC timezone to determine timeframes regardless of
              what timezone is in the settings.</li>
            <li>The New Alarms section will only display alarms that have a service dimension
              defined.</li>
            <li>Platform Services (HDP Summary) alarm panels will only contain data if the Helion
              Development Platform services are installed.</li>
          </ul>
        </li>
        <li>Logging Dashboard <ul id="ul_jfb_d45_tt">
            <li>There is no auto-logon function, users must use the credentials in the <xref
                href="operations/centralized_logging.dita#logging/interface">Logging Overview</xref>
              to access the UI.</li>
            <li>The Kibana console intermittently will not function correctly if Kibana is on a
              different network than the Operations Console. If both Kibana and the Operations
              Console are on the same network then this issue should not present itself.</li>
          </ul></li>
        <li>Alarms <ul id="ul_egb_d45_tt">
            <li>The tile view of alarms with lengthy dimensions does not render cleanly.</li>
          </ul></li>
        <li>Compute Instances <ul id="ul_rgb_d45_tt">
            <li>Compute instance utilization metrics are not available.</li>
          </ul>
        </li>
        <li>Networking - Alarm Summary <ul id="ul_dhb_d45_tt">
            <li>Alarms may be limited to only those with the dimension "service=networking".</li>
          </ul></li>
        <li>Storage - Alarm Summary <ul id="ul_shb_d45_tt">
            <li>Alarms with the dimension "service=compute" sometimes appear in this list in
              addition to the "service=block-storage" and "service=object-storage" ones.</li>
          </ul></li>
        <li>HDP - Alarm Summary <ul id="ul_h3b_d45_tt">
            <li>This section will only contain data if the Helion Development Platform services are
              installed.</li>
          </ul></li>
        <li>Masthead Items <ul id="ul_v3b_d45_tt">
            <li>The Help and EULA links will not work properly. You can view the Help files here:
                <xref href="operations/opsconsole_overview.dita">Operations Console
              Overview</xref></li>
            <li>User information panel fails to load making it so a user cannot update their email
              address or password.</li>
          </ul></li>
      </ul>
      <p><b>Issue: The Operations Console does not support alarm pagination</b></p>
      <p>The Operations Console UI does not support >10,000 simultaneous alarm instances.
        Configuring alarm definitions such that the number of alarms instances exceeeds the 10,000
        mark may cause some alarms to not show in the UI. The workaround for this issue is to access
        those alarms via either the API or the command-line clients.</p>
      <!-- LOGGING -->
      <p id="logging"><b>Centralized Logging Limitations</b></p>
      <p>The Centralized Logging service has the following limitations:</p>
      <ul>
        <li>Alarm definition and generation for Kibana and curator are not integrated.</li>
        <li>Logging-monitor 'stop' playbook is not integrated.</li>
      </ul>
      <p><b>Issue: The Elasticsearch Cluster Name Must Be Unique</b></p>
      <p>The default Elasticsearch cluster name is set to <codeph>elasticsearch</codeph> in the
        logging configuration files. It is necessary during installation to change this value to a
        unique name. This step is included in our installation documentation.</p>
      <p><b>The Elasticsearch high/low watermark alarms will show up in "Undefined"</b></p>
      <p>In the Operations Console, the Elasticsearch high/low watermark alarms will show up in the
        “Undefined” section instead of in the "Logging" section.</p>
      <p><b>Issue: The Heap parameters for Elasticsearch and Logstash are auto-tuned
        incorrectly</b></p>
      <p>There is an issue where the Heap parameters for Elasticsearch and Logstash are incorrectly
        auto-tuned. The default values are listed in <xref
          href="administration/configure_logging.dita#configure_logging/general_config">Configuring
          the Centralized Logging Service</xref>. There is a bug where Ansible is reporting the
        memory values incorrectly as values that are slightly less than what they actually are. So,
        for example, if you have 32GB of RAM you should fall into the "Small" auto-tuning category
        but because of this incorrectly reported value you are actually falling into the "Demo"
        category. We recommend that you manually modify the heap values on the lifecycle-manager
        node for systems with the specified amount of RAM in the table below to the values listed.
        You will edit these values in the <codeph>logging_possible_tunings</codeph> section of the
        file below:</p>
      <codeblock>~/helion/my_cloud/config/logging/main.yml</codeblock>
      <table frame="all" rowsep="1" colsep="1" id="table_pbd_k2y_5t">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <colspec colname="c4" colnum="4"/>
          <thead>
            <row>
              <entry>RAM</entry>
              <entry>Key</entry>
              <entry>elasticsearch_heap_size</entry>
              <entry>logstash_heap_size</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>32GB</entry>
              <entry>demo</entry>
              <entry>from 256m to 8g</entry>
              <entry>from 256m to 2g</entry>
            </row>
            <row>
              <entry>64GB</entry>
              <entry>small</entry>
              <entry>from 8g to 16g</entry>
              <entry>from 2g to 4g</entry>
            </row>
            <row>
              <entry>128GB</entry>
              <entry>medium</entry>
              <entry>from 16g to 32g</entry>
              <entry>from 4g to 8g</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p><b>Issue: Logrotate Settings for MySQL, keepalived, and RabbitMQ are Invalid</b></p>
      <p>There is currently a workaround needed on the controller nodes to fix the logrotate
        settings for the MySQL, keepalived, and RabbitMQ services.</p> The logrotate for MySQL,
      keepalived and RabbitMQ is being overwritten by an invalid logrotate configuration supplied in
      the logging-server component. A valid logrotate configuration for these services is supplied
      in the following files on the deployer:
        <codeblock>/home/stack/scratch/ansible/next/hos/ansible</codeblock><p>The files in that
        directory are:</p><codeblock>
mysql: ./roles/FND-MDB/templates/etc/logrotate.d/mysql
keepalived: ./roles/keepalived/templates/keepalived.logrotate.j2
rabbit: ./roles/rabbitmq/templates/rabbitmq-server.logrotate.j2</codeblock>
      <p>To correct this problem if the above configuration is copied manually into
          <codeph>/etc/logrotate.d</codeph> on the controller nodes to replace the existing
        configuration then this should correct the logrotate configuration for these
        services.</p><p>Note any variables such as <codeph>{{ }}</codeph> need to be replaced
        correctly.</p> In the case of RabbitMQ and keepalived this can be achieved by editing the
        <codeph>logrotate.j2</codeph> with a comment and running a reconfigure
      (rabbitmq-reconfigure.yml and FND-CLU-reconfigure.yml on the deployer) <p>In the case of MySQL
        this needs to be corrected manually as below: <codeblock>File ./roles/FND-MDB/templates/etc/logrotate.d/mysql</codeblock>
        <codeblock>
{ log_dir }}/*.log {
        daily
        rotate 7
        missingok
        create 640 mysql adm
        notifempty
        compress
        delaycompress
        sharedscripts
        postrotate
               if test -x /usr/bin/mysqladmin &#38;&#38; \
                   /usr/bin/mysqladmin ping &#38;>/dev/null
               then
                   roothome=~root
                  /usr/bin/mysqladmin --defaults-file=${roothome}/.my.cnf --socket="{{ server_socket}}" flush-logs
               fi
        endscript
}</codeblock></p>
      <p>should be copied into /etc/logrotate.d/mysql on the controller nodes and variables replaced
        to result in this config:</p>
      <codeblock>
/var/log/mysql/*.log {
          daily
          rotate 7
          missingok
          create 640 mysql adm
          notifempty
          compress
          delaycompress
          sharedscripts
          postrotate
                  if test -x /usr/bin/mysqladmin &#38;&#38; \
                     /usr/bin/mysqladmin ping &#38;>/dev/null
                  then
                     roothome="$(realpath ~root)"
                     /usr/bin/mysqladmin --defaults-file=${roothome}/.my.cnf --socket="/var/run/mysqld/mysqld.sock" flush-logs
                  fi
          endscript
}</codeblock>
      <!-- BLOCK STORAGE -->
      <p><b>Issue: Kilo functionality not supported by the Kilo version of
        python-cinderclient</b></p>
      <p>The version of the python-cinderclient available in HPE Helion OpenStack 2.0 does not
        support some functionality available from the cinder services, including incremental backup
        and private volume types.</p> If you install a new version of python-cinderclient, and if
      you have OS_ENDPOINT_TYPE defined, you must also define CINDER_ENDPOINT_TYPE. For example: <codeblock>export CINDER_ENDPOINT_TYPE=${OS_ENDPOINT_TYPE}</codeblock>
      <p>The workaround for this is the following:</p>
      <ol>
        <li>Create a new virtual environment in a suitable directory:
          <codeblock>cd &#60;dir>
virtualenv cinder_venv</codeblock></li>
        <li>Activate the new virtual environment:
          <codeblock>source ./cinder_venv/bin/activate</codeblock></li>
        <li>Use pip to install the python-cinderclient: <codeblock>pip install python-cinderclient</codeblock>
          <p>or if the machine is behind a proxy, use:</p>
          <codeblock>pip install --proxy [user:passwd@]proxy.server:port python-cinderclient</codeblock></li>
        <li>Verify the version you have installed is version 1.4.0 or newer:
          <codeblock>cinder --version</codeblock></li>
        <li>To use this virtual environment at a later time, or from a different shell session, you
          must activate the virtual environment as stated in step #2 above:
          <codeblock>source &#60;some-dir>/cinder_venv/bin/activate
cinder --version</codeblock></li>
      </ol>
      <p><b>In the Cinder CLI no more than 1000 Cinder volumes can be displayed</b></p>
      <p>Due to a known issue with the version of the OpenStack python-cinderclient in HPE Helion
        OpenStack 2.0 (1.1.3), the maximum number of volumes that can be displayed is 1000. The
        fixed client is 1.3.1. The workaround is to use python-cinderclient 1.3.1.</p>
      <!-- HORIZON -->
      <p><b>Cannot open Launch Instance from Volume on Horizon Dashboard</b></p>
      <p>User cannot open the Launch Instance wizard from a newly created bootable volume’s actions
        dropdown on the Volumes Page. </p><p>The workaround is to refresh the page.</p>
      <p><b>Launch Instance form in Horizon not populated when choosing volumes</b></p>
      <p>When using the Launch Instance action in the Volumes table, the Launch Instance Wizard form
        doesn't get pre-populated with the Volumes option selected, nor does it select the given
        Volume. The workaround is to select an available bootable volume by selecting the Volume
        option from the Select Boot Source dropdown on Launch Instance wizard.</p>
      <p><b>Start Instances in Horizon enabled</b></p>
      <p>Note that for Start Instances at Instances table level in Horizon, the More Actions
        dropdown is enabled even if an instance status is Active.</p><p><b>Issue: The <codeph>Start
            Instances</codeph> Option is Available In Horizon for Instances that are Already
          Started</b></p>
      <p>There is currently an OpenStack bug where the <codeph>Start Instances</codeph> option is
        available in the instance action drop-down even if your instances are already in an Active
        state.</p>
      <p><b>A Horizon create a 'bootable volume' fails UI task</b></p>
      <p>When you navigate to 'volumes' and choose to create a 'bootable volume,' if you do not
        navigate elsewhere, the attempt to launch an instance from that volume will fail. That is,
        nothing will happen.</p><p> To work around this issue, after choosing to create a 'bootable
        volume,' refresh the page in the browser and then proceed.</p>
      <p><b>Horizon throws an error when Nova compute is not present</b></p>
      <p>If using the Entry-scale with Swift example, Horizon displays the error "Invalid Service
        Catalog service: compute."</p>
      <p>To avoid this error, you may alter the default panel (this changes the default for both
        admins and normal users to the object storage page and doesn't raise an error, although the
        problematic pages are still available to be chosen).</p>
      <p>To alter the default, on each controller node perform the following steps:</p>
      <ol>
        <li>Edit the file below:
          <codeblock>/opt/stack/service/horizon/etc/local_settings.py</codeblock></li>
        <li>Below line 37, add the following line:
          <codeblock>HORIZON_CONFIG['user_home'] = "/project/containers"</codeblock></li>
        <li>Then run the following command to restart Apache to have Horizon pick up the change
          (again, on each controller): <codeblock>sudo service apache2 reload</codeblock>
        </li>
      </ol>
      <p>These steps will need to be performed after reinstall or upgrade.</p>
      <!-- HEAT -->
      <p><b>Heat autoscaling not supported</b></p>
      <p>In HPE Helion OpenStack 2.0, the auto-scaling in Heat is not supported because the alarm
        creation in Ceilometer is disabled.</p>
      <!-- BAREMETAL -->
      <p><b>Restarting lifecycle-manager node leaves Apache2 down</b></p>
      <p>If you restart your lifecycle management node, note that Apache2 will not restart on its
        own. You will need to restart it manually.</p>
      <p><b>Moonshot remote console goes blank after operating system install</b></p>
      <p>If you are using Moonshot compute nodes you may find that their remote console is blank or
        garbled after the OS install. If affected, you need to make the following change to
        /etc/default/grub on each affected node, run update-grub and reboot:</p><ol
        id="ol_p1c_d45_tt">
        <li>Find the line starting with GRUB_CMDLINE_LINUX= </li>
        <li>Add the following at the end of the value i.e. inside the double quotes:
          <codeblock>vga=788 console=ttyS0,9600 earlyprintk=serial,ttyS0,9600</codeblock>
        </li>
      </ol>
      <p><b>Softlockup is observed when attempting to remove many LUNs simultaneously</b></p>
      <p>If many multipath LUNs are detached from a compute host in rapid succession a softlockup
        and RCU (read-copy-update) stall may be observed in the kernel log file as follows:</p>
      <codeblock>Oct  7 12:48:52 hlm kernel: [31164.262063]  rport-0:0-2: blocked FC remote port time out: removing target and saving binding
Oct  7 12:48:52 hlm kernel: [31164.262859]  rport-0:0-0: blocked FC remote port time out: removing target and saving binding
Oct  7 12:49:09 hlm kernel: [31181.132369] BUG: soft lockup - CPU#23 stuck for 22s! [systemd-udevd:554]
Oct  7 12:49:09 hlm kernel: [31181.133060] Modules linked in: dm_service_time xt_mac xt_physdev xt_set iptable_mangle xt_comment 
     iptable_nat nf_nat_ipv4 nf_nat iptable_raw ip_set_hash_net ip_set nfnetlink vhost_net vhost macvtap macvlan tun veth bridge 
     ib_iser rdma_cm iw_cm ib_cm ib_sa ib_mad ib_core ib_addr iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi md_mod 
     fuse binfmt_misc xt_tcpudp xt_multiport xt_LOG xt_limit xt_conntrack xt_pkttype ip6table_filter ip6_tables iptable_filter 
     ip_tables x_tables 8021q garp stp mrp llc bonding openvswitch gre libcrc32c nf_conntrack_ipv6 nf_defrag_ipv6 nf_conntrack_ipv4 
     nf_defrag_ipv4 nf_conntrack dm_multipath scsi_dh evdev iTCO_wdt iTCO_vendor_support x86_pkg_temp_thermal intel_powerclamp 
     coretemp kvm_intel kvm crc32_pclmul crc32c_intel ghash_clmulni_intel aesni_intel psmouse aes_x86_64 lrw gf128mul glue_helper 
     ablk_helper cryptd pcspkr serio_raw ioatdma mgag200 dca ttm ipmi_si drm_kms_helper ipmi_msghandler hpilo drm lpc_ich wmi mfd_core 
     agpgart i2c_algo_bit syscopyarea sysfillrect sysimgblt processor i2c_core thermal_sys acpi_power_meter hpwdt button autofs4 ext4 
     crc16 mbcache jbd2 dm_mod sd_mod sg lpfc qla2xxx crc_t10dif uhci_hcd ehci_pci crct10dif_generic xhci_hcd ehci_hcd scsi_transport_fc 
     hpsa usbcore usb_common scsi_mod be2net crct10dif_common
Oct  7 12:49:09 hlm kernel: [31181.133114] CPU: 23 PID: 554 Comm: systemd-udevd Not tainted 3.14.51-1-amd64-hlinux #hlinux1
Oct  7 12:49:09 hlm kernel: [31181.133115] Hardware name: HPE ProLiant BL460c Gen9, BIOS I36 05/06/2015
Oct  7 12:49:09 hlm kernel: [31181.133117] task: ffff883fcf0262c0 ti: ffff883f979ca000 task.ti: ffff883f979ca000
Oct  7 12:49:09 hlm kernel: [31181.133118] RIP: 0010:[&lt;ffffffff81514f5b>]  [&lt;ffffffff81514f5b>] _raw_spin_trylock+0xb/0x40</codeblock>
      <p>The associated stack from the softlockup will show the instruction point either holding or
        attempting to acquire a spinlock. After all the LUNs have successfully detached and the
        devices have been flushed, the messages will subside. It is recommended that you include a
        delay when detaching multiple LUNs to avoid the softlockup and RCU stall messages.</p>
      <!-- NETWORKING -->
      <p><b>IPv6 Traffic Not Supported</b></p>
      <p>The HPE Helion OpenStack 2.0 release includes firewall rules to prevent non-essential
        traffic from accessing the physical systems. However, these rules are limited to IPv4
        traffic only. In order to prevent unsolicited or nefarious IPv6 traffic, you will need to
        block IPv6 at your router, or within the infrastructure.</p><p> You can disable IPv6 on your
        operating systems by adding the following lines to <b>/etc/sysctl.conf</b>:
        <codeblock>net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1</codeblock>
        Once these have been added, you can either reboot the node or run the following command (as
        root): <codeblock># sysctl -p</codeblock> Please refer to your router documentation to block
        IPv6 traffic from reaching your network at the router.</p>
      <!-- MESSAGING -->
      <p><b>Error: Failed to consume message from queue</b></p>
      <p>If RabbitMQ or QPID is chosen as the messaging service backend, OpenStack services might
        display an error-level log message when an application exits, stating: “Failed to consume
        message from queue.” Since this log mesage is benign and since the fix depends on other
        changes including new features, any attempt to fix the issue will cause instability. For
        these reasons, the OpenStack community has decided not to backport the fix from
        stable/liberty into stable/kilo release.</p>
      <p>The solution is to ignore the following error message in the log for both Nova and
        Ceilometer:</p>
      <codeblock>(oslo_messaging._drivers.impl_rabbit): 2015-08-20 01:18:21,280 ERROR impl_rabbit _error_callback Failed 
      to consume message from queue:(amqp): 2015-08-20 01:18:21,282 DEBUG channel _do_close Closed channel #1</codeblock>
    </section>
  </body>
</topic>
