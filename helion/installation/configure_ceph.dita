<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
  <title>HPE Helion <tm tmtype="reg">OpenStack</tm> 2.0: Configuring for Ceph Block Storage
    Backend</title>
  <body>
    <note type="attention">Hyperlinks intermittently do not work in the Google Chrome browser. <xref
      href="http://docs.hpcloud.com/helion/installation/configure_ceph.html" scope="external" format="html">Click
      here</xref> for a frameless version of this page where the links should work.</note>
    <p>This page describes how to configure your Ceph backend for the Helion Entry-scale with KVM
      Cloud model. It consists of the following steps:</p>
    <ul>
      <li><xref href="configure_ceph.dita#config_ceph/config_files">Editing your Ceph Environment
          Input Files</xref></li>
      <li><xref href="configure_ceph.dita#config_ceph/configure_backend">Configure Ceph as the
          Backend</xref></li>
      <li><xref href="configure_ceph.dita#config_ceph/attach_ceph">Configure Nova to Allow
          Attachment of Ceph Volumes to Instances</xref></li>
      <li><xref href="configure_ceph.dita#config_ceph/create_voltype">Create a Volume Type for
        your Volumes</xref></li>
      <li><xref href="configure_ceph.dita#config_ceph/associate_voltype">Associate the Volume
        Type to the Backend</xref></li>
      <li><xref href="configure_ceph.dita#config_ceph/extra_specs">Extra Specification
        Options</xref></li>
      <li><xref href="configure_ceph.dita#config_ceph/post_install">Verifying your Ceph
          backend</xref></li>
    </ul>
    <section id="notes"><title>Notes</title>
      <p>The Ceph cluster expects the network group that will be used for management network traffic
        within the cloud to be left to its default value i.e. MANAGEMENT. Altering the name will
        lead to failures in the cloud deployment, therefore it should be avoided.</p>
      <p>You can deploy the Ceph monitor service on a dedicated resource. Ensure you modify your
        environment after installing the lifecycle manager.</p>
      <p>For more details, refer to <xref
          href="../blockstorage/ceph/deploy_monitor_stand_alone_node.dita">Install a monitor service
          on a dedicated resource node</xref>.</p>
      <p>While executing the <codeph>site.yml</codeph> playbook with the <codeph>--limit</codeph>
        option, it is also expected to include the Ceph Monitor nodes in the list of restricted
        nodes if the cloud is deployed with a Ceph cluster.</p>
    </section>
    <section id="config_files">
      <title>Edit Your Ceph Environment Input Files</title>
      <p>
        <ol>
          <li>Log in to the lifecycle manager.</li>
          <li>Copy the example configuration files into the required setup directory and edit them
            to contain the details of your environment: <codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock>
            <p>Begin inputting your environment information into the configuration files in the
                <codeph>~/helion/my_cloud/definition</codeph> directory.</p>
            <p>Full details of how to do this can be found here: <xref href="../input_model.dita"
                >Helion OpenStack 2.0 Input Model</xref>.</p></li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/disks_osd.yml</codeph> file and
            enter the details for the additional disks meant for OSD data and journal filesystems.
              <p>A sample <codeph>disks_osd.yml</codeph> is as follows:</p>
            <codeblock>disk-models:
  - name: OSD-DISKS
    # Disk model to be used for Ceph OSD nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5

    # Disks to be used by Ceph
    # Additional disks can be added if available
    device-groups:
      - name: ceph-osd-data-only
        devices:
          - name: /dev/sdb
        consumer:
           name: ceph
           attrs:
             usage: data
      - name: ceph-osd-data-and-journal
        devices:
          - name: /dev/sdc
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdd
      - name: ceph-osd-data-and-shared-journal-set-1
        devices:
          - name: /dev/sde
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg
      - name: ceph-osd-data-and-shared-journal-set-2
        devices:
          - name: /dev/sdf
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg</codeblock>
            <p>The above sample file contains three OSD nodes and two journal disks.</p>
            <p>The disk model has the following fields:</p>
            <p>
              <table frame="all" rowsep="1" colsep="1" id="ceph1">
                <tgroup cols="2">
                  <colspec colname="c1" colnum="1"/>
                  <colspec colname="c2" colnum="2"/>
                  <thead>
                    <row>
                      <entry>Value</entry>
                      <entry>Description</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry><b>device-groups</b></entry>
                      <entry>There can be several device groups. This allows different sets of disks
                        to be used for different purposes.</entry>
                    </row>
                    <row>
                      <entry><b>name</b></entry>
                      <entry>This is an arbitrary name for the device group. The name must be
                        unique.</entry>
                    </row>
                    <row>
                      <entry><b>devices</b></entry>
                      <entry>This is a list of devices allocated to the device group. A
                          <codeph>name</codeph> field containing <codeph>/dev/sdb</codeph>,
                          <codeph>/dev/sdc</codeph>, <codeph>/dev/sde</codeph> and
                          <codeph>/dev/sdf</codeph> indicates that the device group is used by
                        Ceph.</entry>
                    </row>
                    <row>
                      <entry><b>consumer</b></entry>
                      <entry>This specifies the service that uses the device group. A
                          <codeph>name</codeph> field containing <b>ceph</b> indicates that the
                        device group is used by Ceph.</entry>
                    </row>
                    <row>
                      <entry><b>attrs</b></entry>
                      <entry>These are the attributes associated with the consumer.</entry>
                    </row>
                    <row>
                      <entry><b>usage</b></entry>
                      <entry>There can be several uses of devices for a particular service. In the
                        above sample, <codeph>usage</codeph> field contains <b>data</b> which
                        indicates that the device is used for data storage.</entry>
                    </row>
                    <row>
                      <entry><b>journal_disk</b></entry>
                      <entry>Disk to be used for storing the journal data. When running multiple
                        Ceph OSD daemons on a single node, a journal disk can be shared between OSDs
                        of the node.</entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
            </p>
            <p><note>Ensure that disks designated to be used for the OSD data and journal storage
                must be in a clean state (i.e. any existing partitions must be deleted). If you do
                not perform this step, Ceph configuration fails with errors.</note></p></li>
          <li>[OPTIONAL] There are parameters for Ceph that can be edited by the admin in the
            locations described below. The default values will work but if you choose to change any
            of these values, ensure that you also change them where they are referenced in your
              <codeph>cinder.conf.j2</codeph> file, which we cover in the next section of this
            guide. <ul>
              <li><codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph>
                <p>In the <codeph>settings.yml</codeph> file, you can edit the following
                  parameters:</p>
                <p>
                  <table frame="all" rowsep="1" colsep="1" id="table_gc4_c5t_5t">
                    <tgroup cols="2">
                      <colspec colname="c1" colnum="1"/>
                      <colspec colname="c2" colnum="2"/>
                      <thead>
                        <row>
                          <entry>Value</entry>
                          <entry>Description</entry>
                        </row>
                      </thead>
                      <tbody>
                        <row>
                          <entry>fsid</entry>
                          <entry>It is a unique identifier for the Ceph cluster.</entry>
                        </row>
                        <row>
                          <entry>ceph_cluster</entry>
                          <entry>
                            <p>Ceph clusters have a cluster name. The default cluster name is ceph,
                              but you may specify a different cluster name.</p>
                          </entry>
                        </row>
                        <row>
                          <entry>osd_settle_time</entry>
                          <entry>
                            <p>Time in seconds to wait for after starting/restarting the Ceph OSD
                              services.</p>
                          </entry>
                        </row>
                        <row>
                          <entry>osd_journal_size</entry>
                          <entry>
                            <p>The size of the journal in megabytes.</p>
                          </entry>
                        </row>
                      </tbody>
                    </tgroup>
                  </table></p></li>
              <li>
                <p>Add any additional configuration parameters for Ceph in the same file
                    (<codeph>settings.yml</codeph> file) under the 'extra:' category as
                  follows:<codeblock>extra:
  osd:
    journal_max_write_entries: 200</codeblock></p>
              </li>
              <li>
                <p><codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph></p>
                <p>The <codeph>user_model.yml</codeph> has the editable values for the different
                  pools created by HPE Helion OpenStack.</p>
              </li>
            </ul></li>
          <li>Commit your
            configuration<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>"</codeblock></li>
        </ol></p>
      <p>After your configuration files are setup, continue with the <xref
          href="install_entryscale_kvm.dita#install_kvm/provision">Entry-scale KVM Cloud
          installation steps.</xref></p>
    </section>

    <section id="configure_backend">
      <title>Configure Ceph as the Backend</title>
      <p>You can use Ceph as either the backend for volumes or volume backups or both. These steps
        will show you how to do this.</p>
      <p>Perform the following procedure on the lifecycle manager to configure Ceph as
        a volume backend:</p>
      <p><b>Prerequisites</b></p>
      <p>In order for Ceph to be used as a backend for volumes, the nodes running these services
        should have the Ceph client installed on them. Use the
          <codeph>ceph-client-prepare.yml</codeph> playbook to deploy the Ceph client on these
        nodes.</p>
      <p>
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</codeblock>
      </p>
      <p>This will also create Ceph users and Ceph pools on the resource nodes.</p>
      <p><b>Ceph Configuration</b></p>
      <p>Continue with the Ceph configuration with the steps below:</p>
      <p>
        <ol>
          <li>Log in to the lifecycle manager.</li>
          <li>Make the following changes to the
              <codeph>~/helion/my_cloud/config/cinder/cinder.conf.j2</codeph> file: <ol>
              <li>Add your Ceph backend to the <codeph>enabled_backends</codeph> section:
                <codeblock># Configure the enabled backends
enabled_backends=ceph1</codeblock></li>
              <!--<li>[OPTIONAL] If you want a use a default volume type, then enter it in the
                  <codeph>[DEFAULT]</codeph> section with the syntax below. You will want to
                remember this value when you create your volume type in the next section.
                <codeblock>[DEFAULT]
# Set the default volume type
default_volume_type = &lt;your new volume type></codeblock></li> -->
              <li>Uncomment the <codeph>ceph</codeph> section and fill the values as per your
                cluster information. If you have more than one cluster, you will need to add another
                similar section with its respective values. In the following example only one
                cluster is added. <codeblock>[ceph1]
rbd_secret_uuid = &#60;secret-uuid>
rbd_user = &#60;ceph-cinder-user>
rbd_pool = &#60;ceph-cinder-volume-pool>
rbd_ceph_conf = &#60;ceph-config-file>
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = &#60;ceph-backend-name></codeblock>
                <p>where:</p>
                <table frame="all" rowsep="1" colsep="1" id="ceph_volume">
                  <tgroup cols="2">
                    <colspec colname="c1" colnum="1"/>
                    <colspec colname="c2" colnum="2"/>
                    <thead>
                      <row>
                        <entry>Value</entry>
                        <entry>Description</entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry>rbd_secret_uuid</entry>
                        <entry>Use the secret_id value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: <b>457eb676-33da-42ec-9a8c-9293d545c337</b>
pools:
    - name: volumes</codeblock></entry>
                      </row>
                      <row>
                        <entry>rbd_user</entry>
                        <entry>Use the username value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: 457eb676-33da-42ec-9a8c-9293d545c337
pools:
    - name: volumes</codeblock></entry>
                      </row>
                      <row>
                        <entry>rbd_pool</entry>
                        <entry>Use the pool name value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below:
                          <codeblock>- user:
    name: cinder
    type: openstack
    secret_id: 457eb676-33da-42ec-9a8c-9293d545c337
pools:
    - name: <b>volumes</b></codeblock></entry>
                      </row>
                      <row>
                        <entry>rbd_ceph_conf</entry>
                        <entry>Enter your Ceph configuration file location, usually
                            <codeph>/etc/ceph/ceph.conf</codeph></entry>
                      </row>
                      <row>
                        <entry>volume_driver</entry>
                        <entry>Cinder volume driver. Leave this as the default value specified for
                          Ceph.</entry>
                      </row>
                      <row>
                        <entry>volume_backend_name</entry>
                        <entry>Name given to the Ceph backend.
                          <!--You will specify this value later in
                          the <xref href="configure_vsa.dita#config_vsa/associate_volume_backend"
                            >Associate the Volume Type to a Backend</xref> steps.--></entry>
                      </row>
                    </tbody>
                  </tgroup>
                </table>
              </li>
            </ol></li>
          <li>To enable Cinder to backup to Ceph, make the following changes to the
              <codeph>~/helion/my_cloud/config/cinder/cinder.conf.j2</codeph> file: <ol>
              <li>Uncomment the <codeph>ceph backup</codeph> section and fill the values: <codeblock>[DEFAULT]
backup_driver = cinder.backup.drivers.ceph
backup_ceph_conf = &#60;ceph-config-file>
backup_ceph_user = &#60;ceph-backup-user>
backup_ceph_pool = &#60;ceph-backup-pool></codeblock>
                <p>where:</p>
                <table frame="all" rowsep="1" colsep="1" id="ceph_backup">
                  <tgroup cols="2">
                    <colspec colname="c1" colnum="1"/>
                    <colspec colname="c2" colnum="2"/>
                    <thead>
                      <row>
                        <entry>Value</entry>
                        <entry>Description</entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry>backup_driver</entry>
                        <entry>Cinder volume driver. Leave this as the default value specified for
                          Ceph.</entry>
                      </row>
                      <row>
                        <entry>backup_ceph_conf</entry>
                        <entry>Enter your Ceph configuration file location, usually
                            <codeph>/etc/ceph/ceph.conf</codeph></entry>
                      </row>
                      <row>
                        <entry>backup_ceph_user</entry>
                        <entry>Use the user name value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>- user:
    name: <b>cinder-backup</b>
    type: openstack
pools:
    - name: backups</codeblock></entry>
                      </row>
                      <row>
                        <entry>backup_ceph_pool</entry>
                        <entry>Use the pool name value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>- user:
    name: <b>cinder-backup</b>
    type: openstack
pools:
    - name: <b>backups</b></codeblock></entry>
                      </row>
                    </tbody>
                  </tgroup>
                </table></li>
            </ol>
          </li>
          <li>To enable Ceph as your Glance backend, make the following changes to the
              <codeph>~/helion/my_cloud/config/glance/glance-api.conf.j2</codeph> file: <ol>
              <li>Uncomment and edit the following values:
                <codeblock>default_store = rbd
stores = rbd
rbd_store_pool = images
rbd_store_user = glance
rbd_store_ceph_conf = /etc/ceph/ceph.conf
rbd_store_chunk_size = 8</codeblock></li>
              <li>In the same file, comment out the following references to Swift:
                <codeblock>stores = {{ glance_stores }}
default_store = {{ glance_default_store }}</codeblock></li>
            </ol></li>
          <li>Copy the corresponding keyring files to the controller nodes: <ol>
              <li>Log in to the controller nodes as a user with sudo access and run the following
                commands: <p>For Ceph as your volume backend:
                  <codeblock>sudo ceph auth get-or-create client.cinder | sudo tee /etc/ceph/ceph.client.cinder.keyring</codeblock></p>
                <p>For Ceph as your volume backup backend:
                  <codeblock>sudo ceph auth get-or-create client.cinder-backup | sudo tee -a /etc/ceph/ceph.client.cinder-backup.keyring</codeblock></p>
                <p>For Ceph as your Glance backend:
                  <codeblock>sudo ceph auth get-or-create client.glance | sudo tee -a  /etc/ceph/ceph.client.glance.keyring</codeblock></p>
                <p><b>OR</b></p>
              </li>
              <li>You can copy the keyring from the lifecycle manager to
                  <codeph>/etc/ceph</codeph> folder on all the controller nodes: <p>For Ceph as your
                  volume backend:
                  <codeblock>scp /etc/ceph/ceph.client.cinder.keyring</codeblock></p>
                <p>For Ceph as your volume backup backend:
                  <codeblock>scp /etc/ceph/ceph.client.cinder-backup.keyring</codeblock></p>
                <p>For Ceph as your Glance backend:
                  <codeblock>scp /etc/ceph/ceph.client.glance.keyring</codeblock></p></li>
              <li>Commit your configuration to a <xref href="using_git.dita">local
                repository</xref>: <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;your commit message>"</codeblock>
                <note>Before you run any playbooks, remember that you need to export the encryption
                  key in the following environment variable:<codeph> export
                    HOS_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key></codeph> See <xref
                    href="install_entryscale_kvm.dita#install_kvm"/> for reference.</note></li>
            </ol></li>
          <li>Run the configuration processor:
            <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
          <li>Run the following command to create a deployment directory:
            <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
          <li>Run the Cinder Reconfigure Playbook:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
        </ol></p>
    </section>
    <section id="attach_ceph">
      <title>Configure Nova to Allow Attachment of Ceph Volumes to Instances</title>
      <p>If you want to attach a volume from a newly-added Ceph backend to an existing Nova virtual
        machine, you must reboot your virtual machine after the new backend has been added.</p>
      <p>Perform the following steps to configure Nova to allow attachment of a Ceph volume to an
        instance:</p>
      <ol>
        <li>Log in to the lifecycle manager.</li>
        <li>Make the following changes to the
            <codeph>~/helion/my_cloud/config/nova/kvm-hypervisor.conf.j2</codeph> file: <ol>
            <li>Uncomment the Ceph backend lines and edit them as follows: <codeblock>[libvirt]
rbd_user = &#60;ceph-user>
rbd_secret_uuid = &#60;secret-uuid></codeblock>
              <p>where:</p>
              <table frame="all" rowsep="1" colsep="1" id="nova_volume">
                <tgroup cols="2">
                  <colspec colname="c1" colnum="1"/>
                  <colspec colname="c2" colnum="2"/>
                  <thead>
                    <row>
                      <entry>Value</entry>
                      <entry>Description</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>rbd_user</entry>
                      <entry>Use the username value from the
                          <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                        highlighted below::
                        <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: 457eb676-33da-42ec-9a8c-9293d545c337</codeblock></entry>
                    </row>
                    <row>
                      <entry>rbd_secret_uuid</entry>
                      <entry>Use the secret_id value from the
                          <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                        highlighted below::
                        <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: <b>457eb676-33da-42ec-9a8c-9293d545c337</b></codeblock></entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
            </li>
          </ol></li>
        <li>Commit your configuration to a <xref href="using_git.dita">local repository</xref>:
          <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;your commit message>"</codeblock></li>
        <li>Run the configuration processor:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
        <li>Run the following command to create a deployment directory:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
        <li>Run the Nova Reconfigure Playbook:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock></li>
      </ol>
    </section>
    
    <section conref="configure_vsa.dita#config_vsa/create_volumetype" id="create_voltype"/>
    <section conref="configure_vsa.dita#config_vsa/associate_volumetype" id="associate_voltype"/>
    
    <section id="extra_specs"><title>Extra Specification Options</title>
      <p>Ceph supports volumes creation with additional attributes. All these attributes can be
        specified using the extra specs options for your volume type. The administrator is expected
        to define appropriate extra spec for Ceph volume type as per the guidelines provided at <xref
          href="http://docs.openstack.org/kilo/config-reference/content/ceph-rados.html#d6e2255"
          format="html" scope="external">here</xref>.</p>
      <p>The following Cinder Volume Type extra-specs option specifies the volume backend name that is used:</p>
        <p><table frame="all"
          rowsep="1" colsep="1" id="table_ntq_swv_yt">
          <tgroup cols="3">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <colspec colname="c3" colnum="3"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value</entry>
                <entry>Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>volume_backend_name</entry>
                <entry><i>volume backend name</i></entry>
                <entry>The name of the backend to which you want to associate the volume type, which
                  you also specified earlier in the <codeph>cinder.conf.j2</codeph> file.</entry>
              </row>
            </tbody>
          </tgroup>
        </table></p>
      <p>Here is what a completed list of extra specs may look like:</p>
      <p><image href="../../media/blockstorage/extraspecs_ceph.png"/></p>
    </section>

    <section id="post_install"><title>Verifying your Ceph backend</title>
      <p>After you have configured Ceph as your Block Storage backend, you can verify this all
        completed successfully by creating a new volume.</p>
      <p>See <xref href="installation_verification.dita">Verifying your Installation</xref> for more
        details.</p>
    </section>
  </body>
</topic>
