<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
  <title>HPE Helion <tm tmtype="reg">OpenStack</tm> 2.0: Configuring for Ceph Block Storage
    Backend</title>
  <body>
    <p>This page describes how to configure your Ceph backend for the Helion Entry-scale with KVM
      Cloud model. It consists of the following steps:</p>
    <ul>
      <li><xref href="configure_ceph.dita#config_ceph/config_files">Editing your Ceph Environment
          Input Files</xref></li>
      <li><xref href="configure_ceph.dita#config_ceph/configure_backend">Configure Ceph as the
          Backend</xref></li>
      <li><xref href="configure_ceph.dita#config_ceph/attach_ceph">Configure Nova to Allow
          Attachment of Ceph Volumes to Instances</xref></li>
      <li><xref href="configure_ceph.dita#config_ceph/post_install">Verifying your Ceph
          backend</xref></li>
    </ul>
    <section id="notes"><title>Notes</title>
      <p>The Ceph cluster expects the network group that will be used for management network traffic
        within the cloud to be left to its default value i.e. MANAGEMENT. Altering the name will
        lead to failures in the cloud deployment, therefore it should be avoided.</p>
      <p>You can deploy the Ceph monitor service on a dedicated resource. Ensure you modify your
        environment after installing the lifecycle-manager.</p>
      <p>For more details, refer to <xref
          href="../blockstorage/ceph/deploy_monitor_stand_alone_node.dita">Install a monitor service
          on a dedicated resource node</xref>.</p>
      <p>While executing the <codeph>site.yml</codeph> playbook with the <codeph>–-limit</codeph>
        option, it is also expected to include the Ceph Monitor nodes in the list of restricted
        nodes if the cloud is deployed with a Ceph cluster.</p>
    </section>
    <section id="config_files">
      <title>Edit Your Ceph Environment Input Files</title>
      <p>
        <ol>
          <li>Log in to the deployer/lifecycle-manager node.</li>
          <li>Copy the example configuration files into the required setup directory and edit them
            to contain the details of your environment: <codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock>
            <p>Begin inputting your environment information into the configuration files in the
                <codeph>~/helion/my_cloud/definition</codeph> directory.</p>
            <p>Full details of how to do this can be found here: <xref href="../input_model.dita"
                >Helion OpenStack 2.0 Input Model</xref>.</p></li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/disks_osd.yml</codeph> file and
            enter the details for the additional disks meant for OSD data and journal filesystems.
              <p>A sample <codeph>disks_osd.yml</codeph> is as follows:</p>
            <codeblock>disk-models:
  - name: OSD-DISKS
    # Disk model to be used for Ceph OSD nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5

    # Disks to be used by Ceph
    # Additional disks can be added if available
    device-groups:
      - name: ceph-osd-data-only
        devices:
          - name: /dev/sdb
        consumer:
           name: ceph
           attrs:
             usage: data
      - name: ceph-osd-data-and-journal
        devices:
          - name: /dev/sdc
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdd
      - name: ceph-osd-data-and-shared-journal-set-1
        devices:
          - name: /dev/sde
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg
      - name: ceph-osd-data-and-shared-journal-set-2
        devices:
          - name: /dev/sdf
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg</codeblock>
            <p>The above sample file contains three OSD nodes and two journal disks.</p>
            <p>The disk model has the following fields:</p>
            <p>
              <table frame="all" rowsep="1" colsep="1" id="ceph1">
                <tgroup cols="2">
                  <colspec colname="c1" colnum="1"/>
                  <colspec colname="c2" colnum="2"/>
                  <thead>
                    <row>
                      <entry>Value</entry>
                      <entry>Description</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry><b>device-groups</b></entry>
                      <entry>There can be several device groups. This allows different sets of disks
                        to be used for different purposes.</entry>
                    </row>
                    <row>
                      <entry><b>name</b></entry>
                      <entry>This is an arbitrary name for the device group. The name must be
                        unique.</entry>
                    </row>
                    <row>
                      <entry><b>devices</b></entry>
                      <entry>This is a list of devices allocated to the device group. A
                          <codeph>name</codeph> field containing <codeph>/dev/sdb</codeph>,
                          <codeph>/dev/sdc</codeph>, <codeph>/dev/sde</codeph> and
                          <codeph>/dev/sdf</codeph> indicates that the device group is used by
                        Ceph.</entry>
                    </row>
                    <row>
                      <entry><b>consumer</b></entry>
                      <entry>This specifies the service that uses the device group. A
                          <codeph>name</codeph> field containing <b>ceph</b> indicates that the
                        device group is used by Ceph.</entry>
                    </row>
                    <row>
                      <entry><b>attrs</b></entry>
                      <entry>These are the attributes associated with the consumer.</entry>
                    </row>
                    <row>
                      <entry><b>usage</b></entry>
                      <entry>There can be several uses of devices for a particular service. In the
                        above sample, <codeph>usage</codeph> field contains <b>data</b> which
                        indicates that the device is used for data storage.</entry>
                    </row>
                    <row>
                      <entry><b>journal_disk</b></entry>
                      <entry>Disk to be used for storing the journal data. When running multiple
                        Ceph OSD daemons on a single node, a journal disk can be shared between OSDs
                        of the node.</entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
            </p>
            <p><note>Ensure that disks designated to be used for the OSD data and journal storage
                must be in a clean state (i.e. any existing partitions must be deleted). If you do
                not perform this step, Ceph configuration fails with errors.</note></p></li>
          <li><p>Editable parameters for Ceph are available in the following locations:</p>
            <ul>
              <li><codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph>
                <p>In the <codeph>settings.yml</codeph> file, you can edit the following
                  parameters:</p>
                <p>
                  <table frame="all" rowsep="1" colsep="1" id="table_gc4_c5t_5t">
                    <tgroup cols="2">
                      <colspec colname="c1" colnum="1"/>
                      <colspec colname="c2" colnum="2"/>
                      <thead>
                        <row>
                          <entry>Value</entry>
                          <entry>Description</entry>
                        </row>
                      </thead>
                      <tbody>
                        <row>
                          <entry><codeph>fsid</codeph></entry>
                          <entry>It is a unique identifier for the Ceph cluster.</entry>
                        </row>
                        <row>
                          <entry><codeph>ceph_cluster</codeph></entry>
                          <entry>
                            <p>Ceph clusters have a cluster name. The default cluster name is ceph,
                              but you may specify a different cluster name.</p>
                          </entry>
                        </row>
                        <row>
                          <entry><codeph>osd_settle_time</codeph></entry>
                          <entry>
                            <p>Time in seconds to wait for after starting/restarting the Ceph OSD
                              services.</p>
                          </entry>
                        </row>
                        <row>
                          <entry><codeph>osd_journal_size</codeph></entry>
                          <entry>
                            <p>The size of the journal in megabytes.</p>
                          </entry>
                        </row>
                      </tbody>
                    </tgroup>
                  </table></p></li>
              <li>
                <p>Add any additional configuration parameters for Ceph in the same file
                    (<codeph>settings.yml</codeph> file) under the 'extra:' category as
                  follows:<codeblock>extra:
  osd:
    journal_max_write_entries: 200</codeblock></p>
              </li>
              <li>
                <p><codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph></p>
                <p>The <codeph>user_model.yml</codeph> has the editable values for the different
                  pools created by HPE Helion OpenStack.</p>
              </li>
            </ul></li>
          <li>Commit your
            configuration<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>"</codeblock></li>
        </ol></p>
      <p>After your configuration files are setup, continue with the <xref
          href="install_entryscale_kvm.dita#install_kvm/provision">Entry-scale KVM Cloud
          installation steps.</xref></p>
    </section>




    <section id="configure_backend">
      <title>Configure Ceph as the Backend</title>
      <p>You can use Ceph as either the backend for volumes or volume backups or both. These steps
        will show you how to do this.</p>
      <p>Perform the following procedure on the deployer/lifecycle-manager node to configure Ceph as
        a volume backend:</p>
      <p><b>Prerequisites</b></p>
      <p>In order for Ceph to be used as a backend for volumes, the nodes running these services
        should have the Ceph client installed on them. Use the
          <codeph>ceph-client-prepare.yml</codeph> playbook to deploy the Ceph client on these
        nodes.</p>
      <p>
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</codeblock>
      </p>
      <p>This will also create Ceph users and Ceph pools on the resource nodes.</p>
      <p><b>Ceph Configuration</b></p>
      <p>Continue with the Ceph configuration with the steps below:</p>
      <p>
        <ol>
          <li>Log in to the lifecycle-manager node.</li>
          <li>Make the following changes to the
              <codeph>~/helion/my_cloud/config/cinder/cinder.conf.j2</codeph> file: <ol>
              <li>Add your Ceph backend to the <codeph>enabled_backends</codeph> section:
                <codeblock># Configure the enabled backends
enabled_backends=ceph1</codeblock></li>
              <!--<li>[OPTIONAL] If you want a use a default volume type, then enter it in the
                  <codeph>[DEFAULT]</codeph> section with the syntax below. You will want to
                remember this value when you create your volume type in the next section.
                <codeblock>[DEFAULT]
# Set the default volume type
default_volume_type = &lt;your new volume type></codeblock></li> -->
              <li>Uncomment the <codeph>ceph</codeph> section and fill the values as per your
                cluster information. If you have more than one cluster, you will need to add another
                similar section with its respective values. In the following example only one
                cluster is added. <codeblock>[ceph1]
rbd_secret_uuid = &#60;secret-uuid>
rbd_user = &#60;ceph-cinder-user>
rbd_pool = &#60;ceph-cinder-volume-pool>
rbd_ceph_conf = &#60;ceph-config-file>
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = &#60;ceph-backend-name></codeblock>
                <p>where:</p>
                <table frame="all" rowsep="1" colsep="1" id="ceph_volume">
                  <tgroup cols="2">
                    <colspec colname="c1" colnum="1"/>
                    <colspec colname="c2" colnum="2"/>
                    <thead>
                      <row>
                        <entry>Value</entry>
                        <entry>Description</entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry>rbd_secret_uuid</entry>
                        <entry>Use the secret_id value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: <b>457eb676-33da-42ec-9a8c-9293d545c337</b>
pools:
    - name: volumes</codeblock></entry>
                      </row>
                      <row>
                        <entry>rbd_user</entry>
                        <entry>Use the username value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: 457eb676-33da-42ec-9a8c-9293d545c337
pools:
    - name: volumes</codeblock></entry>
                      </row>
                      <row>
                        <entry>rbd_pool</entry>
                        <entry>Use the pool name value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below:
                          <codeblock>- user:
    name: cinder
    type: openstack
    secret_id: 457eb676-33da-42ec-9a8c-9293d545c337
pools:
    - name: <b>volumes</b></codeblock></entry>
                      </row>
                      <row>
                        <entry>rbd_ceph_conf</entry>
                        <entry>Enter your Ceph configuration file location, usually
                            <codeph>/etc/ceph/ceph.conf</codeph></entry>
                      </row>
                      <row>
                        <entry>volume_driver</entry>
                        <entry>Cinder volume driver. Leave this as the default value specified for
                          Ceph.</entry>
                      </row>
                      <row>
                        <entry>volume_backend_name</entry>
                        <entry>Name given to the Ceph backend.
                          <!--You will specify this value later in
                          the <xref href="configure_vsa.dita#config_vsa/associate_volume_backend"
                            >Associate the Volume Type to a Backend</xref> steps.--></entry>
                      </row>
                    </tbody>
                  </tgroup>
                </table>
              </li>
              <li>To enable Cinder to backup to Ceph, make the following changes to the
                  <codeph>~/helion/my_cloud/config/cinder/cinder.conf.j2</codeph> file: <ol>
                  <li>Uncomment the <codeph>ceph backup</codeph> section and fill the values: <codeblock>[DEFAULT]
backup_driver = cinder.backup.drivers.ceph
backup_ceph_conf = &#60;ceph-config-file>
backup_ceph_user = &#60;ceph-backup-user>
backup_ceph_pool = &#60;ceph-backup-pool></codeblock>
                    <p>where:</p>
                    <table frame="all" rowsep="1" colsep="1" id="ceph_backup">
                      <tgroup cols="2">
                        <colspec colname="c1" colnum="1"/>
                        <colspec colname="c2" colnum="2"/>
                        <thead>
                          <row>
                            <entry>Value</entry>
                            <entry>Description</entry>
                          </row>
                        </thead>
                        <tbody>
                          <row>
                            <entry>backup_driver</entry>
                            <entry>Cinder volume driver. Leave this as the default value specified
                              for Ceph.</entry>
                          </row>
                          <row>
                            <entry>backup_ceph_conf</entry>
                            <entry>Enter your Ceph configuration file location, usually
                                <codeph>/etc/ceph/ceph.conf</codeph></entry>
                          </row>
                          <row>
                            <entry>backup_ceph_user</entry>
                            <entry>Use the user name value from the
                                <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                              highlighted below::
                              <codeblock>- user:
    name: <b>cinder-backup</b>
    type: openstack
pools:
    - name: backups</codeblock></entry>
                          </row>
                          <row>
                            <entry>backup_ceph_pool</entry>
                            <entry>Use the pool name value from the
                                <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                              highlighted below::
                              <codeblock>- user:
    name: <b>cinder-backup</b>
    type: openstack
pools:
    - name: <b>backups</b></codeblock></entry>
                          </row>
                        </tbody>
                      </tgroup>
                    </table></li>
                </ol>
              </li>
              <li>Copy <codeph>ceph.client.cinder.keyring</codeph> to the controller nodes: <ol>
                  <li>Log in to the controller nodes as a user with sudo access and run the
                    following commands: <codeblock>ceph auth get-or-create client.cinder | tee /etc/ceph/ceph.client.cinder.keyring</codeblock>
                    <codeblock>ceph auth get-or-create client.cinder-backup | -a tee /etc/ceph/ceph.client.cinder-backup.keyring</codeblock>
                    <p><b>OR</b></p></li>
                  <li>You can copy the keyring from the deployer/lifecycle-manager node to
                      <codeph>/etc/ceph</codeph> folder on all the controller nodes:
                    <codeblock>scp /etc/ceph/ceph.client.cinder.keyring</codeblock></li>
                </ol></li>
              <li>Commit your configuration to a <xref href="using_git.dita">local
                repository</xref>: <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;your commit message>"</codeblock>
                <note>Before you run any playbooks, remember that you need to export the encryption
                  key in the following environment variable:<codeph> export
                    HOS_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key></codeph> See <xref
                    href="install_entryscale_kvm.dita#install_kvm"/> for reference.</note></li>
              <li>Run the configuration processor:
                <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
              <li>Run the following command to create a deployment directory:
                <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
              <li>Run the Cinder Reconfigure Playbook:
                <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
            </ol>
          </li>
        </ol>
      </p>
    </section>
    <section id="attach_ceph">
      <title>Configure Nova to Allow Attachment of Ceph Volumes to Instances</title>
      <p>If you want to attach a volume from a newly-added Ceph backend to an existing Nova virtual
        machine, you must reboot your virtual machine after the new backend has been added.</p>
      <p>Perform the following steps to configure Nova to allow attachment of a Ceph volume to an
        instance:</p>
      <ol>
        <li>Log in to the lifecycle-manager node.</li>
        <li>Make the following changes to the
            <codeph>~/helion/my_cloud/config/nova/kvm-hypervisor.conf.j2</codeph> file: <ol>
            <li>Uncomment the Ceph backend lines and edit them as follows: <codeblock>[libvirt]
rbd_user = &#60;ceph-user>
rbd_secret_uuid = &#60;secret-uuid></codeblock>
              <p>where:</p>
              <table frame="all" rowsep="1" colsep="1" id="nova_volume">
                <tgroup cols="2">
                  <colspec colname="c1" colnum="1"/>
                  <colspec colname="c2" colnum="2"/>
                  <thead>
                    <row>
                      <entry>Value</entry>
                      <entry>Description</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>rbd_user</entry>
                      <entry>Use the username value from the
                          <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                        highlighted below::
                        <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: 457eb676-33da-42ec-9a8c-9293d545c337</codeblock></entry>
                    </row>
                    <row>
                      <entry>rbd_secret_uuid</entry>
                      <entry>Use the secret_id value from the
                          <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                        highlighted below::
                        <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: <b>457eb676-33da-42ec-9a8c-9293d545c337</b></codeblock></entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
            </li>
          </ol></li>
        <li>Commit your configuration to a <xref href="using_git.dita">local repository</xref>:
          <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;your commit message>"</codeblock></li>
        <li>Run the configuration processor:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
        <li>Run the following command to create a deployment directory:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
        <li>Run the Nova Reconfigure Playbook:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock></li>
      </ol>
    </section>

    <section id="post_install">
      <title>Verifying your Ceph backend</title>
      <p>After the successful configuration of Cinder, you can launch the Horizon dashboard to
        create a Cinder volume type for Ceph as a backend. For the same volume type, click on the
        ‘View Extra Specs’ and add an extra spec with <b>volume_backend_name</b> as <b>Key</b> and
          <b>ceph</b> as <b>Value</b>. Once you are able to create the volume (of the Ceph volume
        type), you can perform the OpenStack operations. Also, you can verify the Cinder volume
        created for Ceph by logging in. For instructions, see <xref
          href="http://docs-staging.hpcloud.com/bundle-2015-may/helion/installation/installation_verification.html#install_verification__volume_verify"
          format="html" scope="external">Verifying Your Block Storage Backend</xref>.</p>
      <!--<p>After you have successfully configured Ceph as your Block Storage backend, you can login to Horizon Dashboard  and perform the OpenStack Operations to verify the backend configuration.</p><p>See <xref href="installation_verification.dita">Verifying your Installation</xref> for more details.</p>-->
    </section>
  </body>
</topic>
