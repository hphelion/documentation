<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> 2.0: Configuring for Ceph Block Storage
    Backend</title>
  <body>
    <p>This page describes how to configure your Ceph backend for the Helion Entry-scale with KVM
      Cloud model.</p>
    <section id="notes"><title>Notes</title>
      <p>The Ceph cluster expects the network group that will be used for management network traffic
        within the cloud to be left to its default value i.e. MANAGEMENT. Altering the name will
        lead to failures in the cloud deployment, therefore it should be avoided.</p>
      <p>You can deploy the Ceph monitor service on a dedicated resource. Ensure you modify your
        environment after installing the lifecycle-manager.</p>
      <p>For more details, refer to <xref
          href="../blockstorage/ceph/deploy_monitor_stand_alone_node.dita">Install a monitor service
          on a dedicated resource node</xref>.</p>
    </section>
    <section id="config_files">
      <title>Configuring Your Ceph Environment Input Files</title>
      <p>
        <ol>
          <li>Log in to the deployer/lifecycle-manager node.</li>
          <li>Copy the example configuration files into the required setup directory and edit them
            to contain the details of your environment: <codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock>
            <p>Begin inputting your environment information into the configuration files in the
                <codeph>~/helion/my_cloud/definition</codeph> directory.</p>
            <p>Full details of how to do this can be found here: <xref href="../input_model.dita"
                >Helion OpenStack 2.0 Input Model</xref>.</p></li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/disks_osd.yml</codeph> file and
            enter the details for the additional disks meant for OSD data and journal filesystems.
              <p>A sample <codeph>disks_osd.yml</codeph> is as follows:</p>
            <codeblock>disk-models:
  - name: OSD-DISKS
    # Disk model to be used for Ceph OSD nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5

    # Disks to be used by Ceph
    # Additional disks can be added if available
    device-groups:
      - name: ceph-osd-data-only
        devices:
          - name: /dev/sdb
        consumer:
           name: ceph
           attrs:
             usage: data
      - name: ceph-osd-data-and-journal
        devices:
          - name: /dev/sdc
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdd
      - name: ceph-osd-data-and-shared-journal-set-1
        devices:
          - name: /dev/sde
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg
      - name: ceph-osd-data-and-shared-journal-set-2
        devices:
          - name: /dev/sdf
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg</codeblock>
            <p>The above sample file contains three OSD nodes and two journal disks.</p>
            <p>The disk model has the following fields:</p>
            <p>
              <table frame="all" rowsep="1" colsep="1" id="ceph1">
                <tgroup cols="2">
                  <colspec colname="c1" colnum="1"/>
                  <colspec colname="c2" colnum="2"/>
                  <thead>
                    <row>
                      <entry>Value</entry>
                      <entry>Description</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry><b>device-groups</b></entry>
                      <entry>There can be several device groups. This allows different sets of disks
                        to be used for different purposes.</entry>
                    </row>
                    <row>
                      <entry><b>name</b></entry>
                      <entry>This is an arbitrary name for the device group. The name must be
                        unique.</entry>
                    </row>
                    <row>
                      <entry><b>devices</b></entry>
                      <entry>This is a list of devices allocated to the device group. A
                          <codeph>name</codeph> field containing <codeph>/dev/sdb</codeph>,
                          <codeph>/dev/sdc</codeph>, <codeph>/dev/sde</codeph> and
                          <codeph>/dev/sdf</codeph> indicates that the device group is used by
                        Ceph.</entry>
                    </row>
                    <row>
                      <entry><b>consumer</b></entry>
                      <entry>This specifies the service that uses the device group. A
                          <codeph>name</codeph> field containing <b>ceph</b> indicates that the
                        device group is used by Ceph.</entry>
                    </row>
                    <row>
                      <entry><b>attrs</b></entry>
                      <entry>These are the attributes associated with the consumer.</entry>
                    </row>
                    <row>
                      <entry><b>usage</b></entry>
                      <entry>There can be several uses of devices for a particular service. In the
                        above sample, <codeph>usage</codeph> field contains <b>data</b> which
                        indicates that the device is used for data storage.</entry>
                    </row>
                    <row>
                      <entry><b>journal_disk</b></entry>
                      <entry>Disk to be used for storing the journal data. When running multiple
                        Ceph OSD daemons on a single node, a journal disk can be shared between OSDs
                        of the node.</entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
            </p>
            <p><note>Ensure that disks designated to be used for the OSD data and journal storage
                must be in a clean state (i.e. any existing partitions must be deleted). If you do
                not perform this step, Ceph configuration fails with errors.</note></p></li>
          <li><p>Editable parameters for Ceph are available in the following locations:</p>
            <ul>
              <li><codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph>
                <p>In the <codeph>settings.yml</codeph> file, you can edit the following
                  parameters:</p>
                <p>
                  <table frame="all" rowsep="1" colsep="1" id="table_gc4_c5t_5t">
                    <tgroup cols="2">
                      <colspec colname="c1" colnum="1"/>
                      <colspec colname="c2" colnum="2"/>
                      <thead>
                        <row>
                          <entry>Value</entry>
                          <entry>Description</entry>
                        </row>
                      </thead>
                      <tbody>
                        <row>
                          <entry><codeph>fsid</codeph></entry>
                          <entry>It is a unique identifier for the Ceph cluster.</entry>
                        </row>
                        <row>
                          <entry><codeph>ceph_cluster</codeph></entry>
                          <entry>
                            <p>Ceph clusters have a cluster name. The default cluster name is ceph,
                              but you may specify a different cluster name.</p>
                          </entry>
                        </row>
                        <row>
                          <entry><codeph>osd_settle_time</codeph></entry>
                          <entry>
                            <p>Time in seconds to wait for after starting/restarting the Ceph OSD
                              services.</p>
                          </entry>
                        </row>
                        <row>
                          <entry><codeph>osd_journal_size</codeph></entry>
                          <entry>
                            <p>The size of the journal in megabytes.</p>
                          </entry>
                        </row>
                      </tbody>
                    </tgroup>
                  </table></p></li>
              <li>
                <p>Add any additional configuration parameters for Ceph in the same file
                    (<codeph>settings.yml</codeph> file) under the 'extra:' category as
                  follows:<codeblock>extra:
  osd:
    journal_max_write_entries: 200</codeblock></p>
              </li>
              <li>
                <p><codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph></p>
                <p>The <codeph>user_model.yml</codeph> has the editable values for the different
                  pools created by HP Helion OpenStack.</p>
              </li>
            </ul></li>
          <li>Commit your
            configuration<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>"</codeblock></li>
        </ol></p>
      <p>After your configuration files are setup, continue with the <xref
          href="install_entryscale_kvm.dita#install_kvm/provision">Entry-scale KVM Cloud
          installation steps</xref>.</p>
    </section>
    <section id="install_ceph"><title>Run Ceph Client Packages</title>
      <p>The Ceph Monitor service is deployed on the controller nodes and OSD disks are deployed as
        separate nodes (resource nodes).</p>
      <p><note>While executing the <codeph>site.yml</codeph> playbook with the
            <codeph>â€“-limit</codeph> option, it is also expected to include the controller nodes in
          the list of restricted nodes if the cloud is deployed with a Ceph cluster.</note></p>
      <p><b>Run Ceph Client Packages</b></p>
      <p>In order for Ceph to be used as a <xref
          href="installation_verification.dita#install_verification/volume_verify">backend for
          volumes</xref>, the nodes running these services should have the Ceph client installed on
        them. Use the <codeph>ceph-client-prepare.yml</codeph> playbook to deploy the Ceph client on
        these nodes.</p>
      <p>
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</codeblock>
      </p>
      <p>This will also create Ceph users and Ceph pools on the resource nodes.</p>
    </section>
  </body>
</topic>
