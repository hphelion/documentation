<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="troubleshooting_installation">
  <title>HPE Helion <tm tmtype="reg">OpenStack</tm> 2.0: Troubleshooting the Installation</title>
  <body>
    <section id="about">
      <p>We have gathered some of the common issues that occur during installation and organized
        them by when they occur during the installation. These sections will coincide with the steps
        labeled in the installation instructions.</p>
    </section>
    <section id="deployer_setup"><title>Issues during Lifecycle-manager Setup</title>
      <p><b>Issue: Running the <codeph>~/hos-2.0.0/hos-init.bash</codeph> script when configuring
          your deployer does not complete</b></p>
      <p>Part of what the <codeph>hos-init.bash</codeph> script does is install git and so if your
        DNS nameserver(s) is not specified in your <codeph>/etc/resolv.conf</codeph> file, is not
        valid, or is not functioning properly on your lifecycle-manager node then it won't be able
        to complete.</p>
      <p>To resolve this issue, double check your nameserver in your
          <codeph>/etc/resolv.conf</codeph> file and then re-run the script.</p>
    </section>
    <section id="deploy_cobbler"><title>Issues while Provisioning your Baremetal Nodes</title>
      <p><b>Issue: Configuration changes needed after Cobbler deploy</b></p>
      <p>If you've made a mistake or wish to change your
          <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> configuration file after
        you've already run the <codeph>cobbler-deploy.yml</codeph> playbook, follow these steps to
        ensure Cobbler gets updated with the new server information:</p>
      <ol>
        <li>Ensure your <codeph>servers.yml</codeph> file is updated</li>
        <li>Commit your changes to git:
          <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</codeblock></li>
        <li>Determine which nodes you have entered into Cobbler by using:
          <codeblock>sudo cobbler system list</codeblock></li>
        <li>Remove the nodes that had the old information from Cobbler using:
          <codeblock>sudo cobbler system remove --name &lt;nodename></codeblock></li>
        <li>Re-run the <codeph>cobbler-deploy.yml</codeph> playbook to update the new node
          definitions to Cobbler: <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock>
          <note>If you've also already run the <codeph>bm-reimage.yml</codeph> playbook then read
            the <xref href="#troubleshooting_installation/reimage">How to re-image an existing
              node</xref> section below for how to ensure your nodes get re-imaged when re-running
            this playbook.</note>
        </li>
      </ol>
      <p><b>Issue: bm-reimage.yml playbook doesn't find any nodes to image</b></p>
      <p>You may receive this error when running the <codeph>bm-reimage.yml</codeph> playbook:</p>
      <codeblock>TASK: [cobbler | get-nodelist | Check we have targets] ************************
failed: [localhost] => {"failed": true}
msg: There is no default set of nodes for this command, use -e nodelist
        
FATAL: all hosts have already failed -- aborting</codeblock>
      <p>This behavior occurs when you don't specify the <codeph>-e nodelist</codeph> switch to your
        command and all of your nodes are marked as <codeph>netboot-enabled: false</codeph> in
        Cobbler. By default, without the <codeph>-e nodelist</codeph> switch, the
          <codeph>bm-reimage.yml</codeph> playbook will only reimage the nodes marked as
          <codeph>netboot-enabled: True</codeph>, which you can verify which nodes you have that are
        marked as such with this command:</p>
      <codeblock>sudo cobbler system find --netboot-enabled=1</codeblock>
      <p>If this is on a fresh install and none of your nodes have been imaged with the HPE Helion
        OpenStack ISO, then you should be able to remove all of your nodes from Cobbler and re-run
        the <codeph>cobbler-deploy.yml</codeph> playbook. You can do so with these commands:</p>
      <ol>
        <li>Get a list of your nodes in Cobbler:
          <codeblock>sudo cobbler system list</codeblock></li>
        <li>Remove each of them with this command:
          <codeblock>sudo cobbler system remove --name SYSTEM_NAME</codeblock></li>
        <li>Confirm they are all removed in Cobbler:
          <codeblock>sudo cobbler system list</codeblock></li>
        <li>Re-run <codeph>cobbler-deploy.yml</codeph>:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
        <li>Confirm they are all now set to <codeph>netboot-enabled: True</codeph>:
          <codeblock>sudo cobbler system find --netboot-enabled=1</codeblock></li>
      </ol>
      <p><b>Issue: The <codeph>--limit</codeph> switch does not do anything</b></p>
      <p>If the <codeph>cobbler-deploy.yml</codeph> playbook fails, it makes a mention that to retry
        you should use the <codeph>--limit</codeph> switch, as seen below:</p>
      <codeblock>to retry, use: --limit @/home/headmin/cobbler-deploy.retry</codeblock>
      <p>This is a standard Ansible message but it is not needed in this context. Please use our
        instructions described above to remove your systems, if necessary, from Cobbler before
        re-running the playbook as the <codeph>--limit</codeph> is not needed. Note that using the
          <codeph>--limit</codeph> switch does not cause any harm and won't prevent the playbook
        from completing, it's just not a necessary step.</p>
      <p id="install_fail"><b>Issue: Dealing With Nodes that Fail to Install</b></p>
      <p>The <codeph>bm-reimage.yml</codeph> playbook will take every node as far as it can through
        the baremetal install process. Nodes that fail will not prevent the others from continuing
        to completion. At the end of the run you will get a list of the nodes (if any) that failed
        to install.</p>
      <p>If you run <codeph>bm-reimage.yml</codeph> a second time, by default it will target only
        the failed nodes the second time round (because the others are already marked as
        "completed"). Alternatively you can target specific nodes for reimage using <codeph>-e
          nodelist</codeph> as described in the section below.</p>
      <p>The places where you are most likely to see a node fail is timeout in the "wait for
        shutdown" step, which means that the node did not successfully install an operating system
        (e.g. it could be stuck in POST) or a timeout in "wait for ssh" at the end of the baremetal
        install. This means that the node did not come back up after being powered on. To fix the
        issues you'll need to connect to the nodes' consoles and investigate.</p>
      <p id="reimage"><b>How to re-image an existing node</b></p>
      <p>Once Linux for HPE Helion has been successfully installed on a node, that node will be
        marked as "installed" and subsequent runs of <codeph>bm-reimage.yml</codeph> (and related
        playbooks) will not target it. This is deliberate so that you can't reimage the node by
        accident. If you do need to reimage existing nodes you will need to use the <codeph>-e
          nodelist</codeph> option to target them specifically. For example:</p>
      <codeblock>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=cpn-0044,cpn-0045</codeblock>
      <note>You can target all nodes with <codeph>-e nodelist=all</codeph></note>
      <p>This will power cycle the specified nodes and reinstall the operating system on them, using
        the existing settings stored in Cobbler.</p>
      <p>If you want to change settings for a node in the configuration files, see the <xref
          href="#troubleshooting_installation/deploy_cobbler">Configuration changes needed after
          Cobbler deploy</xref> section above.</p>
      <p><b>Issue: Wait for SSH phase in bm-reimage.yml hangs </b></p>
      <p>This issue has been observed during deployment to systems configured with QLogic based
        BCM578XX network adapters utilizing the bnx2x driver and is currently under investigation.
        The symptom manifests following a cold boot during deployment at the "wait for SSH" phase in
        bm-reimage.yml which will result in a hang and eventually timeout, causing the baremetal
        install to fail. The presence of this particular issue can be further confirmed by
        connecting to the remote console of the server via iLO or by checking the server's dmesg
        output for the presence of bnx2_panic_dump messages, similar to the following:
        <codeblock>
bnx2x: [bnx2x_prev_unload_common:10433(eth%d)]Failed to empty BRB, hope for the best  ...
bnx2x: [bnx2x_stats_update:1268(eth0)]storm stats were not updated for 3 times
bnx2x: [bnx2x_stats_update:1269(eth0)]driver assert
bnx2x: [bnx2x_panic_dump:929(eth0)]begin crash dump -----------------  .....
bnx2x: [bnx2x_panic_dump:1163(eth0)]end crash dump -----------------</codeblock>
        This workaround is completed by rebooting the server.</p>
      <p><b>Issue: Soft lockup at imaging</b></p>
      <p>When imaging your nodes, if you see a kernel error of the type "BUG: soft lockup - CPU#10
        stuck...." you should reset the node and make sure it is imaged properly next time. Note
        that depending on when you get the error, you may have to rerun
          <codeph>cobbler-deploy.yml</codeph>. If the bm-reimage playbook says it failed to image
        the node, then cobbler knows this has occurred and you can reset the node. If not, you can
        follow the instructions above for <xref href="#troubleshooting_installation/reimage"
          >reimaging the node</xref>.</p>
      <p><b>Blank Screen Seen When Monitoring the Imaging Step</b></p>
      <p>If you are watching the <codeph>os-install</codeph> process on a node via the console
        output, there can be a pause between 2-3 minutes in length where nothing gets reported to
        the console screen. This is just after the grub menu has been displayed on a UEFI-based
        system.</p>
      <p>This is normal and nothing to be concerned with. The imaging process will continue on after
        this pause.</p>
      <p><b>Issue: The <codeph>--limit</codeph> switch does not do anything</b></p>
      <p>If the <codeph>bm-reimage.yml</codeph> playbook fails, it makes a mention that to retry you
        should use the <codeph>--limit</codeph> switch, as seen below:</p>
      <codeblock>to retry, use: --limit @/home/stack/bm-reimage.retry</codeblock>
      <p>This is a standard Ansible message but it is not needed in this context. Note that using
        the <codeph>--limit</codeph> switch does not cause any harm and won't prevent the playbook
        from completing, it's just not a necessary step.</p>
    </section>

    <section id="config_processor"><title>Issues while Updating Configuration Files</title>
      <p>If you have made corrections to your configuration files and need to re-run the
        Configuration Proceessor, the only thing you need to do is commit your changes to your local
        git:</p>
      <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "commit message"</codeblock>
      <p>You can then re-run the Configuration Processor:</p>
      <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
    </section>

    <section id="deploy_cloud"><title>Issues while Deploying the Cloud</title>
      <p><b>Issue: If the <codeph>site.yml</codeph> playbook fails, you can query the log for the
          reason</b></p>
      <p>Ansible is good about outputting the errors into the command line output, however if you'd
        like to view the full log for any reason the location is:</p>
      <p>
        <codeblock>~/.ansible/ansible.log</codeblock>
      </p>
      <p>This log is updated real time as you run ansible playbooks.</p>
      <note type="tip">Use grep to parse through the log. Usage: <codeph>grep &lt;text>
          ~/.ansible/ansible.log</codeph></note>
      <p id="wipe"><b>Issue: How to Wipe the Disks of your Machines</b></p>
      <p>If you have re-run the <codeph>site.yml</codeph> playbook, you may need to wipe the disks
        of your nodes</p>
      <p>You would generally run the playbook below after re-running the
          <codeph>bm-reimage.yml</codeph> playbook but before you re-run the
          <codeph>site.yml</codeph> playbook.</p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts wipe_disks.yml</codeblock>
      <p>The playbook will show you the disks to be wiped in the output and allow you to confirm
        that you want to complete this action or abort it if you do not want to proceed. You can
        optionally use the <codeph>--limit &lt;NODE_NAME></codeph> switch on this playbook to
        restrict it to specific nodes.</p>
      <p>If you receive an error stating that <codeph>osconfig</codeph> has already run on your
        nodes then you will need to remove the <codeph>/etc/hos/osconfig-ran</codeph> file on each
        of the nodes you want to wipe with this command:</p>
      <codeblock>sudo rm /etc/hos/osconfig-ran</codeblock>
      <p>That will clear this flag and allow the disk to be wiped.</p>
      <p><b>Issue: Freezer installation fails if an independent network is used for the
          External_API.</b></p>
      <p> Currently the Freezer installation fails if an independent network is used for the
        External_API. If you intend to deploy the External API <!--in Beta 2--> on an independent
        network, the following changes need to be made: </p>
      <p>In <codeph>roles/freezer-agent/defaults/main.yml</codeph> add the following line:
        <codeblock>backup_freezer_api_url: "{{ FRE_API | item('advertises.vips.private[0].url', default=' ') }}"</codeblock>
        In <codeph>roles/freezer-agent/templates/backup.osrc.j2</codeph> add the following line:
        <codeblock>export OS_FREEZER_URL={{ backup_freezer_api_url }}</codeblock>
      </p>
      <p><b>Error Received if Root Logical Volume is Too Small</b></p>
      <p>When running the <codeph>site.yml</codeph> playbook, you may receive the error below if
        your root logical-volume is too small:</p>
      <codeblock>2015-09-29 15:54:02,751 p=26345 u=stack |  TASK: [osconfig | disk config | Extend root LV] *******************************
2015-09-29 15:54:03,021 p=26345 u=stack |  failed: [helion-ccp-swpac-m1-mgmt] => (item=({'physical_volumes': ['/dev/sda_root'], 'consumer': {'name': 'os'},
'name': 'hlm-vg'}, {'mount': '/', 'fstype': 'ext4', 'name': 'root', 'size': '10%'})) => {"changed": true, "cmd": ["lvextend", "-l", "10%VG", "/dev/hlm-
vg/root"], "delta": "0:00:00.022983", "end": "2015-09-29 10:54:18.925855", "failed": true, "failed_when_result": true, "item": [{"consumer": {"name": 
"os"}, "name": "hlm-vg", "physical_volumes": ["/dev/sda_root"]}, {"fstype": "ext4", "mount": "/", "name": "root", "size": "10%"}], "rc": 3, "start": "2015-
09-29 10:54:18.902872", "stdout_lines": [], "warnings": []}
2015-09-29 15:54:03,022 p=26345 u=stack |  stderr:   New size given (7128 extents) not larger than existing size (7629 extents)</codeblock>
      <p>The specific part of this error to parse out and resolve is:</p>
      <codeblock>stderr:   New size given (7128 extents) not larger than existing size (7629 extents)</codeblock>
      <p>The error also references the root volume:</p>
      <codeblock>"name": "root", "size": "10%"</codeblock>
      <p>The problem is that the root logical-volume, as specified in the
          <codeph>disks_controller.yml</codeph> file, is set to <codeph>10%</codeph> of the overall
        physical volume and this value is too small.</p>
      <p>To resolve this issue you need to ensure that the percentage is set properly for the size
        of your logical-volume. The default values in the configuration files is based on a 500GB
        disk, so if your logical-volumes are smaller you may need to increase the percentage so
        there is enough room.</p>
    </section>
  </body>
</topic>
